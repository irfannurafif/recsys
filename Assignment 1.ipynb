{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'#'analogy_alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 8. ... 0. 0. 0.]\n",
      " [1. 8. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'she': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'it': 7,\n",
       " 'of': 8,\n",
       " 'was': 9,\n",
       " 'in': 10,\n",
       " 'alice': 11,\n",
       " 'you': 12,\n",
       " 'that': 13,\n",
       " 'her': 14,\n",
       " 'as': 15,\n",
       " 'said': 16,\n",
       " 'had': 17,\n",
       " 'for': 18,\n",
       " 'but': 19,\n",
       " 'be': 20,\n",
       " 'on': 21,\n",
       " 'all': 22,\n",
       " 'with': 23,\n",
       " 'little': 24,\n",
       " 'mouse': 25,\n",
       " 'down': 26,\n",
       " 'very': 27,\n",
       " 'this': 28,\n",
       " 'not': 29,\n",
       " 'so': 30,\n",
       " 'out': 31,\n",
       " 'if': 32,\n",
       " 'is': 33,\n",
       " 'at': 34,\n",
       " 't': 35,\n",
       " 's': 36,\n",
       " 'll': 37,\n",
       " 'how': 38,\n",
       " 'they': 39,\n",
       " 'about': 40,\n",
       " 'herself': 41,\n",
       " 'me': 42,\n",
       " 'up': 43,\n",
       " 'what': 44,\n",
       " 'way': 45,\n",
       " 'when': 46,\n",
       " 'like': 47,\n",
       " 'one': 48,\n",
       " 'do': 49,\n",
       " 'no': 50,\n",
       " 'oh': 51,\n",
       " 'went': 52,\n",
       " 'thought': 53,\n",
       " 'again': 54,\n",
       " 'there': 55,\n",
       " 'see': 56,\n",
       " 'or': 57,\n",
       " 'could': 58,\n",
       " 'would': 59,\n",
       " 'think': 60,\n",
       " 'them': 61,\n",
       " 'know': 62,\n",
       " 'rabbit': 63,\n",
       " 'dear': 64,\n",
       " 'were': 65,\n",
       " 'time': 66,\n",
       " 'get': 67,\n",
       " 'here': 68,\n",
       " 'must': 69,\n",
       " 'my': 70,\n",
       " 'by': 71,\n",
       " 'into': 72,\n",
       " 'found': 73,\n",
       " 'such': 74,\n",
       " 'began': 75,\n",
       " 'soon': 76,\n",
       " 'm': 77,\n",
       " 'quite': 78,\n",
       " 'then': 79,\n",
       " 'off': 80,\n",
       " 'now': 81,\n",
       " 'go': 82,\n",
       " 'say': 83,\n",
       " 'have': 84,\n",
       " 'which': 85,\n",
       " 'come': 86,\n",
       " 'dinah': 87,\n",
       " 'your': 88,\n",
       " 'thing': 89,\n",
       " 'dodo': 90,\n",
       " 'much': 91,\n",
       " 'shall': 92,\n",
       " 'things': 93,\n",
       " 'long': 94,\n",
       " 'door': 95,\n",
       " 'who': 96,\n",
       " 'can': 97,\n",
       " 'once': 98,\n",
       " 'did': 99,\n",
       " 'over': 100,\n",
       " 'feet': 101,\n",
       " 'after': 102,\n",
       " 'wonder': 103,\n",
       " 'first': 104,\n",
       " 'let': 105,\n",
       " 'are': 106,\n",
       " 'cats': 107,\n",
       " 'round': 108,\n",
       " 'poor': 109,\n",
       " 'back': 110,\n",
       " 'he': 111,\n",
       " 'nothing': 112,\n",
       " 'seemed': 113,\n",
       " 'its': 114,\n",
       " 'never': 115,\n",
       " 'going': 116,\n",
       " 'great': 117,\n",
       " 'got': 118,\n",
       " 'ever': 119,\n",
       " 'table': 120,\n",
       " 'any': 121,\n",
       " 'only': 122,\n",
       " 'more': 123,\n",
       " 'cried': 124,\n",
       " 'gloves': 125,\n",
       " 'fan': 126,\n",
       " 'well': 127,\n",
       " 'white': 128,\n",
       " 'looked': 129,\n",
       " 'before': 130,\n",
       " 'moment': 131,\n",
       " 'why': 132,\n",
       " 'an': 133,\n",
       " 'eat': 134,\n",
       " 'however': 135,\n",
       " 'sure': 136,\n",
       " 'pool': 137,\n",
       " 'getting': 138,\n",
       " 'took': 139,\n",
       " 'either': 140,\n",
       " 'just': 141,\n",
       " 'upon': 142,\n",
       " 've': 143,\n",
       " 'through': 144,\n",
       " 'wish': 145,\n",
       " 'tell': 146,\n",
       " 'hall': 147,\n",
       " 'key': 148,\n",
       " 'half': 149,\n",
       " 'find': 150,\n",
       " 'his': 151,\n",
       " 'use': 152,\n",
       " 'large': 153,\n",
       " 'look': 154,\n",
       " 'too': 155,\n",
       " 'house': 156,\n",
       " 'good': 157,\n",
       " 'right': 158,\n",
       " 'words': 159,\n",
       " 'am': 160,\n",
       " 'might': 161,\n",
       " 'question': 162,\n",
       " 'came': 163,\n",
       " 'dry': 164,\n",
       " 'away': 165,\n",
       " 'been': 166,\n",
       " 'small': 167,\n",
       " 'high': 168,\n",
       " 'garden': 169,\n",
       " 'tears': 170,\n",
       " 'don': 171,\n",
       " 'won': 172,\n",
       " 'doesn': 173,\n",
       " 'without': 174,\n",
       " 'suddenly': 175,\n",
       " 'eyes': 176,\n",
       " 'some': 177,\n",
       " 'put': 178,\n",
       " 'sort': 179,\n",
       " 'their': 180,\n",
       " 'rather': 181,\n",
       " 'talking': 182,\n",
       " 'saying': 183,\n",
       " 'felt': 184,\n",
       " 'hand': 185,\n",
       " 'low': 186,\n",
       " 'trying': 187,\n",
       " 'golden': 188,\n",
       " 'head': 189,\n",
       " 'indeed': 190,\n",
       " 'will': 191,\n",
       " 'while': 192,\n",
       " 'same': 193,\n",
       " 'try': 194,\n",
       " 'beg': 195,\n",
       " 'tone': 196,\n",
       " 'our': 197,\n",
       " 'd': 198,\n",
       " 'called': 199,\n",
       " 'birds': 200,\n",
       " 'lory': 201,\n",
       " 'race': 202,\n",
       " 'looking': 203,\n",
       " 'course': 204,\n",
       " 'book': 205,\n",
       " 'made': 206,\n",
       " 'ran': 207,\n",
       " 'seen': 208,\n",
       " 'fell': 209,\n",
       " 'next': 210,\n",
       " 'tried': 211,\n",
       " 'make': 212,\n",
       " 'anything': 213,\n",
       " 'four': 214,\n",
       " 'nice': 215,\n",
       " 'people': 216,\n",
       " 'please': 217,\n",
       " 'should': 218,\n",
       " 'remember': 219,\n",
       " 'turned': 220,\n",
       " 'other': 221,\n",
       " 'glass': 222,\n",
       " 'than': 223,\n",
       " 'marked': 224,\n",
       " 'best': 225,\n",
       " 'sat': 226,\n",
       " 'two': 227,\n",
       " 'english': 228,\n",
       " 'pair': 229,\n",
       " 'heard': 230,\n",
       " 'kid': 231,\n",
       " 'ready': 232,\n",
       " 'voice': 233,\n",
       " 'queer': 234,\n",
       " 'changed': 235,\n",
       " 'swam': 236,\n",
       " 'talk': 237,\n",
       " 'offended': 238,\n",
       " 'we': 239,\n",
       " 'party': 240,\n",
       " 'prizes': 241,\n",
       " 'chapter': 242,\n",
       " 'hole': 243,\n",
       " 'tired': 244,\n",
       " 'having': 245,\n",
       " 'pictures': 246,\n",
       " 'own': 247,\n",
       " 'mind': 248,\n",
       " 'hot': 249,\n",
       " 'day': 250,\n",
       " 'pocket': 251,\n",
       " 'hurried': 252,\n",
       " 'take': 253,\n",
       " 'another': 254,\n",
       " 'coming': 255,\n",
       " 'from': 256,\n",
       " 'fall': 257,\n",
       " 'even': 258,\n",
       " 'near': 259,\n",
       " 'idea': 260,\n",
       " 'among': 261,\n",
       " 'didn': 262,\n",
       " 'ask': 263,\n",
       " 'air': 264,\n",
       " 'perhaps': 265,\n",
       " 'else': 266,\n",
       " 'cat': 267,\n",
       " 'afraid': 268,\n",
       " 'bats': 269,\n",
       " 'rate': 270,\n",
       " 'those': 271,\n",
       " 'really': 272,\n",
       " 'bottle': 273,\n",
       " 'children': 274,\n",
       " 'enough': 275,\n",
       " 'anxiously': 276,\n",
       " 'surprised': 277,\n",
       " 'speak': 278,\n",
       " 'hastily': 279,\n",
       " 'duchess': 280,\n",
       " 'kept': 281,\n",
       " 'everything': 282,\n",
       " 'mabel': 283,\n",
       " 'capital': 284,\n",
       " 'tail': 285,\n",
       " 'something': 286,\n",
       " 'william': 287,\n",
       " 'pardon': 288,\n",
       " 'crowded': 289,\n",
       " 'whole': 290,\n",
       " 'last': 291,\n",
       " 'old': 292,\n",
       " 'him': 293,\n",
       " 'thimble': 294,\n",
       " 'close': 295,\n",
       " 'hear': 296,\n",
       " 'itself': 297,\n",
       " 'late': 298,\n",
       " 'ought': 299,\n",
       " 'under': 300,\n",
       " 'world': 301,\n",
       " 'deep': 302,\n",
       " 'slowly': 303,\n",
       " 'happen': 304,\n",
       " 'dark': 305,\n",
       " 'noticed': 306,\n",
       " 'somebody': 307,\n",
       " 'home': 308,\n",
       " 'end': 309,\n",
       " 'many': 310,\n",
       " 'fallen': 311,\n",
       " 'several': 312,\n",
       " 'lessons': 313,\n",
       " 'still': 314,\n",
       " 'distance': 315,\n",
       " 'seem': 316,\n",
       " 'walk': 317,\n",
       " 'heads': 318,\n",
       " 'name': 319,\n",
       " 'ma': 320,\n",
       " 'fancy': 321,\n",
       " 're': 322,\n",
       " 'manage': 323,\n",
       " 'night': 324,\n",
       " 'mice': 325,\n",
       " 'answer': 326,\n",
       " 'passage': 327,\n",
       " 'sight': 328,\n",
       " 'behind': 329,\n",
       " 'every': 330,\n",
       " 'three': 331,\n",
       " 'alas': 332,\n",
       " 'inches': 333,\n",
       " 'along': 334,\n",
       " 'bright': 335,\n",
       " 'happened': 336,\n",
       " 'few': 337,\n",
       " 'drink': 338,\n",
       " 'hurry': 339,\n",
       " 'poison': 340,\n",
       " 'finger': 341,\n",
       " 'almost': 342,\n",
       " 'certain': 343,\n",
       " 'finished': 344,\n",
       " 'curious': 345,\n",
       " 'face': 346,\n",
       " 'candle': 347,\n",
       " 'generally': 348,\n",
       " 'remembered': 349,\n",
       " 'box': 350,\n",
       " 'left': 351,\n",
       " 'person': 352,\n",
       " 'eye': 353,\n",
       " 'lying': 354,\n",
       " 'cake': 355,\n",
       " 'life': 356,\n",
       " 'deal': 357,\n",
       " 'dropped': 358,\n",
       " 'times': 359,\n",
       " 'seems': 360,\n",
       " 'stay': 361,\n",
       " 'being': 362,\n",
       " 'sudden': 363,\n",
       " 'cause': 364,\n",
       " 'water': 365,\n",
       " 'o': 366,\n",
       " 'history': 367,\n",
       " 'trembling': 368,\n",
       " 'always': 369,\n",
       " 'dogs': 370,\n",
       " 'eagerly': 371,\n",
       " 'fetch': 372,\n",
       " 'us': 373,\n",
       " 'duck': 374,\n",
       " 'eaglet': 375,\n",
       " 'caucus': 376,\n",
       " 'tale': 377,\n",
       " 'replied': 378,\n",
       " 'turning': 379,\n",
       " 'sister': 380,\n",
       " 'bank': 381,\n",
       " 'considering': 382,\n",
       " 'feel': 383,\n",
       " 'sleepy': 384,\n",
       " 'stupid': 385,\n",
       " 'whether': 386,\n",
       " 'making': 387,\n",
       " 'worth': 388,\n",
       " 'trouble': 389,\n",
       " 'remarkable': 390,\n",
       " 'natural': 391,\n",
       " 'watch': 392,\n",
       " 'waistcoat': 393,\n",
       " 'started': 394,\n",
       " 'across': 395,\n",
       " 'falling': 396,\n",
       " 'filled': 397,\n",
       " 'cupboards': 398,\n",
       " 'shelves': 399,\n",
       " 'saw': 400,\n",
       " 'jar': 401,\n",
       " 'fear': 402,\n",
       " 'managed': 403,\n",
       " 'wouldn': 404,\n",
       " 'top': 405,\n",
       " 'likely': 406,\n",
       " 'miles': 407,\n",
       " 'aloud': 408,\n",
       " 'somewhere': 409,\n",
       " 'earth': 410,\n",
       " 'though': 411,\n",
       " 'opportunity': 412,\n",
       " 'knowledge': 413,\n",
       " 'listen': 414,\n",
       " 'yes': 415,\n",
       " 'latitude': 416,\n",
       " 'longitude': 417,\n",
       " 'funny': 418,\n",
       " 'glad': 419,\n",
       " 'new': 420,\n",
       " 'spoke': 421,\n",
       " 'girl': 422,\n",
       " 'asking': 423,\n",
       " 'miss': 424,\n",
       " 'catch': 425,\n",
       " 'bat': 426,\n",
       " 'sometimes': 427,\n",
       " 'matter': 428,\n",
       " 'begun': 429,\n",
       " 'walking': 430,\n",
       " 'thump': 431,\n",
       " 'bit': 432,\n",
       " 'hurt': 433,\n",
       " 'lost': 434,\n",
       " 'corner': 435,\n",
       " 'ears': 436,\n",
       " 'whiskers': 437,\n",
       " 'row': 438,\n",
       " 'roof': 439,\n",
       " 'doors': 440,\n",
       " 'side': 441,\n",
       " 'walked': 442,\n",
       " 'sadly': 443,\n",
       " 'middle': 444,\n",
       " 'tiny': 445,\n",
       " 'opened': 446,\n",
       " 'led': 447,\n",
       " 'larger': 448,\n",
       " 'shut': 449,\n",
       " 'telescope': 450,\n",
       " 'knew': 451,\n",
       " 'waiting': 452,\n",
       " 'hoping': 453,\n",
       " 'rules': 454,\n",
       " 'shutting': 455,\n",
       " 'certainly': 456,\n",
       " 'beautifully': 457,\n",
       " 'hold': 458,\n",
       " 'usually': 459,\n",
       " 'forgotten': 460,\n",
       " 'taste': 461,\n",
       " 'finding': 462,\n",
       " 'fact': 463,\n",
       " 'feeling': 464,\n",
       " 'size': 465,\n",
       " 'waited': 466,\n",
       " 'minutes': 467,\n",
       " 'altogether': 468,\n",
       " 'reach': 469,\n",
       " 'crying': 470,\n",
       " 'sharply': 471,\n",
       " 'leave': 472,\n",
       " 'minute': 473,\n",
       " 'gave': 474,\n",
       " 'severely': 475,\n",
       " 'against': 476,\n",
       " 'child': 477,\n",
       " 'fond': 478,\n",
       " 'makes': 479,\n",
       " 'grow': 480,\n",
       " 'happens': 481,\n",
       " 'holding': 482,\n",
       " 'growing': 483,\n",
       " 'curiouser': 484,\n",
       " 'forgot': 485,\n",
       " 'far': 486,\n",
       " 'dears': 487,\n",
       " 'give': 488,\n",
       " 'sending': 489,\n",
       " 'foot': 490,\n",
       " 'nonsense': 491,\n",
       " 'nine': 492,\n",
       " 'cry': 493,\n",
       " 'yourself': 494,\n",
       " 'stop': 495,\n",
       " 'pattering': 496,\n",
       " 'trotting': 497,\n",
       " 'muttering': 498,\n",
       " 'help': 499,\n",
       " 'sir': 500,\n",
       " 'hard': 501,\n",
       " 'morning': 502,\n",
       " 'ah': 503,\n",
       " 'age': 504,\n",
       " 'hair': 505,\n",
       " 'ringlets': 506,\n",
       " 'mine': 507,\n",
       " 'sorts': 508,\n",
       " 'puzzling': 509,\n",
       " 'used': 510,\n",
       " 'paris': 511,\n",
       " 'rome': 512,\n",
       " 'doth': 513,\n",
       " 'hands': 514,\n",
       " 'alone': 515,\n",
       " 'done': 516,\n",
       " 'shrinking': 517,\n",
       " 'frightened': 518,\n",
       " 'change': 519,\n",
       " 'bad': 520,\n",
       " 'slipped': 521,\n",
       " 'salt': 522,\n",
       " 'sea': 523,\n",
       " 'case': 524,\n",
       " 'railway': 525,\n",
       " 'hadn': 526,\n",
       " 'suppose': 527,\n",
       " 'swimming': 528,\n",
       " 'speaking': 529,\n",
       " 'understand': 530,\n",
       " 'french': 531,\n",
       " 'conqueror': 532,\n",
       " 'lesson': 533,\n",
       " 'angry': 534,\n",
       " 'show': 535,\n",
       " 'paws': 536,\n",
       " 'nurse': 537,\n",
       " 'catching': 538,\n",
       " 'subject': 539,\n",
       " 'sit': 540,\n",
       " 'says': 541,\n",
       " 'useful': 542,\n",
       " 'shore': 543,\n",
       " 'hate': 544,\n",
       " 'animals': 545,\n",
       " 'fur': 546,\n",
       " 'wet': 547,\n",
       " 'better': 548,\n",
       " 'ring': 549,\n",
       " 'silence': 550,\n",
       " 'wanted': 551,\n",
       " 'edwin': 552,\n",
       " 'morcar': 553,\n",
       " 'earls': 554,\n",
       " 'mercia': 555,\n",
       " 'northumbria': 556,\n",
       " 'advisable': 557,\n",
       " 'meet': 558,\n",
       " 'melancholy': 559,\n",
       " 'solemnly': 560,\n",
       " 'explain': 561,\n",
       " 'running': 562,\n",
       " 'liked': 563,\n",
       " 'has': 564,\n",
       " 'chorus': 565,\n",
       " 'comfits': 566,\n",
       " 'speech': 567,\n",
       " 'caused': 568,\n",
       " 'sad': 569,\n",
       " 'fury': 570,\n",
       " 'trial': 571,\n",
       " 'jury': 572,\n",
       " 'judge': 573,\n",
       " 'finish': 574,\n",
       " 'story': 575,\n",
       " 'crab': 576,\n",
       " 'nobody': 577,\n",
       " 'ferrets': 578,\n",
       " 'hunting': 579,\n",
       " 'mary': 580,\n",
       " 'ann': 581,\n",
       " 'messages': 582,\n",
       " 'room': 583,\n",
       " 'beginning': 584,\n",
       " 'sitting': 585,\n",
       " 'twice': 586,\n",
       " 'peeped': 587,\n",
       " 'reading': 588,\n",
       " 'conversations': 589,\n",
       " 'pleasure': 590,\n",
       " 'daisy': 591,\n",
       " 'chain': 592,\n",
       " 'picking': 593,\n",
       " 'daisies': 594,\n",
       " 'pink': 595,\n",
       " 'nor': 596,\n",
       " 'afterwards': 597,\n",
       " 'occurred': 598,\n",
       " 'wondered': 599,\n",
       " 'actually': 600,\n",
       " 'flashed': 601,\n",
       " 'burning': 602,\n",
       " 'curiosity': 603,\n",
       " 'field': 604,\n",
       " 'fortunately': 605,\n",
       " 'pop': 606,\n",
       " 'hedge': 607,\n",
       " 'straight': 608,\n",
       " 'tunnel': 609,\n",
       " 'dipped': 610,\n",
       " 'stopping': 611,\n",
       " 'plenty': 612,\n",
       " 'sides': 613,\n",
       " 'maps': 614,\n",
       " 'hung': 615,\n",
       " 'pegs': 616,\n",
       " 'passed': 617,\n",
       " 'labelled': 618,\n",
       " 'orange': 619,\n",
       " 'marmalade': 620,\n",
       " 'disappointment': 621,\n",
       " 'empty': 622,\n",
       " 'drop': 623,\n",
       " 'killing': 624,\n",
       " 'past': 625,\n",
       " 'tumbling': 626,\n",
       " 'stairs': 627,\n",
       " 'brave': 628,\n",
       " 'true': 629,\n",
       " 'centre': 630,\n",
       " 'thousand': 631,\n",
       " 'learnt': 632,\n",
       " 'schoolroom': 633,\n",
       " 'showing': 634,\n",
       " 'practice': 635,\n",
       " 'grand': 636,\n",
       " 'presently': 637,\n",
       " 'downward': 638,\n",
       " 'antipathies': 639,\n",
       " 'listening': 640,\n",
       " 'sound': 641,\n",
       " 'word': 642,\n",
       " 'country': 643,\n",
       " 'zealand': 644,\n",
       " 'australia': 645,\n",
       " 'curtsey': 646,\n",
       " 'curtseying': 647,\n",
       " 'ignorant': 648,\n",
       " 'written': 649,\n",
       " 'hope': 650,\n",
       " 'saucer': 651,\n",
       " 'milk': 652,\n",
       " 'tea': 653,\n",
       " 'dreamy': 654,\n",
       " 'couldn': 655,\n",
       " 'dozing': 656,\n",
       " 'dream': 657,\n",
       " 'earnestly': 658,\n",
       " 'truth': 659,\n",
       " 'heap': 660,\n",
       " 'sticks': 661,\n",
       " 'leaves': 662,\n",
       " 'jumped': 663,\n",
       " 'overhead': 664,\n",
       " 'hurrying': 665,\n",
       " 'wind': 666,\n",
       " 'longer': 667,\n",
       " 'lit': 668,\n",
       " 'lamps': 669,\n",
       " 'hanging': 670,\n",
       " 'locked': 671,\n",
       " 'wondering': 672,\n",
       " 'legged': 673,\n",
       " 'solid': 674,\n",
       " 'except': 675,\n",
       " 'belong': 676,\n",
       " 'locks': 677,\n",
       " 'open': 678,\n",
       " 'second': 679,\n",
       " 'curtain': 680,\n",
       " 'fifteen': 681,\n",
       " 'lock': 682,\n",
       " 'delight': 683,\n",
       " 'fitted': 684,\n",
       " 'rat': 685,\n",
       " 'knelt': 686,\n",
       " 'loveliest': 687,\n",
       " 'longed': 688,\n",
       " 'wander': 689,\n",
       " 'beds': 690,\n",
       " 'flowers': 691,\n",
       " 'cool': 692,\n",
       " 'fountains': 693,\n",
       " 'doorway': 694,\n",
       " 'shoulders': 695,\n",
       " 'begin': 696,\n",
       " 'lately': 697,\n",
       " 'telescopes': 698,\n",
       " 'neck': 699,\n",
       " 'paper': 700,\n",
       " 'label': 701,\n",
       " 'printed': 702,\n",
       " 'wise': 703,\n",
       " 'read': 704,\n",
       " 'histories': 705,\n",
       " 'burnt': 706,\n",
       " 'eaten': 707,\n",
       " 'wild': 708,\n",
       " 'beasts': 709,\n",
       " 'unpleasant': 710,\n",
       " 'because': 711,\n",
       " 'simple': 712,\n",
       " 'friends': 713,\n",
       " 'taught': 714,\n",
       " 'red': 715,\n",
       " 'poker': 716,\n",
       " 'burn': 717,\n",
       " 'cut': 718,\n",
       " 'deeply': 719,\n",
       " 'knife': 720,\n",
       " 'bleeds': 721,\n",
       " 'disagree': 722,\n",
       " 'sooner': 723,\n",
       " 'later': 724,\n",
       " 'ventured': 725,\n",
       " 'mixed': 726,\n",
       " 'flavour': 727,\n",
       " 'cherry': 728,\n",
       " 'tart': 729,\n",
       " 'custard': 730,\n",
       " 'pine': 731,\n",
       " 'apple': 732,\n",
       " 'roast': 733,\n",
       " 'turkey': 734,\n",
       " 'toffee': 735,\n",
       " 'buttered': 736,\n",
       " 'toast': 737,\n",
       " 'ten': 738,\n",
       " 'brightened': 739,\n",
       " 'lovely': 740,\n",
       " 'shrink': 741,\n",
       " 'further': 742,\n",
       " 'nervous': 743,\n",
       " 'flame': 744,\n",
       " 'blown': 745,\n",
       " 'decided': 746,\n",
       " 'possibly': 747,\n",
       " 'plainly': 748,\n",
       " 'climb': 749,\n",
       " 'legs': 750,\n",
       " 'slippery': 751,\n",
       " 'advise': 752,\n",
       " 'advice': 753,\n",
       " 'seldom': 754,\n",
       " 'followed': 755,\n",
       " 'scolded': 756,\n",
       " 'bring': 757,\n",
       " 'cheated': 758,\n",
       " 'game': 759,\n",
       " 'croquet': 760,\n",
       " 'playing': 761,\n",
       " 'pretending': 762,\n",
       " 'pretend': 763,\n",
       " 'hardly': 764,\n",
       " 'respectable': 765,\n",
       " 'currants': 766,\n",
       " 'smaller': 767,\n",
       " 'creep': 768,\n",
       " 'care': 769,\n",
       " 'ate': 770,\n",
       " 'remained': 771,\n",
       " 'eats': 772,\n",
       " 'expecting': 773,\n",
       " 'dull': 774,\n",
       " 'common': 775,\n",
       " 'set': 776,\n",
       " 'work': 777,\n",
       " 'ii': 778,\n",
       " 'opening': 779,\n",
       " 'largest': 780,\n",
       " 'bye': 781,\n",
       " 'shoes': 782,\n",
       " 'stockings': 783,\n",
       " 'shan': 784,\n",
       " 'able': 785,\n",
       " 'myself': 786,\n",
       " 'kind': 787,\n",
       " 'want': 788,\n",
       " 'boots': 789,\n",
       " 'christmas': 790,\n",
       " 'planning': 791,\n",
       " 'carrier': 792,\n",
       " 'presents': 793,\n",
       " 'odd': 794,\n",
       " 'directions': 795,\n",
       " 'esq': 796,\n",
       " 'hearthrug': 797,\n",
       " 'fender': 798,\n",
       " 'love': 799,\n",
       " 'struck': 800,\n",
       " 'hopeless': 801,\n",
       " 'ashamed': 802,\n",
       " 'shedding': 803,\n",
       " 'gallons': 804,\n",
       " 'until': 805,\n",
       " 'reaching': 806,\n",
       " 'dried': 807,\n",
       " 'returning': 808,\n",
       " 'splendidly': 809,\n",
       " 'dressed': 810,\n",
       " 'himself': 811,\n",
       " 'savage': 812,\n",
       " 'desperate': 813,\n",
       " 'timid': 814,\n",
       " 'violently': 815,\n",
       " 'skurried': 816,\n",
       " 'darkness': 817,\n",
       " 'fanning': 818,\n",
       " 'yesterday': 819,\n",
       " 'usual': 820,\n",
       " 'different': 821,\n",
       " 'puzzle': 822,\n",
       " 'thinking': 823,\n",
       " 'ada': 824,\n",
       " 'goes': 825,\n",
       " 'knows': 826,\n",
       " 'besides': 827,\n",
       " 'five': 828,\n",
       " 'twelve': 829,\n",
       " 'six': 830,\n",
       " 'thirteen': 831,\n",
       " 'seven': 832,\n",
       " 'twenty': 833,\n",
       " 'multiplication': 834,\n",
       " 'signify': 835,\n",
       " 'geography': 836,\n",
       " 'london': 837,\n",
       " 'wrong': 838,\n",
       " 'crossed': 839,\n",
       " 'lap': 840,\n",
       " 'repeat': 841,\n",
       " 'sounded': 842,\n",
       " 'hoarse': 843,\n",
       " 'strange': 844,\n",
       " 'crocodile': 845,\n",
       " 'improve': 846,\n",
       " 'shining': 847,\n",
       " 'pour': 848,\n",
       " 'waters': 849,\n",
       " 'nile': 850,\n",
       " 'scale': 851,\n",
       " 'cheerfully': 852,\n",
       " 'grin': 853,\n",
       " 'neatly': 854,\n",
       " 'spread': 855,\n",
       " 'claws': 856,\n",
       " 'welcome': 857,\n",
       " 'fishes': 858,\n",
       " 'gently': 859,\n",
       " 'smiling': 860,\n",
       " 'jaws': 861,\n",
       " 'live': 862,\n",
       " 'poky': 863,\n",
       " 'toys': 864,\n",
       " 'play': 865,\n",
       " 'learn': 866,\n",
       " 'putting': 867,\n",
       " 'till': 868,\n",
       " 'burst': 869,\n",
       " 'measure': 870,\n",
       " 'nearly': 871,\n",
       " 'guess': 872,\n",
       " 'rapidly': 873,\n",
       " 'avoid': 874,\n",
       " 'narrow': 875,\n",
       " 'escape': 876,\n",
       " 'existence': 877,\n",
       " 'speed': 878,\n",
       " 'worse': 879,\n",
       " 'declare': 880,\n",
       " 'these': 881,\n",
       " 'splash': 882,\n",
       " 'chin': 883,\n",
       " 'somehow': 884,\n",
       " 'seaside': 885,\n",
       " 'general': 886,\n",
       " 'conclusion': 887,\n",
       " 'wherever': 888,\n",
       " 'coast': 889,\n",
       " 'number': 890,\n",
       " 'bathing': 891,\n",
       " 'machines': 892,\n",
       " 'digging': 893,\n",
       " 'sand': 894,\n",
       " 'wooden': 895,\n",
       " 'spades': 896,\n",
       " 'lodging': 897,\n",
       " 'houses': 898,\n",
       " 'station': 899,\n",
       " 'wept': 900,\n",
       " 'punished': 901,\n",
       " 'drowned': 902,\n",
       " 'splashing': 903,\n",
       " 'nearer': 904,\n",
       " 'walrus': 905,\n",
       " 'hippopotamus': 906,\n",
       " 'harm': 907,\n",
       " 'brother': 908,\n",
       " 'latin': 909,\n",
       " 'grammar': 910,\n",
       " 'inquisitively': 911,\n",
       " 'wink': 912,\n",
       " 'daresay': 913,\n",
       " 'clear': 914,\n",
       " 'notion': 915,\n",
       " 'ago': 916,\n",
       " 'ou': 917,\n",
       " 'est': 918,\n",
       " 'chatte': 919,\n",
       " 'sentence': 920,\n",
       " 'leap': 921,\n",
       " 'quiver': 922,\n",
       " 'fright': 923,\n",
       " 'animal': 924,\n",
       " 'feelings': 925,\n",
       " 'shrill': 926,\n",
       " 'passionate': 927,\n",
       " 'soothing': 928,\n",
       " 'yet': 929,\n",
       " 'quiet': 930,\n",
       " 'lazily': 931,\n",
       " 'sits': 932,\n",
       " 'purring': 933,\n",
       " 'nicely': 934,\n",
       " 'fire': 935,\n",
       " 'licking': 936,\n",
       " 'washing': 937,\n",
       " 'soft': 938,\n",
       " 'bristling': 939,\n",
       " 'family': 940,\n",
       " 'hated': 941,\n",
       " 'nasty': 942,\n",
       " 'vulgar': 943,\n",
       " 'conversation': 944,\n",
       " 'dog': 945,\n",
       " 'eyed': 946,\n",
       " 'terrier': 947,\n",
       " 'curly': 948,\n",
       " 'brown': 949,\n",
       " 'throw': 950,\n",
       " 'dinner': 951,\n",
       " 'belongs': 952,\n",
       " 'farmer': 953,\n",
       " 'hundred': 954,\n",
       " 'pounds': 955,\n",
       " 'kills': 956,\n",
       " 'rats': 957,\n",
       " 'sorrowful': 958,\n",
       " 'commotion': 959,\n",
       " 'softly': 960,\n",
       " 'pale': 961,\n",
       " 'passion': 962,\n",
       " 'creatures': 963,\n",
       " 'iii': 964,\n",
       " 'assembled': 965,\n",
       " 'draggled': 966,\n",
       " 'feathers': 967,\n",
       " 'clinging': 968,\n",
       " 'dripping': 969,\n",
       " 'cross': 970,\n",
       " 'uncomfortable': 971,\n",
       " 'consultation': 972,\n",
       " 'familiarly': 973,\n",
       " 'known': 974,\n",
       " 'argument': 975,\n",
       " 'sulky': 976,\n",
       " 'older': 977,\n",
       " 'allow': 978,\n",
       " 'knowing': 979,\n",
       " 'positively': 980,\n",
       " 'refused': 981,\n",
       " 'authority': 982,\n",
       " 'fixed': 983,\n",
       " 'cold': 984,\n",
       " 'ahem': 985,\n",
       " 'important': 986,\n",
       " 'driest': 987,\n",
       " 'whose': 988,\n",
       " 'favoured': 989,\n",
       " 'pope': 990,\n",
       " 'submitted': 991,\n",
       " 'leaders': 992,\n",
       " 'accustomed': 993,\n",
       " 'usurpation': 994,\n",
       " 'conquest': 995,\n",
       " 'ugh': 996,\n",
       " 'shiver': 997,\n",
       " 'frowning': 998,\n",
       " 'politely': 999,\n",
       " 'proceed': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import codecs, scipy.sparse, numpy\n",
    "\n",
    "#create co-occurrence matrix\n",
    "def create_cooccurrence_matrix(filename,window_size):\n",
    "    vocabulary={}\n",
    "    data=[]\n",
    "    row=[]\n",
    "    col=[]\n",
    "    for sentence in codecs.open(filename,\"r\",\"utf-8\"):\n",
    "        sentence=sentence.strip()\n",
    "        tokens=[token for token in corpus]\n",
    "        for pos,token in enumerate(tokens):\n",
    "            i=vocabulary.setdefault(token,len(vocabulary))\n",
    "            start=max(0,pos-window_size_corpus)\n",
    "            end=min(len(tokens),pos+window_size_corpus+1)\n",
    "            for pos2 in xrange(start,end):\n",
    "                if pos2==pos: \n",
    "                    continue\n",
    "                j=vocabulary.setdefault(tokens[pos2],len(vocabulary))\n",
    "                data.append(1.); row.append(i); col.append(j);\n",
    "    cooccurrence_matrix=sparse.coo_matrix((data,(row,col)))\n",
    "    return vocabulary,cooccurrence_matrix\n",
    "\n",
    "#create_cooccurrence_matrix('alice.txt',corpus,window_size)\n",
    "#print(corpus)\n",
    "#coo=numpy.zeros((V, V))\n",
    "#coo=[[0 for x in range(V)] for y in range(V)] \n",
    "vocabulary={}\n",
    "data=[]\n",
    "row=[]\n",
    "col=[]\n",
    "for sentence in corpus:\n",
    "    for pos,word in enumerate(sentence):\n",
    "        i=vocabulary.setdefault(word,len(vocabulary))\n",
    "        start=max(0,pos-window_size_corpus)\n",
    "        end=min(len(sentence),pos+window_size_corpus+1)\n",
    "        for pos2 in range(start,end):\n",
    "            if pos2==pos: \n",
    "                continue\n",
    "            j=vocabulary.setdefault(sentence[pos2],len(vocabulary))\n",
    "            if(j!=i):\n",
    "                data.append(1.); row.append(i); col.append(j);\n",
    "            \n",
    "cooccurrence_matrix=scipy.sparse.coo_matrix((data,(row,col)))\n",
    "#print(cooccurrence_matrix.T * cooccurrence_matrix)\n",
    "print(cooccurrence_matrix.toarray())\n",
    "#print(cooccurrence_matrix.toarray()[1180][0])\n",
    "tokenizer.word_index\n",
    "#The dog chased the cat away from the garden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "87\n",
      "63\n",
      "[[0.48967328]]\n",
      "0.48967327730053256\n",
      "[[0.08862668]]\n",
      "0.08862667920586557\n",
      "[[0.13969969]]\n",
      "0.1396996890567782\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "import math\n",
    "def cos_sim(a,b):\n",
    "    sum_c=0;len_c=len(a);sum_as=0;sum_bs=0;\n",
    "    for ii in range(len_c):\n",
    "        sum_c=sum_c+(a[ii]*b[ii])\n",
    "        sum_as=sum_as+(a[ii]*a[ii])\n",
    "        sum_bs=sum_bs+(b[ii]*b[ii])\n",
    "    return sum_c/(math.sqrt(sum_as)*(math.sqrt(sum_bs)))\n",
    "\n",
    "\n",
    "print(tokenizer.word_index['alice'])  #11\n",
    "print(tokenizer.word_index['dinah'])  #87\n",
    "print(tokenizer.word_index['rabbit']) #63\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cooc=cooccurrence_matrix.toarray()\n",
    "tf_cooc=numpy.zeros((V-1, V-1))\n",
    "for i, sentence in enumerate(cooc):\n",
    "    sumf=0\n",
    "    for wordf in sentence:\n",
    "        sumf+=wordf\n",
    "    for j,wordf in enumerate(sentence):\n",
    "        if(sumf>0):\n",
    "            tf_cooc[i][j]=cooc[i][j]/sumf\n",
    "        else:\n",
    "            continue\n",
    "#print(tf_cooc)\n",
    "\n",
    "#print(len(cooc))\n",
    "#print(len(cooc[10]))\n",
    "#print(V)\n",
    "#print(cooc[0])\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#print(len(tf_cooc[10:11][0]))\n",
    "#print(len(cooc[V-2]))\n",
    "#print(cooc.shape)\n",
    "#print(tf_cooc.shape)\n",
    "print(cosine_similarity(tf_cooc[10:11], tf_cooc[86:87]))\n",
    "print(cos_sim(tf_cooc[10],tf_cooc[86]))                  \n",
    "print(cosine_similarity(tf_cooc[10:11], tf_cooc[62:63]))\n",
    "print(cos_sim(tf_cooc[10],tf_cooc[62]))\n",
    "print(cosine_similarity(tf_cooc[86:87], tf_cooc[62:63]))\n",
    "print(cos_sim(tf_cooc[86],tf_cooc[62]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  34  16 161  82   2]\n",
      "closest word= t, had, question, say, to\n",
      "0.7033898864296415\n",
      "0.6995182846911174\n",
      "0.6726422527548841\n",
      "0.6685274041392557\n",
      "0.6544770424322105\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "maxid=0\n",
    "maxval=0\n",
    "simi=np.zeros(V-1)\n",
    "for i in range(V-1):\n",
    "    simi[i]=cosine_similarity(tf_cooc[10:11], tf_cooc[i:i+1])[0][0]\n",
    "indices=simi.argsort()[-6:][::-1]\n",
    "print(indices)\n",
    "print(\"closest word= {}, {}, {}, {}, {}\".format(list(tokenizer.word_index)[indices[1]],list(tokenizer.word_index)[indices[2]],list(tokenizer.word_index)[indices[3]],list(tokenizer.word_index)[indices[4]],list(tokenizer.word_index)[indices[5]]))            \n",
    "for i in range(1,6):\n",
    "    print(cosine_similarity(tf_cooc[10:11], tf_cooc[indices[i]:indices[i]+1])[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1182 is out of bounds for axis 0 with size 1182",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-7c74821cd022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1182 is out of bounds for axis 0 with size 1182"
     ]
    }
   ],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = cooccurrence_matrix.toarray()\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6559, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data for cbow\n",
    "import math\n",
    "reversed_index = {v:k for k,v in tokenizer.word_index.items()}\n",
    "\n",
    "def prep_cbow_data(corpus=corpus,window_size=window_size,V=V):\n",
    "    flat = [word for line in corpus for word in line]\n",
    "    vectors = []\n",
    "    L = len(flat)\n",
    "    start = int(window_size)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(start,L-start):\n",
    "        context_before = flat[i-start:i]\n",
    "        target = np.zeros(V,dtype=int)\n",
    "        target[flat[i]-1] = 1\n",
    "        context_after = flat[i+1:i+1+start]\n",
    "        context = context_before + context_after\n",
    "        #onehot_context = []\n",
    "        #for word in context:\n",
    "        #    vector = np.zeros(V,dtype=int)\n",
    "        #    vector[word-1] = 1\n",
    "        #    onehot_context.append(vector)\n",
    "            \n",
    "        if len(context) == 4:\n",
    "            #onehot_context = np.asarray(onehot_context)\n",
    "            x.append(context)\n",
    "            y.append(target)\n",
    "        \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "prep_cbow_data()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size X: 6559,size Y:6559\n",
      "(6559, 4) (6559, 1183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=1183, output_dim=100, input_length=4, embeddings_initializer=\"glorot_uniform\")`\n",
      "  \n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1183, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model\n",
    "from keras.layers import Flatten\n",
    "X,Y = prep_cbow_data()\n",
    "features = len(X)\n",
    "print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "print(X.shape,Y.shape)\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "cbow.add(Flatten())\n",
    "cbow.add(Dense(V, init='glorot_uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5903 samples, validate on 656 samples\n",
      "Epoch 1/100\n",
      "5903/5903 [==============================] - 3s 547us/step - loss: 7.0495 - val_loss: 7.0279\n",
      "Epoch 2/100\n",
      "5903/5903 [==============================] - 3s 496us/step - loss: 6.9876 - val_loss: 6.9825\n",
      "Epoch 3/100\n",
      "5903/5903 [==============================] - 3s 485us/step - loss: 6.9145 - val_loss: 6.9132\n",
      "Epoch 4/100\n",
      "5903/5903 [==============================] - 3s 488us/step - loss: 6.8049 - val_loss: 6.7977\n",
      "Epoch 5/100\n",
      "5903/5903 [==============================] - 3s 480us/step - loss: 6.6370 - val_loss: 6.6216\n",
      "Epoch 6/100\n",
      "5903/5903 [==============================] - 3s 470us/step - loss: 6.4109 - val_loss: 6.4074\n",
      "Epoch 7/100\n",
      "5903/5903 [==============================] - 3s 453us/step - loss: 6.1725 - val_loss: 6.2239\n",
      "Epoch 8/100\n",
      "5903/5903 [==============================] - 3s 464us/step - loss: 5.9754 - val_loss: 6.0881\n",
      "Epoch 9/100\n",
      "5903/5903 [==============================] - 3s 457us/step - loss: 5.8161 - val_loss: 5.9828\n",
      "Epoch 10/100\n",
      "5903/5903 [==============================] - 3s 466us/step - loss: 5.6796 - val_loss: 5.8917\n",
      "Epoch 11/100\n",
      "5903/5903 [==============================] - 3s 482us/step - loss: 5.5570 - val_loss: 5.8114\n",
      "Epoch 12/100\n",
      "5903/5903 [==============================] - 3s 473us/step - loss: 5.4442 - val_loss: 5.7403\n",
      "Epoch 13/100\n",
      "5903/5903 [==============================] - 3s 478us/step - loss: 5.3385 - val_loss: 5.6753\n",
      "Epoch 14/100\n",
      "5903/5903 [==============================] - 3s 469us/step - loss: 5.2390 - val_loss: 5.6169\n",
      "Epoch 15/100\n",
      "5903/5903 [==============================] - 3s 475us/step - loss: 5.1443 - val_loss: 5.5632\n",
      "Epoch 16/100\n",
      "5903/5903 [==============================] - 3s 476us/step - loss: 5.0538 - val_loss: 5.5149\n",
      "Epoch 17/100\n",
      "5903/5903 [==============================] - 3s 467us/step - loss: 4.9665 - val_loss: 5.4677\n",
      "Epoch 18/100\n",
      "5903/5903 [==============================] - 3s 473us/step - loss: 4.8824 - val_loss: 5.4241\n",
      "Epoch 19/100\n",
      "5903/5903 [==============================] - 3s 472us/step - loss: 4.8022 - val_loss: 5.3846\n",
      "Epoch 20/100\n",
      "5903/5903 [==============================] - 3s 474us/step - loss: 4.7246 - val_loss: 5.3489\n",
      "Epoch 21/100\n",
      "5903/5903 [==============================] - 3s 487us/step - loss: 4.6496 - val_loss: 5.3148\n",
      "Epoch 22/100\n",
      "5903/5903 [==============================] - 3s 524us/step - loss: 4.5766 - val_loss: 5.2840\n",
      "Epoch 23/100\n",
      "5903/5903 [==============================] - 3s 506us/step - loss: 4.5054 - val_loss: 5.2552\n",
      "Epoch 24/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 4.4365 - val_loss: 5.2268\n",
      "Epoch 25/100\n",
      "5903/5903 [==============================] - 3s 520us/step - loss: 4.3692 - val_loss: 5.2011\n",
      "Epoch 26/100\n",
      "5903/5903 [==============================] - 3s 502us/step - loss: 4.3031 - val_loss: 5.1779\n",
      "Epoch 27/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 4.2391 - val_loss: 5.1539\n",
      "Epoch 28/100\n",
      "5903/5903 [==============================] - 3s 501us/step - loss: 4.1764 - val_loss: 5.1340\n",
      "Epoch 29/100\n",
      "5903/5903 [==============================] - 3s 521us/step - loss: 4.1147 - val_loss: 5.1111\n",
      "Epoch 30/100\n",
      "5903/5903 [==============================] - 3s 517us/step - loss: 4.0545 - val_loss: 5.0939\n",
      "Epoch 31/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 3.9958 - val_loss: 5.0774\n",
      "Epoch 32/100\n",
      "5903/5903 [==============================] - 3s 512us/step - loss: 3.9370 - val_loss: 5.0615\n",
      "Epoch 33/100\n",
      "5903/5903 [==============================] - 3s 502us/step - loss: 3.8795 - val_loss: 5.0446\n",
      "Epoch 34/100\n",
      "5903/5903 [==============================] - 3s 508us/step - loss: 3.8226 - val_loss: 5.0320\n",
      "Epoch 35/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 3.7672 - val_loss: 5.0181\n",
      "Epoch 36/100\n",
      "5903/5903 [==============================] - 3s 500us/step - loss: 3.7129 - val_loss: 5.0026\n",
      "Epoch 37/100\n",
      "5903/5903 [==============================] - 3s 515us/step - loss: 3.6593 - val_loss: 4.9881\n",
      "Epoch 38/100\n",
      "5903/5903 [==============================] - 3s 499us/step - loss: 3.6065 - val_loss: 4.9766\n",
      "Epoch 39/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 3.5535 - val_loss: 4.9656\n",
      "Epoch 40/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 3.5024 - val_loss: 4.9569\n",
      "Epoch 41/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.4515 - val_loss: 4.9470\n",
      "Epoch 42/100\n",
      "5903/5903 [==============================] - 3s 508us/step - loss: 3.4017 - val_loss: 4.9332\n",
      "Epoch 43/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.3526 - val_loss: 4.9240\n",
      "Epoch 44/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 3.3048 - val_loss: 4.9146\n",
      "Epoch 45/100\n",
      "5903/5903 [==============================] - 3s 519us/step - loss: 3.2574 - val_loss: 4.9087\n",
      "Epoch 46/100\n",
      "5903/5903 [==============================] - 3s 503us/step - loss: 3.2104 - val_loss: 4.9015\n",
      "Epoch 47/100\n",
      "5903/5903 [==============================] - 3s 512us/step - loss: 3.1638 - val_loss: 4.8949\n",
      "Epoch 48/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.1182 - val_loss: 4.8887\n",
      "Epoch 49/100\n",
      "5903/5903 [==============================] - 3s 515us/step - loss: 3.0731 - val_loss: 4.8816\n",
      "Epoch 50/100\n",
      "5903/5903 [==============================] - 3s 513us/step - loss: 3.0292 - val_loss: 4.8751\n",
      "Epoch 51/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 2.9854 - val_loss: 4.8706\n",
      "Epoch 52/100\n",
      "5903/5903 [==============================] - 3s 511us/step - loss: 2.9428 - val_loss: 4.8660\n",
      "Epoch 53/100\n",
      "5903/5903 [==============================] - 3s 510us/step - loss: 2.8997 - val_loss: 4.8603\n",
      "Epoch 54/100\n",
      "5903/5903 [==============================] - 3s 520us/step - loss: 2.8585 - val_loss: 4.8569\n",
      "Epoch 55/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 2.8173 - val_loss: 4.8546\n",
      "Epoch 56/100\n",
      "5903/5903 [==============================] - 3s 501us/step - loss: 2.7764 - val_loss: 4.8481\n",
      "Epoch 57/100\n",
      "5903/5903 [==============================] - 3s 510us/step - loss: 2.7371 - val_loss: 4.8446\n",
      "Epoch 58/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 2.6966 - val_loss: 4.8403\n",
      "Epoch 59/100\n",
      "5903/5903 [==============================] - 3s 506us/step - loss: 2.6579 - val_loss: 4.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1baf76451d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "from keras import callbacks\n",
    "epochs=100\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "cbow.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#get x and y's for data\n",
    "x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "\n",
    "#save the preprocessed data of Skipgram\n",
    "f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "for input,outcome  in zip(x,y):\n",
    "    input = np.concatenate(input)\n",
    "    f.write(\" \".join(map(str, list(input))))\n",
    "    f.write(\",\")\n",
    "    outcome = np.concatenate(outcome)\n",
    "    f.write(\" \".join(map(str,list(outcome))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file():\n",
    "    f = open('data_skipgram.txt' ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram model\n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42011.7700715065\n",
      "1 38353.8222258091\n",
      "2 38931.23595356941\n",
      "3 39340.806143045425\n",
      "4 39514.309755563736\n",
      "5 39681.91601896286\n",
      "6 39847.13162755966\n",
      "7 40027.34559297562\n",
      "8 40214.910954236984\n",
      "9 40403.25219488144\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=1183, output_dim=100, input_length=4, embeddings_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1183, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "cbow2 = Sequential()\n",
    "cbow2.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "cbow2.add(Flatten())\n",
    "cbow2.add(Dense(V,activation='relu'))\n",
    "cbow2.add(Dense(V, init='glorot_uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n",
    "cbow2.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5903 samples, validate on 656 samples\n",
      "Epoch 1/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 6.7766 - val_loss: 6.1094\n",
      "Epoch 2/100\n",
      "5903/5903 [==============================] - 9s 1ms/step - loss: 5.8327 - val_loss: 5.7606\n",
      "Epoch 3/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 5.5145 - val_loss: 5.5747\n",
      "Epoch 4/100\n",
      "5903/5903 [==============================] - 8s 1ms/step - loss: 5.2639 - val_loss: 5.4150\n",
      "Epoch 5/100\n",
      "5903/5903 [==============================] - 8s 1ms/step - loss: 5.0215 - val_loss: 5.2781\n",
      "Epoch 6/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 4.7894 - val_loss: 5.1477\n",
      "Epoch 7/100\n",
      "5903/5903 [==============================] - 11s 2ms/step - loss: 4.5724 - val_loss: 5.0567\n",
      "Epoch 8/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 4.3716 - val_loss: 4.9879\n",
      "Epoch 9/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 4.1812 - val_loss: 4.9395\n",
      "Epoch 10/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.9998 - val_loss: 4.8841\n",
      "Epoch 11/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.8235 - val_loss: 4.8715\n",
      "Epoch 12/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.6504 - val_loss: 4.8396\n",
      "Epoch 13/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.4827 - val_loss: 4.8171\n",
      "Epoch 14/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 3.3142 - val_loss: 4.8193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bafacd8668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model for CBOW + dense\n",
    "epochs=100\n",
    "\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "cbow2.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "skipgram2 = Sequential()\n",
    "skipgram2.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram2.add(Reshape((dim, )))\n",
    "skipgram2.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "skipgram2.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram2.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41944.31506729126\n",
      "1 38392.68016695976\n",
      "2 39111.658846616745\n",
      "3 39808.2161591053\n",
      "4 40187.521782159805\n",
      "5 40399.22357773781\n",
      "6 40525.47047138214\n",
      "7 40603.02894616127\n",
      "8 40627.425698041916\n",
      "9 40142.643817186356\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram2.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram2.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram2.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"tes\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'the'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f746c5319173>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"glove_converted.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mglove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1002\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'the'"
     ]
    }
   ],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "import gensim\n",
    "#path = \"tes\\glove.6B\\glove.6B.300d_converted.txt\"\n",
    "path = \"tes\\glove.6B.300d.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
