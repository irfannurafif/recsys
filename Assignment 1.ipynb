{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'#'analogy_alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 8. ... 0. 0. 0.]\n",
      " [1. 8. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'she': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'it': 7,\n",
       " 'of': 8,\n",
       " 'was': 9,\n",
       " 'in': 10,\n",
       " 'alice': 11,\n",
       " 'you': 12,\n",
       " 'that': 13,\n",
       " 'her': 14,\n",
       " 'as': 15,\n",
       " 'said': 16,\n",
       " 'had': 17,\n",
       " 'for': 18,\n",
       " 'but': 19,\n",
       " 'be': 20,\n",
       " 'on': 21,\n",
       " 'all': 22,\n",
       " 'with': 23,\n",
       " 'little': 24,\n",
       " 'mouse': 25,\n",
       " 'down': 26,\n",
       " 'very': 27,\n",
       " 'this': 28,\n",
       " 'not': 29,\n",
       " 'so': 30,\n",
       " 'out': 31,\n",
       " 'if': 32,\n",
       " 'is': 33,\n",
       " 'at': 34,\n",
       " 't': 35,\n",
       " 's': 36,\n",
       " 'll': 37,\n",
       " 'how': 38,\n",
       " 'they': 39,\n",
       " 'about': 40,\n",
       " 'herself': 41,\n",
       " 'me': 42,\n",
       " 'up': 43,\n",
       " 'what': 44,\n",
       " 'way': 45,\n",
       " 'when': 46,\n",
       " 'like': 47,\n",
       " 'one': 48,\n",
       " 'do': 49,\n",
       " 'no': 50,\n",
       " 'oh': 51,\n",
       " 'went': 52,\n",
       " 'thought': 53,\n",
       " 'again': 54,\n",
       " 'there': 55,\n",
       " 'see': 56,\n",
       " 'or': 57,\n",
       " 'could': 58,\n",
       " 'would': 59,\n",
       " 'think': 60,\n",
       " 'them': 61,\n",
       " 'know': 62,\n",
       " 'rabbit': 63,\n",
       " 'dear': 64,\n",
       " 'were': 65,\n",
       " 'time': 66,\n",
       " 'get': 67,\n",
       " 'here': 68,\n",
       " 'must': 69,\n",
       " 'my': 70,\n",
       " 'by': 71,\n",
       " 'into': 72,\n",
       " 'found': 73,\n",
       " 'such': 74,\n",
       " 'began': 75,\n",
       " 'soon': 76,\n",
       " 'm': 77,\n",
       " 'quite': 78,\n",
       " 'then': 79,\n",
       " 'off': 80,\n",
       " 'now': 81,\n",
       " 'go': 82,\n",
       " 'say': 83,\n",
       " 'have': 84,\n",
       " 'which': 85,\n",
       " 'come': 86,\n",
       " 'dinah': 87,\n",
       " 'your': 88,\n",
       " 'thing': 89,\n",
       " 'dodo': 90,\n",
       " 'much': 91,\n",
       " 'shall': 92,\n",
       " 'things': 93,\n",
       " 'long': 94,\n",
       " 'door': 95,\n",
       " 'who': 96,\n",
       " 'can': 97,\n",
       " 'once': 98,\n",
       " 'did': 99,\n",
       " 'over': 100,\n",
       " 'feet': 101,\n",
       " 'after': 102,\n",
       " 'wonder': 103,\n",
       " 'first': 104,\n",
       " 'let': 105,\n",
       " 'are': 106,\n",
       " 'cats': 107,\n",
       " 'round': 108,\n",
       " 'poor': 109,\n",
       " 'back': 110,\n",
       " 'he': 111,\n",
       " 'nothing': 112,\n",
       " 'seemed': 113,\n",
       " 'its': 114,\n",
       " 'never': 115,\n",
       " 'going': 116,\n",
       " 'great': 117,\n",
       " 'got': 118,\n",
       " 'ever': 119,\n",
       " 'table': 120,\n",
       " 'any': 121,\n",
       " 'only': 122,\n",
       " 'more': 123,\n",
       " 'cried': 124,\n",
       " 'gloves': 125,\n",
       " 'fan': 126,\n",
       " 'well': 127,\n",
       " 'white': 128,\n",
       " 'looked': 129,\n",
       " 'before': 130,\n",
       " 'moment': 131,\n",
       " 'why': 132,\n",
       " 'an': 133,\n",
       " 'eat': 134,\n",
       " 'however': 135,\n",
       " 'sure': 136,\n",
       " 'pool': 137,\n",
       " 'getting': 138,\n",
       " 'took': 139,\n",
       " 'either': 140,\n",
       " 'just': 141,\n",
       " 'upon': 142,\n",
       " 've': 143,\n",
       " 'through': 144,\n",
       " 'wish': 145,\n",
       " 'tell': 146,\n",
       " 'hall': 147,\n",
       " 'key': 148,\n",
       " 'half': 149,\n",
       " 'find': 150,\n",
       " 'his': 151,\n",
       " 'use': 152,\n",
       " 'large': 153,\n",
       " 'look': 154,\n",
       " 'too': 155,\n",
       " 'house': 156,\n",
       " 'good': 157,\n",
       " 'right': 158,\n",
       " 'words': 159,\n",
       " 'am': 160,\n",
       " 'might': 161,\n",
       " 'question': 162,\n",
       " 'came': 163,\n",
       " 'dry': 164,\n",
       " 'away': 165,\n",
       " 'been': 166,\n",
       " 'small': 167,\n",
       " 'high': 168,\n",
       " 'garden': 169,\n",
       " 'tears': 170,\n",
       " 'don': 171,\n",
       " 'won': 172,\n",
       " 'doesn': 173,\n",
       " 'without': 174,\n",
       " 'suddenly': 175,\n",
       " 'eyes': 176,\n",
       " 'some': 177,\n",
       " 'put': 178,\n",
       " 'sort': 179,\n",
       " 'their': 180,\n",
       " 'rather': 181,\n",
       " 'talking': 182,\n",
       " 'saying': 183,\n",
       " 'felt': 184,\n",
       " 'hand': 185,\n",
       " 'low': 186,\n",
       " 'trying': 187,\n",
       " 'golden': 188,\n",
       " 'head': 189,\n",
       " 'indeed': 190,\n",
       " 'will': 191,\n",
       " 'while': 192,\n",
       " 'same': 193,\n",
       " 'try': 194,\n",
       " 'beg': 195,\n",
       " 'tone': 196,\n",
       " 'our': 197,\n",
       " 'd': 198,\n",
       " 'called': 199,\n",
       " 'birds': 200,\n",
       " 'lory': 201,\n",
       " 'race': 202,\n",
       " 'looking': 203,\n",
       " 'course': 204,\n",
       " 'book': 205,\n",
       " 'made': 206,\n",
       " 'ran': 207,\n",
       " 'seen': 208,\n",
       " 'fell': 209,\n",
       " 'next': 210,\n",
       " 'tried': 211,\n",
       " 'make': 212,\n",
       " 'anything': 213,\n",
       " 'four': 214,\n",
       " 'nice': 215,\n",
       " 'people': 216,\n",
       " 'please': 217,\n",
       " 'should': 218,\n",
       " 'remember': 219,\n",
       " 'turned': 220,\n",
       " 'other': 221,\n",
       " 'glass': 222,\n",
       " 'than': 223,\n",
       " 'marked': 224,\n",
       " 'best': 225,\n",
       " 'sat': 226,\n",
       " 'two': 227,\n",
       " 'english': 228,\n",
       " 'pair': 229,\n",
       " 'heard': 230,\n",
       " 'kid': 231,\n",
       " 'ready': 232,\n",
       " 'voice': 233,\n",
       " 'queer': 234,\n",
       " 'changed': 235,\n",
       " 'swam': 236,\n",
       " 'talk': 237,\n",
       " 'offended': 238,\n",
       " 'we': 239,\n",
       " 'party': 240,\n",
       " 'prizes': 241,\n",
       " 'chapter': 242,\n",
       " 'hole': 243,\n",
       " 'tired': 244,\n",
       " 'having': 245,\n",
       " 'pictures': 246,\n",
       " 'own': 247,\n",
       " 'mind': 248,\n",
       " 'hot': 249,\n",
       " 'day': 250,\n",
       " 'pocket': 251,\n",
       " 'hurried': 252,\n",
       " 'take': 253,\n",
       " 'another': 254,\n",
       " 'coming': 255,\n",
       " 'from': 256,\n",
       " 'fall': 257,\n",
       " 'even': 258,\n",
       " 'near': 259,\n",
       " 'idea': 260,\n",
       " 'among': 261,\n",
       " 'didn': 262,\n",
       " 'ask': 263,\n",
       " 'air': 264,\n",
       " 'perhaps': 265,\n",
       " 'else': 266,\n",
       " 'cat': 267,\n",
       " 'afraid': 268,\n",
       " 'bats': 269,\n",
       " 'rate': 270,\n",
       " 'those': 271,\n",
       " 'really': 272,\n",
       " 'bottle': 273,\n",
       " 'children': 274,\n",
       " 'enough': 275,\n",
       " 'anxiously': 276,\n",
       " 'surprised': 277,\n",
       " 'speak': 278,\n",
       " 'hastily': 279,\n",
       " 'duchess': 280,\n",
       " 'kept': 281,\n",
       " 'everything': 282,\n",
       " 'mabel': 283,\n",
       " 'capital': 284,\n",
       " 'tail': 285,\n",
       " 'something': 286,\n",
       " 'william': 287,\n",
       " 'pardon': 288,\n",
       " 'crowded': 289,\n",
       " 'whole': 290,\n",
       " 'last': 291,\n",
       " 'old': 292,\n",
       " 'him': 293,\n",
       " 'thimble': 294,\n",
       " 'close': 295,\n",
       " 'hear': 296,\n",
       " 'itself': 297,\n",
       " 'late': 298,\n",
       " 'ought': 299,\n",
       " 'under': 300,\n",
       " 'world': 301,\n",
       " 'deep': 302,\n",
       " 'slowly': 303,\n",
       " 'happen': 304,\n",
       " 'dark': 305,\n",
       " 'noticed': 306,\n",
       " 'somebody': 307,\n",
       " 'home': 308,\n",
       " 'end': 309,\n",
       " 'many': 310,\n",
       " 'fallen': 311,\n",
       " 'several': 312,\n",
       " 'lessons': 313,\n",
       " 'still': 314,\n",
       " 'distance': 315,\n",
       " 'seem': 316,\n",
       " 'walk': 317,\n",
       " 'heads': 318,\n",
       " 'name': 319,\n",
       " 'ma': 320,\n",
       " 'fancy': 321,\n",
       " 're': 322,\n",
       " 'manage': 323,\n",
       " 'night': 324,\n",
       " 'mice': 325,\n",
       " 'answer': 326,\n",
       " 'passage': 327,\n",
       " 'sight': 328,\n",
       " 'behind': 329,\n",
       " 'every': 330,\n",
       " 'three': 331,\n",
       " 'alas': 332,\n",
       " 'inches': 333,\n",
       " 'along': 334,\n",
       " 'bright': 335,\n",
       " 'happened': 336,\n",
       " 'few': 337,\n",
       " 'drink': 338,\n",
       " 'hurry': 339,\n",
       " 'poison': 340,\n",
       " 'finger': 341,\n",
       " 'almost': 342,\n",
       " 'certain': 343,\n",
       " 'finished': 344,\n",
       " 'curious': 345,\n",
       " 'face': 346,\n",
       " 'candle': 347,\n",
       " 'generally': 348,\n",
       " 'remembered': 349,\n",
       " 'box': 350,\n",
       " 'left': 351,\n",
       " 'person': 352,\n",
       " 'eye': 353,\n",
       " 'lying': 354,\n",
       " 'cake': 355,\n",
       " 'life': 356,\n",
       " 'deal': 357,\n",
       " 'dropped': 358,\n",
       " 'times': 359,\n",
       " 'seems': 360,\n",
       " 'stay': 361,\n",
       " 'being': 362,\n",
       " 'sudden': 363,\n",
       " 'cause': 364,\n",
       " 'water': 365,\n",
       " 'o': 366,\n",
       " 'history': 367,\n",
       " 'trembling': 368,\n",
       " 'always': 369,\n",
       " 'dogs': 370,\n",
       " 'eagerly': 371,\n",
       " 'fetch': 372,\n",
       " 'us': 373,\n",
       " 'duck': 374,\n",
       " 'eaglet': 375,\n",
       " 'caucus': 376,\n",
       " 'tale': 377,\n",
       " 'replied': 378,\n",
       " 'turning': 379,\n",
       " 'sister': 380,\n",
       " 'bank': 381,\n",
       " 'considering': 382,\n",
       " 'feel': 383,\n",
       " 'sleepy': 384,\n",
       " 'stupid': 385,\n",
       " 'whether': 386,\n",
       " 'making': 387,\n",
       " 'worth': 388,\n",
       " 'trouble': 389,\n",
       " 'remarkable': 390,\n",
       " 'natural': 391,\n",
       " 'watch': 392,\n",
       " 'waistcoat': 393,\n",
       " 'started': 394,\n",
       " 'across': 395,\n",
       " 'falling': 396,\n",
       " 'filled': 397,\n",
       " 'cupboards': 398,\n",
       " 'shelves': 399,\n",
       " 'saw': 400,\n",
       " 'jar': 401,\n",
       " 'fear': 402,\n",
       " 'managed': 403,\n",
       " 'wouldn': 404,\n",
       " 'top': 405,\n",
       " 'likely': 406,\n",
       " 'miles': 407,\n",
       " 'aloud': 408,\n",
       " 'somewhere': 409,\n",
       " 'earth': 410,\n",
       " 'though': 411,\n",
       " 'opportunity': 412,\n",
       " 'knowledge': 413,\n",
       " 'listen': 414,\n",
       " 'yes': 415,\n",
       " 'latitude': 416,\n",
       " 'longitude': 417,\n",
       " 'funny': 418,\n",
       " 'glad': 419,\n",
       " 'new': 420,\n",
       " 'spoke': 421,\n",
       " 'girl': 422,\n",
       " 'asking': 423,\n",
       " 'miss': 424,\n",
       " 'catch': 425,\n",
       " 'bat': 426,\n",
       " 'sometimes': 427,\n",
       " 'matter': 428,\n",
       " 'begun': 429,\n",
       " 'walking': 430,\n",
       " 'thump': 431,\n",
       " 'bit': 432,\n",
       " 'hurt': 433,\n",
       " 'lost': 434,\n",
       " 'corner': 435,\n",
       " 'ears': 436,\n",
       " 'whiskers': 437,\n",
       " 'row': 438,\n",
       " 'roof': 439,\n",
       " 'doors': 440,\n",
       " 'side': 441,\n",
       " 'walked': 442,\n",
       " 'sadly': 443,\n",
       " 'middle': 444,\n",
       " 'tiny': 445,\n",
       " 'opened': 446,\n",
       " 'led': 447,\n",
       " 'larger': 448,\n",
       " 'shut': 449,\n",
       " 'telescope': 450,\n",
       " 'knew': 451,\n",
       " 'waiting': 452,\n",
       " 'hoping': 453,\n",
       " 'rules': 454,\n",
       " 'shutting': 455,\n",
       " 'certainly': 456,\n",
       " 'beautifully': 457,\n",
       " 'hold': 458,\n",
       " 'usually': 459,\n",
       " 'forgotten': 460,\n",
       " 'taste': 461,\n",
       " 'finding': 462,\n",
       " 'fact': 463,\n",
       " 'feeling': 464,\n",
       " 'size': 465,\n",
       " 'waited': 466,\n",
       " 'minutes': 467,\n",
       " 'altogether': 468,\n",
       " 'reach': 469,\n",
       " 'crying': 470,\n",
       " 'sharply': 471,\n",
       " 'leave': 472,\n",
       " 'minute': 473,\n",
       " 'gave': 474,\n",
       " 'severely': 475,\n",
       " 'against': 476,\n",
       " 'child': 477,\n",
       " 'fond': 478,\n",
       " 'makes': 479,\n",
       " 'grow': 480,\n",
       " 'happens': 481,\n",
       " 'holding': 482,\n",
       " 'growing': 483,\n",
       " 'curiouser': 484,\n",
       " 'forgot': 485,\n",
       " 'far': 486,\n",
       " 'dears': 487,\n",
       " 'give': 488,\n",
       " 'sending': 489,\n",
       " 'foot': 490,\n",
       " 'nonsense': 491,\n",
       " 'nine': 492,\n",
       " 'cry': 493,\n",
       " 'yourself': 494,\n",
       " 'stop': 495,\n",
       " 'pattering': 496,\n",
       " 'trotting': 497,\n",
       " 'muttering': 498,\n",
       " 'help': 499,\n",
       " 'sir': 500,\n",
       " 'hard': 501,\n",
       " 'morning': 502,\n",
       " 'ah': 503,\n",
       " 'age': 504,\n",
       " 'hair': 505,\n",
       " 'ringlets': 506,\n",
       " 'mine': 507,\n",
       " 'sorts': 508,\n",
       " 'puzzling': 509,\n",
       " 'used': 510,\n",
       " 'paris': 511,\n",
       " 'rome': 512,\n",
       " 'doth': 513,\n",
       " 'hands': 514,\n",
       " 'alone': 515,\n",
       " 'done': 516,\n",
       " 'shrinking': 517,\n",
       " 'frightened': 518,\n",
       " 'change': 519,\n",
       " 'bad': 520,\n",
       " 'slipped': 521,\n",
       " 'salt': 522,\n",
       " 'sea': 523,\n",
       " 'case': 524,\n",
       " 'railway': 525,\n",
       " 'hadn': 526,\n",
       " 'suppose': 527,\n",
       " 'swimming': 528,\n",
       " 'speaking': 529,\n",
       " 'understand': 530,\n",
       " 'french': 531,\n",
       " 'conqueror': 532,\n",
       " 'lesson': 533,\n",
       " 'angry': 534,\n",
       " 'show': 535,\n",
       " 'paws': 536,\n",
       " 'nurse': 537,\n",
       " 'catching': 538,\n",
       " 'subject': 539,\n",
       " 'sit': 540,\n",
       " 'says': 541,\n",
       " 'useful': 542,\n",
       " 'shore': 543,\n",
       " 'hate': 544,\n",
       " 'animals': 545,\n",
       " 'fur': 546,\n",
       " 'wet': 547,\n",
       " 'better': 548,\n",
       " 'ring': 549,\n",
       " 'silence': 550,\n",
       " 'wanted': 551,\n",
       " 'edwin': 552,\n",
       " 'morcar': 553,\n",
       " 'earls': 554,\n",
       " 'mercia': 555,\n",
       " 'northumbria': 556,\n",
       " 'advisable': 557,\n",
       " 'meet': 558,\n",
       " 'melancholy': 559,\n",
       " 'solemnly': 560,\n",
       " 'explain': 561,\n",
       " 'running': 562,\n",
       " 'liked': 563,\n",
       " 'has': 564,\n",
       " 'chorus': 565,\n",
       " 'comfits': 566,\n",
       " 'speech': 567,\n",
       " 'caused': 568,\n",
       " 'sad': 569,\n",
       " 'fury': 570,\n",
       " 'trial': 571,\n",
       " 'jury': 572,\n",
       " 'judge': 573,\n",
       " 'finish': 574,\n",
       " 'story': 575,\n",
       " 'crab': 576,\n",
       " 'nobody': 577,\n",
       " 'ferrets': 578,\n",
       " 'hunting': 579,\n",
       " 'mary': 580,\n",
       " 'ann': 581,\n",
       " 'messages': 582,\n",
       " 'room': 583,\n",
       " 'beginning': 584,\n",
       " 'sitting': 585,\n",
       " 'twice': 586,\n",
       " 'peeped': 587,\n",
       " 'reading': 588,\n",
       " 'conversations': 589,\n",
       " 'pleasure': 590,\n",
       " 'daisy': 591,\n",
       " 'chain': 592,\n",
       " 'picking': 593,\n",
       " 'daisies': 594,\n",
       " 'pink': 595,\n",
       " 'nor': 596,\n",
       " 'afterwards': 597,\n",
       " 'occurred': 598,\n",
       " 'wondered': 599,\n",
       " 'actually': 600,\n",
       " 'flashed': 601,\n",
       " 'burning': 602,\n",
       " 'curiosity': 603,\n",
       " 'field': 604,\n",
       " 'fortunately': 605,\n",
       " 'pop': 606,\n",
       " 'hedge': 607,\n",
       " 'straight': 608,\n",
       " 'tunnel': 609,\n",
       " 'dipped': 610,\n",
       " 'stopping': 611,\n",
       " 'plenty': 612,\n",
       " 'sides': 613,\n",
       " 'maps': 614,\n",
       " 'hung': 615,\n",
       " 'pegs': 616,\n",
       " 'passed': 617,\n",
       " 'labelled': 618,\n",
       " 'orange': 619,\n",
       " 'marmalade': 620,\n",
       " 'disappointment': 621,\n",
       " 'empty': 622,\n",
       " 'drop': 623,\n",
       " 'killing': 624,\n",
       " 'past': 625,\n",
       " 'tumbling': 626,\n",
       " 'stairs': 627,\n",
       " 'brave': 628,\n",
       " 'true': 629,\n",
       " 'centre': 630,\n",
       " 'thousand': 631,\n",
       " 'learnt': 632,\n",
       " 'schoolroom': 633,\n",
       " 'showing': 634,\n",
       " 'practice': 635,\n",
       " 'grand': 636,\n",
       " 'presently': 637,\n",
       " 'downward': 638,\n",
       " 'antipathies': 639,\n",
       " 'listening': 640,\n",
       " 'sound': 641,\n",
       " 'word': 642,\n",
       " 'country': 643,\n",
       " 'zealand': 644,\n",
       " 'australia': 645,\n",
       " 'curtsey': 646,\n",
       " 'curtseying': 647,\n",
       " 'ignorant': 648,\n",
       " 'written': 649,\n",
       " 'hope': 650,\n",
       " 'saucer': 651,\n",
       " 'milk': 652,\n",
       " 'tea': 653,\n",
       " 'dreamy': 654,\n",
       " 'couldn': 655,\n",
       " 'dozing': 656,\n",
       " 'dream': 657,\n",
       " 'earnestly': 658,\n",
       " 'truth': 659,\n",
       " 'heap': 660,\n",
       " 'sticks': 661,\n",
       " 'leaves': 662,\n",
       " 'jumped': 663,\n",
       " 'overhead': 664,\n",
       " 'hurrying': 665,\n",
       " 'wind': 666,\n",
       " 'longer': 667,\n",
       " 'lit': 668,\n",
       " 'lamps': 669,\n",
       " 'hanging': 670,\n",
       " 'locked': 671,\n",
       " 'wondering': 672,\n",
       " 'legged': 673,\n",
       " 'solid': 674,\n",
       " 'except': 675,\n",
       " 'belong': 676,\n",
       " 'locks': 677,\n",
       " 'open': 678,\n",
       " 'second': 679,\n",
       " 'curtain': 680,\n",
       " 'fifteen': 681,\n",
       " 'lock': 682,\n",
       " 'delight': 683,\n",
       " 'fitted': 684,\n",
       " 'rat': 685,\n",
       " 'knelt': 686,\n",
       " 'loveliest': 687,\n",
       " 'longed': 688,\n",
       " 'wander': 689,\n",
       " 'beds': 690,\n",
       " 'flowers': 691,\n",
       " 'cool': 692,\n",
       " 'fountains': 693,\n",
       " 'doorway': 694,\n",
       " 'shoulders': 695,\n",
       " 'begin': 696,\n",
       " 'lately': 697,\n",
       " 'telescopes': 698,\n",
       " 'neck': 699,\n",
       " 'paper': 700,\n",
       " 'label': 701,\n",
       " 'printed': 702,\n",
       " 'wise': 703,\n",
       " 'read': 704,\n",
       " 'histories': 705,\n",
       " 'burnt': 706,\n",
       " 'eaten': 707,\n",
       " 'wild': 708,\n",
       " 'beasts': 709,\n",
       " 'unpleasant': 710,\n",
       " 'because': 711,\n",
       " 'simple': 712,\n",
       " 'friends': 713,\n",
       " 'taught': 714,\n",
       " 'red': 715,\n",
       " 'poker': 716,\n",
       " 'burn': 717,\n",
       " 'cut': 718,\n",
       " 'deeply': 719,\n",
       " 'knife': 720,\n",
       " 'bleeds': 721,\n",
       " 'disagree': 722,\n",
       " 'sooner': 723,\n",
       " 'later': 724,\n",
       " 'ventured': 725,\n",
       " 'mixed': 726,\n",
       " 'flavour': 727,\n",
       " 'cherry': 728,\n",
       " 'tart': 729,\n",
       " 'custard': 730,\n",
       " 'pine': 731,\n",
       " 'apple': 732,\n",
       " 'roast': 733,\n",
       " 'turkey': 734,\n",
       " 'toffee': 735,\n",
       " 'buttered': 736,\n",
       " 'toast': 737,\n",
       " 'ten': 738,\n",
       " 'brightened': 739,\n",
       " 'lovely': 740,\n",
       " 'shrink': 741,\n",
       " 'further': 742,\n",
       " 'nervous': 743,\n",
       " 'flame': 744,\n",
       " 'blown': 745,\n",
       " 'decided': 746,\n",
       " 'possibly': 747,\n",
       " 'plainly': 748,\n",
       " 'climb': 749,\n",
       " 'legs': 750,\n",
       " 'slippery': 751,\n",
       " 'advise': 752,\n",
       " 'advice': 753,\n",
       " 'seldom': 754,\n",
       " 'followed': 755,\n",
       " 'scolded': 756,\n",
       " 'bring': 757,\n",
       " 'cheated': 758,\n",
       " 'game': 759,\n",
       " 'croquet': 760,\n",
       " 'playing': 761,\n",
       " 'pretending': 762,\n",
       " 'pretend': 763,\n",
       " 'hardly': 764,\n",
       " 'respectable': 765,\n",
       " 'currants': 766,\n",
       " 'smaller': 767,\n",
       " 'creep': 768,\n",
       " 'care': 769,\n",
       " 'ate': 770,\n",
       " 'remained': 771,\n",
       " 'eats': 772,\n",
       " 'expecting': 773,\n",
       " 'dull': 774,\n",
       " 'common': 775,\n",
       " 'set': 776,\n",
       " 'work': 777,\n",
       " 'ii': 778,\n",
       " 'opening': 779,\n",
       " 'largest': 780,\n",
       " 'bye': 781,\n",
       " 'shoes': 782,\n",
       " 'stockings': 783,\n",
       " 'shan': 784,\n",
       " 'able': 785,\n",
       " 'myself': 786,\n",
       " 'kind': 787,\n",
       " 'want': 788,\n",
       " 'boots': 789,\n",
       " 'christmas': 790,\n",
       " 'planning': 791,\n",
       " 'carrier': 792,\n",
       " 'presents': 793,\n",
       " 'odd': 794,\n",
       " 'directions': 795,\n",
       " 'esq': 796,\n",
       " 'hearthrug': 797,\n",
       " 'fender': 798,\n",
       " 'love': 799,\n",
       " 'struck': 800,\n",
       " 'hopeless': 801,\n",
       " 'ashamed': 802,\n",
       " 'shedding': 803,\n",
       " 'gallons': 804,\n",
       " 'until': 805,\n",
       " 'reaching': 806,\n",
       " 'dried': 807,\n",
       " 'returning': 808,\n",
       " 'splendidly': 809,\n",
       " 'dressed': 810,\n",
       " 'himself': 811,\n",
       " 'savage': 812,\n",
       " 'desperate': 813,\n",
       " 'timid': 814,\n",
       " 'violently': 815,\n",
       " 'skurried': 816,\n",
       " 'darkness': 817,\n",
       " 'fanning': 818,\n",
       " 'yesterday': 819,\n",
       " 'usual': 820,\n",
       " 'different': 821,\n",
       " 'puzzle': 822,\n",
       " 'thinking': 823,\n",
       " 'ada': 824,\n",
       " 'goes': 825,\n",
       " 'knows': 826,\n",
       " 'besides': 827,\n",
       " 'five': 828,\n",
       " 'twelve': 829,\n",
       " 'six': 830,\n",
       " 'thirteen': 831,\n",
       " 'seven': 832,\n",
       " 'twenty': 833,\n",
       " 'multiplication': 834,\n",
       " 'signify': 835,\n",
       " 'geography': 836,\n",
       " 'london': 837,\n",
       " 'wrong': 838,\n",
       " 'crossed': 839,\n",
       " 'lap': 840,\n",
       " 'repeat': 841,\n",
       " 'sounded': 842,\n",
       " 'hoarse': 843,\n",
       " 'strange': 844,\n",
       " 'crocodile': 845,\n",
       " 'improve': 846,\n",
       " 'shining': 847,\n",
       " 'pour': 848,\n",
       " 'waters': 849,\n",
       " 'nile': 850,\n",
       " 'scale': 851,\n",
       " 'cheerfully': 852,\n",
       " 'grin': 853,\n",
       " 'neatly': 854,\n",
       " 'spread': 855,\n",
       " 'claws': 856,\n",
       " 'welcome': 857,\n",
       " 'fishes': 858,\n",
       " 'gently': 859,\n",
       " 'smiling': 860,\n",
       " 'jaws': 861,\n",
       " 'live': 862,\n",
       " 'poky': 863,\n",
       " 'toys': 864,\n",
       " 'play': 865,\n",
       " 'learn': 866,\n",
       " 'putting': 867,\n",
       " 'till': 868,\n",
       " 'burst': 869,\n",
       " 'measure': 870,\n",
       " 'nearly': 871,\n",
       " 'guess': 872,\n",
       " 'rapidly': 873,\n",
       " 'avoid': 874,\n",
       " 'narrow': 875,\n",
       " 'escape': 876,\n",
       " 'existence': 877,\n",
       " 'speed': 878,\n",
       " 'worse': 879,\n",
       " 'declare': 880,\n",
       " 'these': 881,\n",
       " 'splash': 882,\n",
       " 'chin': 883,\n",
       " 'somehow': 884,\n",
       " 'seaside': 885,\n",
       " 'general': 886,\n",
       " 'conclusion': 887,\n",
       " 'wherever': 888,\n",
       " 'coast': 889,\n",
       " 'number': 890,\n",
       " 'bathing': 891,\n",
       " 'machines': 892,\n",
       " 'digging': 893,\n",
       " 'sand': 894,\n",
       " 'wooden': 895,\n",
       " 'spades': 896,\n",
       " 'lodging': 897,\n",
       " 'houses': 898,\n",
       " 'station': 899,\n",
       " 'wept': 900,\n",
       " 'punished': 901,\n",
       " 'drowned': 902,\n",
       " 'splashing': 903,\n",
       " 'nearer': 904,\n",
       " 'walrus': 905,\n",
       " 'hippopotamus': 906,\n",
       " 'harm': 907,\n",
       " 'brother': 908,\n",
       " 'latin': 909,\n",
       " 'grammar': 910,\n",
       " 'inquisitively': 911,\n",
       " 'wink': 912,\n",
       " 'daresay': 913,\n",
       " 'clear': 914,\n",
       " 'notion': 915,\n",
       " 'ago': 916,\n",
       " 'ou': 917,\n",
       " 'est': 918,\n",
       " 'chatte': 919,\n",
       " 'sentence': 920,\n",
       " 'leap': 921,\n",
       " 'quiver': 922,\n",
       " 'fright': 923,\n",
       " 'animal': 924,\n",
       " 'feelings': 925,\n",
       " 'shrill': 926,\n",
       " 'passionate': 927,\n",
       " 'soothing': 928,\n",
       " 'yet': 929,\n",
       " 'quiet': 930,\n",
       " 'lazily': 931,\n",
       " 'sits': 932,\n",
       " 'purring': 933,\n",
       " 'nicely': 934,\n",
       " 'fire': 935,\n",
       " 'licking': 936,\n",
       " 'washing': 937,\n",
       " 'soft': 938,\n",
       " 'bristling': 939,\n",
       " 'family': 940,\n",
       " 'hated': 941,\n",
       " 'nasty': 942,\n",
       " 'vulgar': 943,\n",
       " 'conversation': 944,\n",
       " 'dog': 945,\n",
       " 'eyed': 946,\n",
       " 'terrier': 947,\n",
       " 'curly': 948,\n",
       " 'brown': 949,\n",
       " 'throw': 950,\n",
       " 'dinner': 951,\n",
       " 'belongs': 952,\n",
       " 'farmer': 953,\n",
       " 'hundred': 954,\n",
       " 'pounds': 955,\n",
       " 'kills': 956,\n",
       " 'rats': 957,\n",
       " 'sorrowful': 958,\n",
       " 'commotion': 959,\n",
       " 'softly': 960,\n",
       " 'pale': 961,\n",
       " 'passion': 962,\n",
       " 'creatures': 963,\n",
       " 'iii': 964,\n",
       " 'assembled': 965,\n",
       " 'draggled': 966,\n",
       " 'feathers': 967,\n",
       " 'clinging': 968,\n",
       " 'dripping': 969,\n",
       " 'cross': 970,\n",
       " 'uncomfortable': 971,\n",
       " 'consultation': 972,\n",
       " 'familiarly': 973,\n",
       " 'known': 974,\n",
       " 'argument': 975,\n",
       " 'sulky': 976,\n",
       " 'older': 977,\n",
       " 'allow': 978,\n",
       " 'knowing': 979,\n",
       " 'positively': 980,\n",
       " 'refused': 981,\n",
       " 'authority': 982,\n",
       " 'fixed': 983,\n",
       " 'cold': 984,\n",
       " 'ahem': 985,\n",
       " 'important': 986,\n",
       " 'driest': 987,\n",
       " 'whose': 988,\n",
       " 'favoured': 989,\n",
       " 'pope': 990,\n",
       " 'submitted': 991,\n",
       " 'leaders': 992,\n",
       " 'accustomed': 993,\n",
       " 'usurpation': 994,\n",
       " 'conquest': 995,\n",
       " 'ugh': 996,\n",
       " 'shiver': 997,\n",
       " 'frowning': 998,\n",
       " 'politely': 999,\n",
       " 'proceed': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import codecs, scipy.sparse, numpy\n",
    "\n",
    "#create co-occurrence matrix\n",
    "\n",
    "\n",
    "#create_cooccurrence_matrix('alice.txt',corpus,window_size)\n",
    "#print(corpus)\n",
    "#coo=numpy.zeros((V, V))\n",
    "#coo=[[0 for x in range(V)] for y in range(V)] \n",
    "vocabulary={}\n",
    "data=[]\n",
    "row=[]\n",
    "col=[]\n",
    "for sentence in corpus:\n",
    "    for pos,word in enumerate(sentence):\n",
    "        i=vocabulary.setdefault(word,len(vocabulary))\n",
    "        start=max(0,pos-window_size_corpus)\n",
    "        end=min(len(sentence),pos+window_size_corpus+1)\n",
    "        for pos2 in range(start,end):\n",
    "            if pos2==pos: \n",
    "                continue\n",
    "            j=vocabulary.setdefault(sentence[pos2],len(vocabulary))\n",
    "            if(j!=i):\n",
    "                data.append(1.); row.append(i); col.append(j);\n",
    "            \n",
    "cooccurrence_matrix=scipy.sparse.coo_matrix((data,(row,col)))\n",
    "#print(cooccurrence_matrix.T * cooccurrence_matrix)\n",
    "print(cooccurrence_matrix.toarray())\n",
    "#print(cooccurrence_matrix.toarray()[1180][0])\n",
    "tokenizer.word_index\n",
    "#The dog chased the cat away from the garden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "87\n",
      "63\n",
      "[[0.48967328]]\n",
      "0.48967327730053256\n",
      "[[0.08862668]]\n",
      "0.08862667920586557\n",
      "[[0.13969969]]\n",
      "0.1396996890567782\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "import math\n",
    "def cos_sim(a,b):\n",
    "    sum_c=0;len_c=len(a);sum_as=0;sum_bs=0;\n",
    "    for ii in range(len_c):\n",
    "        sum_c=sum_c+(a[ii]*b[ii])\n",
    "        sum_as=sum_as+(a[ii]*a[ii])\n",
    "        sum_bs=sum_bs+(b[ii]*b[ii])\n",
    "    return sum_c/(math.sqrt(sum_as)*(math.sqrt(sum_bs)))\n",
    "\n",
    "\n",
    "print(tokenizer.word_index['alice'])  #11\n",
    "print(tokenizer.word_index['dinah'])  #87\n",
    "print(tokenizer.word_index['rabbit']) #63\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cooc=cooccurrence_matrix.toarray()\n",
    "tf_cooc=numpy.zeros((V-1, V-1))\n",
    "for i, sentence in enumerate(cooc):\n",
    "    sumf=0\n",
    "    for wordf in sentence:\n",
    "        sumf+=wordf\n",
    "    for j,wordf in enumerate(sentence):\n",
    "        if(sumf>0):\n",
    "            tf_cooc[i][j]=cooc[i][j]/sumf\n",
    "        else:\n",
    "            continue\n",
    "#print(tf_cooc)\n",
    "\n",
    "#print(len(cooc))\n",
    "#print(len(cooc[10]))\n",
    "#print(V)\n",
    "#print(cooc[0])\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#print(len(tf_cooc[10:11][0]))\n",
    "#print(len(cooc[V-2]))\n",
    "#print(cooc.shape)\n",
    "#print(tf_cooc.shape)\n",
    "print(cosine_similarity(tf_cooc[10:11], tf_cooc[86:87]))\n",
    "print(cos_sim(tf_cooc[10],tf_cooc[86]))                  \n",
    "print(cosine_similarity(tf_cooc[10:11], tf_cooc[62:63]))\n",
    "print(cos_sim(tf_cooc[10],tf_cooc[62]))\n",
    "print(cosine_similarity(tf_cooc[86:87], tf_cooc[62:63]))\n",
    "print(cos_sim(tf_cooc[86],tf_cooc[62]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  34  16 161  82   2]\n",
      "closest word= t, had, question, say, to\n",
      "0.7033898864296415\n",
      "0.6995182846911174\n",
      "0.6726422527548841\n",
      "0.6685274041392557\n",
      "0.6544770424322105\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "maxid=0\n",
    "maxval=0\n",
    "simi=np.zeros(V-1)\n",
    "for i in range(V-1):\n",
    "    simi[i]=cosine_similarity(tf_cooc[10:11], tf_cooc[i:i+1])[0][0]\n",
    "indices=simi.argsort()[-6:][::-1]\n",
    "print(indices)\n",
    "print(\"closest word= {}, {}, {}, {}, {}\".format(list(tokenizer.word_index)[indices[1]],list(tokenizer.word_index)[indices[2]],list(tokenizer.word_index)[indices[3]],list(tokenizer.word_index)[indices[4]],list(tokenizer.word_index)[indices[5]]))            \n",
    "for i in range(1,6):\n",
    "    print(cosine_similarity(tf_cooc[10:11], tf_cooc[indices[i]:indices[i]+1])[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = cooccurrence_matrix.toarray()\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i-1,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6559, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data for cbow\n",
    "import math\n",
    "reversed_index = {v:k for k,v in tokenizer.word_index.items()}\n",
    "\n",
    "def prep_cbow_data(corpus=corpus,window_size=window_size,V=V):\n",
    "    flat = [word for line in corpus for word in line]\n",
    "    vectors = []\n",
    "    L = len(flat)\n",
    "    start = int(window_size)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(start,L-start):\n",
    "        context_before = flat[i-start:i]\n",
    "        target = np.zeros(V,dtype=int)\n",
    "        target[flat[i]-1] = 1\n",
    "        context_after = flat[i+1:i+1+start]\n",
    "        context = context_before + context_after\n",
    "        #onehot_context = []\n",
    "        #for word in context:\n",
    "        #    vector = np.zeros(V,dtype=int)\n",
    "        #    vector[word-1] = 1\n",
    "        #    onehot_context.append(vector)\n",
    "            \n",
    "        if len(context) == 4:\n",
    "            #onehot_context = np.asarray(onehot_context)\n",
    "            x.append(context)\n",
    "            y.append(target)\n",
    "        \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "prep_cbow_data()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size X: 6559,size Y:6559\n",
      "(6559, 4) (6559, 1183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=1183, output_dim=100, input_length=4, embeddings_initializer=\"glorot_uniform\")`\n",
      "  \n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1183, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model\n",
    "from keras.layers import Flatten\n",
    "X,Y = prep_cbow_data()\n",
    "features = len(X)\n",
    "print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "print(X.shape,Y.shape)\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "cbow.add(Flatten())\n",
    "cbow.add(Dense(V, init='glorot_uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5903 samples, validate on 656 samples\n",
      "Epoch 1/100\n",
      "5903/5903 [==============================] - 3s 547us/step - loss: 7.0495 - val_loss: 7.0279\n",
      "Epoch 2/100\n",
      "5903/5903 [==============================] - 3s 496us/step - loss: 6.9876 - val_loss: 6.9825\n",
      "Epoch 3/100\n",
      "5903/5903 [==============================] - 3s 485us/step - loss: 6.9145 - val_loss: 6.9132\n",
      "Epoch 4/100\n",
      "5903/5903 [==============================] - 3s 488us/step - loss: 6.8049 - val_loss: 6.7977\n",
      "Epoch 5/100\n",
      "5903/5903 [==============================] - 3s 480us/step - loss: 6.6370 - val_loss: 6.6216\n",
      "Epoch 6/100\n",
      "5903/5903 [==============================] - 3s 470us/step - loss: 6.4109 - val_loss: 6.4074\n",
      "Epoch 7/100\n",
      "5903/5903 [==============================] - 3s 453us/step - loss: 6.1725 - val_loss: 6.2239\n",
      "Epoch 8/100\n",
      "5903/5903 [==============================] - 3s 464us/step - loss: 5.9754 - val_loss: 6.0881\n",
      "Epoch 9/100\n",
      "5903/5903 [==============================] - 3s 457us/step - loss: 5.8161 - val_loss: 5.9828\n",
      "Epoch 10/100\n",
      "5903/5903 [==============================] - 3s 466us/step - loss: 5.6796 - val_loss: 5.8917\n",
      "Epoch 11/100\n",
      "5903/5903 [==============================] - 3s 482us/step - loss: 5.5570 - val_loss: 5.8114\n",
      "Epoch 12/100\n",
      "5903/5903 [==============================] - 3s 473us/step - loss: 5.4442 - val_loss: 5.7403\n",
      "Epoch 13/100\n",
      "5903/5903 [==============================] - 3s 478us/step - loss: 5.3385 - val_loss: 5.6753\n",
      "Epoch 14/100\n",
      "5903/5903 [==============================] - 3s 469us/step - loss: 5.2390 - val_loss: 5.6169\n",
      "Epoch 15/100\n",
      "5903/5903 [==============================] - 3s 475us/step - loss: 5.1443 - val_loss: 5.5632\n",
      "Epoch 16/100\n",
      "5903/5903 [==============================] - 3s 476us/step - loss: 5.0538 - val_loss: 5.5149\n",
      "Epoch 17/100\n",
      "5903/5903 [==============================] - 3s 467us/step - loss: 4.9665 - val_loss: 5.4677\n",
      "Epoch 18/100\n",
      "5903/5903 [==============================] - 3s 473us/step - loss: 4.8824 - val_loss: 5.4241\n",
      "Epoch 19/100\n",
      "5903/5903 [==============================] - 3s 472us/step - loss: 4.8022 - val_loss: 5.3846\n",
      "Epoch 20/100\n",
      "5903/5903 [==============================] - 3s 474us/step - loss: 4.7246 - val_loss: 5.3489\n",
      "Epoch 21/100\n",
      "5903/5903 [==============================] - 3s 487us/step - loss: 4.6496 - val_loss: 5.3148\n",
      "Epoch 22/100\n",
      "5903/5903 [==============================] - 3s 524us/step - loss: 4.5766 - val_loss: 5.2840\n",
      "Epoch 23/100\n",
      "5903/5903 [==============================] - 3s 506us/step - loss: 4.5054 - val_loss: 5.2552\n",
      "Epoch 24/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 4.4365 - val_loss: 5.2268\n",
      "Epoch 25/100\n",
      "5903/5903 [==============================] - 3s 520us/step - loss: 4.3692 - val_loss: 5.2011\n",
      "Epoch 26/100\n",
      "5903/5903 [==============================] - 3s 502us/step - loss: 4.3031 - val_loss: 5.1779\n",
      "Epoch 27/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 4.2391 - val_loss: 5.1539\n",
      "Epoch 28/100\n",
      "5903/5903 [==============================] - 3s 501us/step - loss: 4.1764 - val_loss: 5.1340\n",
      "Epoch 29/100\n",
      "5903/5903 [==============================] - 3s 521us/step - loss: 4.1147 - val_loss: 5.1111\n",
      "Epoch 30/100\n",
      "5903/5903 [==============================] - 3s 517us/step - loss: 4.0545 - val_loss: 5.0939\n",
      "Epoch 31/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 3.9958 - val_loss: 5.0774\n",
      "Epoch 32/100\n",
      "5903/5903 [==============================] - 3s 512us/step - loss: 3.9370 - val_loss: 5.0615\n",
      "Epoch 33/100\n",
      "5903/5903 [==============================] - 3s 502us/step - loss: 3.8795 - val_loss: 5.0446\n",
      "Epoch 34/100\n",
      "5903/5903 [==============================] - 3s 508us/step - loss: 3.8226 - val_loss: 5.0320\n",
      "Epoch 35/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 3.7672 - val_loss: 5.0181\n",
      "Epoch 36/100\n",
      "5903/5903 [==============================] - 3s 500us/step - loss: 3.7129 - val_loss: 5.0026\n",
      "Epoch 37/100\n",
      "5903/5903 [==============================] - 3s 515us/step - loss: 3.6593 - val_loss: 4.9881\n",
      "Epoch 38/100\n",
      "5903/5903 [==============================] - 3s 499us/step - loss: 3.6065 - val_loss: 4.9766\n",
      "Epoch 39/100\n",
      "5903/5903 [==============================] - 3s 514us/step - loss: 3.5535 - val_loss: 4.9656\n",
      "Epoch 40/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 3.5024 - val_loss: 4.9569\n",
      "Epoch 41/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.4515 - val_loss: 4.9470\n",
      "Epoch 42/100\n",
      "5903/5903 [==============================] - 3s 508us/step - loss: 3.4017 - val_loss: 4.9332\n",
      "Epoch 43/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.3526 - val_loss: 4.9240\n",
      "Epoch 44/100\n",
      "5903/5903 [==============================] - 3s 509us/step - loss: 3.3048 - val_loss: 4.9146\n",
      "Epoch 45/100\n",
      "5903/5903 [==============================] - 3s 519us/step - loss: 3.2574 - val_loss: 4.9087\n",
      "Epoch 46/100\n",
      "5903/5903 [==============================] - 3s 503us/step - loss: 3.2104 - val_loss: 4.9015\n",
      "Epoch 47/100\n",
      "5903/5903 [==============================] - 3s 512us/step - loss: 3.1638 - val_loss: 4.8949\n",
      "Epoch 48/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 3.1182 - val_loss: 4.8887\n",
      "Epoch 49/100\n",
      "5903/5903 [==============================] - 3s 515us/step - loss: 3.0731 - val_loss: 4.8816\n",
      "Epoch 50/100\n",
      "5903/5903 [==============================] - 3s 513us/step - loss: 3.0292 - val_loss: 4.8751\n",
      "Epoch 51/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 2.9854 - val_loss: 4.8706\n",
      "Epoch 52/100\n",
      "5903/5903 [==============================] - 3s 511us/step - loss: 2.9428 - val_loss: 4.8660\n",
      "Epoch 53/100\n",
      "5903/5903 [==============================] - 3s 510us/step - loss: 2.8997 - val_loss: 4.8603\n",
      "Epoch 54/100\n",
      "5903/5903 [==============================] - 3s 520us/step - loss: 2.8585 - val_loss: 4.8569\n",
      "Epoch 55/100\n",
      "5903/5903 [==============================] - 3s 516us/step - loss: 2.8173 - val_loss: 4.8546\n",
      "Epoch 56/100\n",
      "5903/5903 [==============================] - 3s 501us/step - loss: 2.7764 - val_loss: 4.8481\n",
      "Epoch 57/100\n",
      "5903/5903 [==============================] - 3s 510us/step - loss: 2.7371 - val_loss: 4.8446\n",
      "Epoch 58/100\n",
      "5903/5903 [==============================] - 3s 505us/step - loss: 2.6966 - val_loss: 4.8403\n",
      "Epoch 59/100\n",
      "5903/5903 [==============================] - 3s 506us/step - loss: 2.6579 - val_loss: 4.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1baf76451d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "from keras import callbacks\n",
    "epochs=100\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "cbow.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#get x and y's for data\n",
    "x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "\n",
    "#save the preprocessed data of Skipgram\n",
    "f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "for input,outcome  in zip(x,y):\n",
    "    input = np.concatenate(input)\n",
    "    f.write(\" \".join(map(str, list(input))))\n",
    "    f.write(\",\")\n",
    "    outcome = np.concatenate(outcome)\n",
    "    f.write(\" \".join(map(str,list(outcome))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file():\n",
    "    f = open('data_skipgram.txt' ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram model\n",
    "\n",
    "dim1=50\n",
    "dim2=150\n",
    "dim3=300\n",
    "skipgram_50 = Sequential()\n",
    "skipgram_50.add(Embedding(input_dim=V, output_dim=dim1, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_50.add(Reshape((dim1, )))\n",
    "skipgram_50.add(Dense(input_dim=dim1, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_150 = Sequential()\n",
    "skipgram_150.add(Embedding(input_dim=V, output_dim=dim2, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_150.add(Reshape((dim2, )))\n",
    "skipgram_150.add(Dense(input_dim=dim2, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_300 = Sequential()\n",
    "skipgram_300.add(Embedding(input_dim=V, output_dim=dim3, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_300.add(Reshape((dim3, )))\n",
    "skipgram_300.add(Dense(input_dim=dim3, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n",
    "skipgram_50.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "skipgram_150.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "skipgram_300.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42019.0201215744\n",
      "1 38379.62837553024\n",
      "2 39003.5622587204\n",
      "3 39509.36110877991\n",
      "4 39705.85113596916\n",
      "5 39885.63010597229\n",
      "6 40046.92625975609\n",
      "7 40210.36398935318\n",
      "8 40391.53400826454\n",
      "9 40586.17857551575\n",
      "0 42003.86192703247\n",
      "1 38329.2822098732\n",
      "2 38850.55951523781\n",
      "3 39214.83134794235\n",
      "4 39390.2365026474\n",
      "5 39541.21982002258\n",
      "6 39695.80431985855\n",
      "7 39864.555876255035\n",
      "8 40036.627875089645\n",
      "9 40212.43623352051\n",
      "0 41972.32948112488\n",
      "1 38263.37252306938\n",
      "2 38669.966866493225\n",
      "3 38963.96811294556\n",
      "4 39128.82766032219\n",
      "5 39276.19422340393\n",
      "6 39422.713296175\n",
      "7 39560.45543265343\n",
      "8 39694.1005204916\n",
      "9 39825.880415678024\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_50.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_50.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim1)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_50.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_150.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_150.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim2)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_150.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_300.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_300.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim3)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_300.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=1183, output_dim=100, input_length=4, embeddings_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1183, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "cbow2 = Sequential()\n",
    "cbow2.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "cbow2.add(Flatten())\n",
    "cbow2.add(Dense(V,activation='relu'))\n",
    "cbow2.add(Dense(V, init='glorot_uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n",
    "cbow2.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5903 samples, validate on 656 samples\n",
      "Epoch 1/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 6.7766 - val_loss: 6.1094\n",
      "Epoch 2/100\n",
      "5903/5903 [==============================] - 9s 1ms/step - loss: 5.8327 - val_loss: 5.7606\n",
      "Epoch 3/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 5.5145 - val_loss: 5.5747\n",
      "Epoch 4/100\n",
      "5903/5903 [==============================] - 8s 1ms/step - loss: 5.2639 - val_loss: 5.4150\n",
      "Epoch 5/100\n",
      "5903/5903 [==============================] - 8s 1ms/step - loss: 5.0215 - val_loss: 5.2781\n",
      "Epoch 6/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 4.7894 - val_loss: 5.1477\n",
      "Epoch 7/100\n",
      "5903/5903 [==============================] - 11s 2ms/step - loss: 4.5724 - val_loss: 5.0567\n",
      "Epoch 8/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 4.3716 - val_loss: 4.9879\n",
      "Epoch 9/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 4.1812 - val_loss: 4.9395\n",
      "Epoch 10/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.9998 - val_loss: 4.8841\n",
      "Epoch 11/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.8235 - val_loss: 4.8715\n",
      "Epoch 12/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.6504 - val_loss: 4.8396\n",
      "Epoch 13/100\n",
      "5903/5903 [==============================] - 9s 2ms/step - loss: 3.4827 - val_loss: 4.8171\n",
      "Epoch 14/100\n",
      "5903/5903 [==============================] - 10s 2ms/step - loss: 3.3142 - val_loss: 4.8193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bafacd8668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model for CBOW + dense\n",
    "epochs=100\n",
    "\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "cbow2.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "\n",
    "#create Skipgram model\n",
    "\n",
    "skipgram_50_dense = Sequential()\n",
    "skipgram_50_dense.add(Embedding(input_dim=V, output_dim=dim1, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_50_dense.add(Reshape((dim1, )))\n",
    "skipgram_50_dense.add(Dense(V,activation='relu'))\n",
    "skipgram_50_dense.add(Dense(input_dim=dim1, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_150_dense = Sequential()\n",
    "skipgram_150_dense.add(Embedding(input_dim=V, output_dim=dim2, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_150_dense.add(Reshape((dim2, )))\n",
    "skipgram_150_dense.add(Dense(V,activation='relu'))\n",
    "skipgram_150_dense.add(Dense(input_dim=dim2, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_300_dense = Sequential()\n",
    "skipgram_300_dense.add(Embedding(input_dim=V, output_dim=dim3, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_300_dense.add(Reshape((dim3, )))\n",
    "skipgram_300_dense.add(Dense(V,activation='relu'))\n",
    "skipgram_300_dense.add(Dense(input_dim=dim3, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "#skipgram2 = Sequential()\n",
    "#skipgram2.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "#skipgram2.add(Reshape((dim, )))\n",
    "#skipgram2.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#skipgram2.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram_50_dense.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "skipgram_150_dense.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "skipgram_300_dense.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39444.32987213135\n",
      "1 38264.48951482773\n",
      "2 37948.66480302811\n",
      "3 37769.99622249603\n",
      "4 37750.78943347931\n",
      "5 37725.23108911514\n",
      "6 37674.930666565895\n",
      "7 37620.79466640949\n",
      "8 37566.527002334595\n",
      "9 37519.70512115955\n",
      "0 39471.03422307968\n",
      "1 38220.93018221855\n",
      "2 37718.286794900894\n",
      "3 37538.61131024361\n",
      "4 37493.13153910637\n",
      "5 37456.600872159004\n",
      "6 37402.676347732544\n",
      "7 37335.5792144537\n",
      "8 37265.79830777645\n",
      "9 37196.82523834705\n",
      "0 39445.47623085976\n",
      "1 38082.24569654465\n",
      "2 37592.96987724304\n",
      "3 37330.86567401886\n",
      "4 37259.75267124176\n",
      "5 37224.620362997055\n",
      "6 37161.21744906902\n",
      "7 37086.09090960026\n",
      "8 37002.261269927025\n",
      "9 36925.072355270386\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_50_dense.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_50_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim1)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_50_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_150_dense.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_150_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim2)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_150_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file():\n",
    "        loss += skipgram_300_dense.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    \n",
    "f = open('vectors_skipgram_300_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim3)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram_300_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "skipgram_50.save('skipgram_50.h5')\n",
    "skipgram_150.save('skipgram_150.h5')\n",
    "skipgram_300.save('skipgram_300.h5')\n",
    "skipgram_50_dense.save('skipgram_50_dense.h5')\n",
    "skipgram_150_dense.save('skipgram_150_dense.h5')\n",
    "skipgram_300_dense.save('skipgram_300_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement your own analogy function\n",
    "\n",
    "key_cbow_50 ={}\n",
    "key_cbow_150 ={}\n",
    "key_cbow_300 ={}\n",
    "key_cbow_50_dense ={}\n",
    "key_cbow_150_dense ={}\n",
    "key_cbow_300_dense ={}\n",
    "key_skipgram_50 ={}\n",
    "key_skipgram_150 ={}\n",
    "key_skipgram_300 ={}\n",
    "key_skipgram_50_dense ={}\n",
    "key_skipgram_150_dense ={}\n",
    "key_skipgram_300_dense ={}\n",
    "\n",
    "embed_cbow_50 =[]\n",
    "embed_cbow_150 =[]\n",
    "embed_cbow_300 =[]\n",
    "embed_cbow_50_dense =[]\n",
    "embed_cbow_150_dense =[]\n",
    "embed_cbow_300_dense =[]\n",
    "embed_skipgram_50 =[]\n",
    "embed_skipgram_150 =[]\n",
    "embed_skipgram_300 =[]\n",
    "embed_skipgram_50_dense =[]\n",
    "embed_skipgram_150_dense =[]\n",
    "embed_skipgram_300_dense =[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_from_file(a):\n",
    "    f = open(a ,'r')\n",
    "    key_dict={}\n",
    "    vect=[]\n",
    "    for row in f:\n",
    "        dimention=0\n",
    "        inp = row.split(\"\\n\")[0].split(\" \")\n",
    "        if(len(inp)==2):\n",
    "            dimention=inp[1]\n",
    "        else:\n",
    "            key_dict.update({inp[0]:len(key_dict)})\n",
    "            vect.append(np.asarray(inp[1:],dtype=float))\n",
    "    return (vect,key_dict)\n",
    "\n",
    "(embed_cbow_50,key_cbow_50)=load_from_file('vectors_cbow_50.txt')\n",
    "(embed_cbow_150,key_cbow_150)=load_from_file('vectors_cbow_150.txt')\n",
    "(embed_cbow_300,key_cbow_300)=load_from_file('vectors_cbow_300.txt')\n",
    "(embed_cbow_50_dense,key_cbow_50_dense)=load_from_file('vectors_cbow_50_dense.txt')\n",
    "(embed_cbow_150_dense,key_cbow_150_dense)=load_from_file('vectors_cbow_150_dense.txt')\n",
    "(embed_cbow_300_dense,key_cbow_300_dense)=load_from_file('vectors_cbow_300_dense.txt')\n",
    "(embed_skipgram_50,key_skipgram_50)=load_from_file('vectors_skipgram_50.txt')\n",
    "(embed_skipgram_150,key_skipgram_150)=load_from_file('vectors_skipgram_150.txt')\n",
    "(embed_skipgram_300,key_skipgram_300)=load_from_file('vectors_skipgram_300.txt')\n",
    "(embed_skipgram_50_dense,key_skipgram_50_dense)=load_from_file('vectors_skipgram_50_dense.txt')\n",
    "(embed_skipgram_150_dense,key_skipgram_150_dense)=load_from_file('vectors_skipgram_150_dense.txt')\n",
    "(embed_skipgram_300_dense,key_skipgram_300_dense)=load_from_file('vectors_skipgram_300_dense.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08582410447347387"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator \n",
    "def load_analogy():\n",
    "    f = open('analogy_alice.txt' ,'r')\n",
    "    analogy_list=[]\n",
    "    for row in f:\n",
    "        a = row.split(\"\\n\")[0].split(\" \")\n",
    "        analogy_list.append(np.asarray(a))\n",
    "    return analogy_list\n",
    "\n",
    "def analogy_check(analogy_list,embed_matrix,dictionary):\n",
    "    v=[[],[],[],[]]\n",
    "    for i in range(4):\n",
    "        if(analogy_list[i] in dictionary):\n",
    "            v[i]=embed_matrix[dictionary[analogy_list[i]]-1]\n",
    "        else:\n",
    "            v[i]=np.zeros(150)\n",
    "    #print(dictionary)\n",
    "    #return abs(cos_sim(v[0],v[1])-cos_sim(v[2],v[3]))\n",
    "    return cos_sim( list(map(operator.sub, list(map(sum, zip(v[0],v[1]))), v[2])),v[3])\n",
    "\n",
    "analogy_list=load_analogy()\n",
    "#print(analogy_list[0])\n",
    "#analogy_check([\"go\",\"going\",\"look\",\"looking\"],embed_skipgram_150, tokenizer.word_index )\n",
    "analogy_check([\"say\",\"saying\",\"sit\",\"sitting\"],cooccurrence_matrix.toarray(), tokenizer.word_index )\n",
    "analogy_check([\"sudden\",\"suddenly\",\"usual\",\"usually\"],embed_skipgram_300_dense, tokenizer.word_index )\n",
    "#analogy_check([\"fancy\",\"fancying\",\"alice\",\"rabbit\"],embed_skipgram_150, tokenizer.word_index )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read model from tim\n",
    "\n",
    "from keras.models import load_model\n",
    "cbow_50 = load_model('cbow50.h5')\n",
    "cbow_150 = load_model('cbow150.h5')\n",
    "cbow_300 = load_model('cbow300.h5')\n",
    "cbow_50_dense = load_model('cbow_extended_50.h5')\n",
    "cbow_150_dense = load_model('cbow_extended_150.h5')\n",
    "cbow_300_dense = load_model('cbow_extended_300.h5')\n",
    "\n",
    "f = open('vectors_cbow_50.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim1)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_50.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('vectors_cbow_150.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim2)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_150.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('vectors_cbow_300.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim3)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_300.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('vectors_cbow_50_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim1)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_50_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('vectors_cbow_150_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim2)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_150_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('vectors_cbow_300_dense.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim3)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = cbow_300_dense.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram\n",
    "\n",
    "CBOW tries to predict a word by context, it sees the context and maximizes the probability of the target word.\n",
    "This means CBOW is good at predicting frequent words. To train CBOW a reasonably low amount of data is sufficient.\n",
    "The drawback of CBOW is that whilst it will preform well at predicting frequent words it will have low accuracy for less frequent words.\n",
    "this is because some words compete in the sense that they are a valid target for the same context.\n",
    "The more frequent word will then be predicted.\n",
    "\n",
    "In negative sampling we choose random words to pair with the target and have an output of 0. \n",
    "The updating of the weights is then performed on these K samples, which reduces the computational requirements.\n",
    "The model does not need all observations but simply only the K sampled pairs (context + target).\n",
    "\n",
    "Skip gram is designed to predict context. It sees the target and tries to find the context around the word. \n",
    "Skip gram is rather well suited even for rare words. Thake the example delightfull, it will try to predict something like yesterday was a day.\n",
    "Whilst if CBOw would have gotten this context delightfull would have never been predicted it would have chosen more frequent words like good.\n",
    "The drawback for skip gram is that it requires a large amount of data to train. This is because if we for example look at the delightfull word it will have context.\n",
    "Similar nice will also have a context, delightfull day and nice day are 2 independant sets. In cbow this use of nice and delightfull would be competing since the context day has both \n",
    "delightfull and nice as targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"tes\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "import gensim\n",
    "#path = \"tes\\glove.6B\\glove.6B.300d_converted.txt\"\n",
    "path = \"tes\\glove.6B.300d.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(\"glove_converted.txt\", binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v: sudden, suddenly, usual, usually: 0.22255026056670402\n",
      "glove: sudden, suddenly, usual, usually: 0.3100777731068751\n",
      "cbow: sudden, suddenly, usual, usually: -0.07675966522933991\n",
      "cbow_dense: sudden, suddenly, usual, usually: 0.021816142041618355\n",
      "skipgram: sudden, suddenly, usual, usually: 0.013981381463225495\n",
      "skipgram_dense: sudden, suddenly, usual, usually: -0.08582410447347387\n",
      "\n",
      "w2v: bad, worse, good, better: 0.6724906513772779\n",
      "glove: bad, worse, good, better: 0.6565830036364912\n",
      "cbow: bad, worse, good, better: -0.14211218867763775\n",
      "cbow_dense: bad, worse, good, better: -0.06851437505368992\n",
      "skipgram: bad, worse, good, better: -0.05229406240425803\n",
      "skipgram_dense: bad, worse, good, better: 0.0010391543210957021\n",
      "\n",
      "w2v: go, going, look, looking: 0.4437767197062708\n",
      "glove: go, going, look, looking: 0.6695781772903449\n",
      "cbow: go, going, look, looking: 0.0013563966233273222\n",
      "cbow_dense: go, going, look, looking: 0.16969527138432886\n",
      "skipgram: go, going, look, looking: 0.0857564110649774\n",
      "skipgram_dense: go, going, look, looking: 0.03098357758763984\n",
      "\n",
      "w2v: he, she, his, her: 0.661601926269319\n",
      "glove: he, she, his, her: 0.7343965200095653\n",
      "cbow: he, she, his, her: 0.14181520782518023\n",
      "cbow_dense: he, she, his, her: 0.06405350025790807\n",
      "skipgram: he, she, his, her: 0.05539611965867066\n",
      "skipgram_dense: he, she, his, her: 0.047066524315540086\n",
      "\n",
      "w2v: brother, sister, his, her: 0.29478371779476814\n",
      "glove: brother, sister, his, her: 0.565070276491166\n",
      "cbow: brother, sister, his, her: 0.1738845887364074\n",
      "cbow_dense: brother, sister, his, her: 0.15306492585478185\n",
      "skipgram: brother, sister, his, her: -0.05641767468527157\n",
      "skipgram_dense: brother, sister, his, her: -0.009200589532626787\n",
      "\n",
      "w2v: listen, listening, look, looking: 0.21896109979844153\n",
      "glove: listen, listening, look, looking: 0.4071841107300849\n",
      "cbow: listen, listening, look, looking: 0.00960132291661633\n",
      "cbow_dense: listen, listening, look, looking: 0.11961182797045093\n",
      "skipgram: listen, listening, look, looking: -0.0841335857178494\n",
      "skipgram_dense: listen, listening, look, looking: -0.037008750133731456\n",
      "\n",
      "w2v: saying, said, thinking, thought: 0.40956290213903407\n",
      "glove: saying, said, thinking, thought: 0.49768632598408674\n",
      "cbow: saying, said, thinking, thought: 0.33954839323186276\n",
      "cbow_dense: saying, said, thinking, thought: 0.1291448144807512\n",
      "skipgram: saying, said, thinking, thought: 0.3259387717229089\n",
      "skipgram_dense: saying, said, thinking, thought: -0.022102505774773195\n",
      "\n",
      "w2v: bird, birds, cat, cats: 0.548314343723498\n",
      "glove: bird, birds, cat, cats: 0.48856988240789534\n",
      "cbow: bird, birds, cat, cats: -0.21468265880268259\n",
      "cbow_dense: bird, birds, cat, cats: -0.10054438635684715\n",
      "skipgram: bird, birds, cat, cats: 0.03250104634631262\n",
      "skipgram_dense: bird, birds, cat, cats: 0.11074620778124797\n",
      "\n",
      "w2v: good, better, old, older: 0.21694786651491343\n",
      "glove: good, better, old, older: 0.40961181661362545\n",
      "cbow: good, better, old, older: 0.04180303804268945\n",
      "cbow_dense: good, better, old, older: 0.05047850869217344\n",
      "skipgram: good, better, old, older: 0.08390944033445641\n",
      "skipgram_dense: good, better, old, older: -0.014284331275707515\n",
      "\n",
      "w2v: good, better, quick, quicker: 0.5330237925619995\n",
      "glove: good, better, quick, quicker: 0.5328881245927063\n",
      "cbow: good, better, quick, quicker: 0.043084136200568085\n",
      "cbow_dense: good, better, quick, quicker: 0.08780310156990714\n",
      "skipgram: good, better, quick, quicker: -0.0672776369563426\n",
      "skipgram_dense: good, better, quick, quicker: 0.028810861971552044\n",
      "\n",
      "w2v: large, largest, good, best: 0.21683160333463167\n",
      "glove: large, largest, good, best: 0.2652648164246623\n",
      "cbow: large, largest, good, best: 0.31235314624380967\n",
      "cbow_dense: large, largest, good, best: 0.06919605281969435\n",
      "skipgram: large, largest, good, best: -0.006536230294100379\n",
      "skipgram_dense: large, largest, good, best: 0.022095812827602777\n",
      "\n",
      "w2v: falling, fell, knowing, knew: 0.14287526113350987\n",
      "glove: falling, fell, knowing, knew: 0.15409427727346053\n",
      "cbow: falling, fell, knowing, knew: -0.07905697790725436\n",
      "cbow_dense: falling, fell, knowing, knew: -0.1048865932317624\n",
      "skipgram: falling, fell, knowing, knew: 0.013853888594661638\n",
      "skipgram_dense: falling, fell, knowing, knew: 0.024131281276904854\n",
      "\n",
      "w2v: walk, walking, think, thinking: 0.25107561442774934\n",
      "glove: walk, walking, think, thinking: 0.4035633827468627\n",
      "cbow: walk, walking, think, thinking: -0.013221233663537528\n",
      "cbow_dense: walk, walking, think, thinking: -0.009731531377755356\n",
      "skipgram: walk, walking, think, thinking: 0.09443912798901292\n",
      "skipgram_dense: walk, walking, think, thinking: 0.0017553334445679046\n",
      "\n",
      "w2v: child, children, cat, cats: 0.36624447596308296\n",
      "glove: child, children, cat, cats: 0.28044855781310063\n",
      "cbow: child, children, cat, cats: -0.2576635618221906\n",
      "cbow_dense: child, children, cat, cats: -0.0630106301661655\n",
      "skipgram: child, children, cat, cats: 0.08883936307509875\n",
      "skipgram_dense: child, children, cat, cats: -0.026278953768836256\n",
      "\n",
      "w2v: dog, dogs, eye, eyes: 0.13321162577935888\n",
      "glove: dog, dogs, eye, eyes: 0.3071176163860798\n",
      "cbow: dog, dogs, eye, eyes: -0.07113528677106216\n",
      "cbow_dense: dog, dogs, eye, eyes: -0.08522655447996517\n",
      "skipgram: dog, dogs, eye, eyes: 0.07650736551249597\n",
      "skipgram_dense: dog, dogs, eye, eyes: 0.045406220394282076\n",
      "\n",
      "w2v: hand, hands, rat, rats: 0.04862699542761563\n",
      "glove: hand, hands, rat, rats: 0.05072466972752036\n",
      "cbow: hand, hands, rat, rats: 0.1932542609051188\n",
      "cbow_dense: hand, hands, rat, rats: -0.15502528932785348\n",
      "skipgram: hand, hands, rat, rats: 0.03946198477425701\n",
      "skipgram_dense: hand, hands, rat, rats: -0.07919303696947005\n",
      "\n",
      "w2v: eat, eats, find, finds: 0.2830192036624857\n",
      "glove: eat, eats, find, finds: 0.30765008316898457\n",
      "cbow: eat, eats, find, finds: 0.1406046690404185\n",
      "cbow_dense: eat, eats, find, finds: 0.0219764605368637\n",
      "skipgram: eat, eats, find, finds: -0.01920813874990434\n",
      "skipgram_dense: eat, eats, find, finds: 0.02862671211042982\n",
      "\n",
      "w2v: find, finds, say, says: 0.38303518154481636\n",
      "glove: find, finds, say, says: 0.4858085924624003\n",
      "cbow: find, finds, say, says: -0.10766476872833465\n",
      "cbow_dense: find, finds, say, says: -0.0819073553800007\n",
      "skipgram: find, finds, say, says: -0.08034059519651741\n",
      "skipgram_dense: find, finds, say, says: 0.041517536625278816\n",
      "\n",
      "w2v: old, older, good, better: 0.21694786651491343\n",
      "glove: old, older, good, better: 0.40961181661362545\n",
      "cbow: old, older, good, better: -0.03153937056499282\n",
      "cbow_dense: old, older, good, better: 0.06568098484014025\n",
      "skipgram: old, older, good, better: -0.023182690867631403\n",
      "skipgram_dense: old, older, good, better: 0.03750692335347313\n",
      "\n",
      "w2v: large, larger, quick, quicker: 0.22521266715730429\n",
      "glove: large, larger, quick, quicker: 0.22567543578240246\n",
      "cbow: large, larger, quick, quicker: -0.018327700770753793\n",
      "cbow_dense: large, larger, quick, quicker: -0.049348306302746295\n",
      "skipgram: large, larger, quick, quicker: 0.014788514009054106\n",
      "skipgram_dense: large, larger, quick, quicker: 0.02371671053988241\n",
      "\n",
      "w2v: go, going, listen, listening: 0.23913213804946878\n",
      "glove: go, going, listen, listening: 0.42662491131551733\n",
      "cbow: go, going, listen, listening: -0.05352097746896414\n",
      "cbow_dense: go, going, listen, listening: -0.14896358119691314\n",
      "skipgram: go, going, listen, listening: 0.10172302696406732\n",
      "skipgram_dense: go, going, listen, listening: 0.03722928802883272\n",
      "\n",
      "w2v: run, running, walk, walking: 0.4228847541114353\n",
      "glove: run, running, walk, walking: 0.47095163467027384\n",
      "cbow: run, running, walk, walking: -0.0028142046713905233\n",
      "cbow_dense: run, running, walk, walking: 0.08147060418902584\n",
      "skipgram: run, running, walk, walking: -0.007724702519940744\n",
      "skipgram_dense: run, running, walk, walking: 0.02603747460367717\n",
      "\n",
      "w2v: run, running, think, thinking: 0.24565625385619846\n",
      "glove: run, running, think, thinking: 0.3926802350946642\n",
      "cbow: run, running, think, thinking: -0.006785781141912233\n",
      "cbow_dense: run, running, think, thinking: 0.05699287127087976\n",
      "skipgram: run, running, think, thinking: 0.060924769391190445\n",
      "skipgram_dense: run, running, think, thinking: 0.03230106826210846\n",
      "\n",
      "w2v: say, saying, sit, sitting: 0.19387733146836375\n",
      "glove: say, saying, sit, sitting: 0.33033972016602986\n",
      "cbow: say, saying, sit, sitting: -0.035323962797726406\n",
      "cbow_dense: say, saying, sit, sitting: 0.10109881160563665\n",
      "skipgram: say, saying, sit, sitting: 0.04378602995374177\n",
      "skipgram_dense: say, saying, sit, sitting: 0.03827250222830714\n",
      "\n",
      "w2v: alice, she, rabbit, he: 0.4352757480653299\n",
      "glove: alice, she, rabbit, he: 0.4769463106145698\n",
      "cbow: alice, she, rabbit, he: 0.02158846341550625\n",
      "cbow_dense: alice, she, rabbit, he: 0.0959762805982485\n",
      "skipgram: alice, she, rabbit, he: 0.08717707473555629\n",
      "skipgram_dense: alice, she, rabbit, he: 0.12265448691108558\n",
      "\n",
      "w2v: alice, her, rabbit, him: 0.4179929072878149\n",
      "glove: alice, her, rabbit, him: 0.4330083526998539\n",
      "cbow: alice, her, rabbit, him: 0.04790479808956305\n",
      "cbow_dense: alice, her, rabbit, him: 0.09920782186208495\n",
      "skipgram: alice, her, rabbit, him: 0.01711302802717834\n",
      "skipgram_dense: alice, her, rabbit, him: -0.009536472808953146\n",
      "\n",
      "w2v: alice, girl, rabbit, sir: 0.391360154857756\n",
      "glove: alice, girl, rabbit, sir: 0.3090358756196289\n",
      "cbow: alice, girl, rabbit, sir: 0.020245237921716373\n",
      "cbow_dense: alice, girl, rabbit, sir: 0.03404486457448577\n",
      "skipgram: alice, girl, rabbit, sir: -0.015589099372999526\n",
      "skipgram_dense: alice, girl, rabbit, sir: 0.17267874308297823\n",
      "\n",
      "w2v: his, her, he, she: 0.661601926269319\n",
      "glove: his, her, he, she: 0.7343965200095653\n",
      "cbow: his, her, he, she: 0.023657427031521093\n",
      "cbow_dense: his, her, he, she: -0.1228006092516608\n",
      "skipgram: his, her, he, she: -0.023710586771567784\n",
      "skipgram_dense: his, her, he, she: 0.0641726993931325\n",
      "\n",
      "w2v: long, longer, quick, quicker: 0.27729260342003326\n",
      "glove: long, longer, quick, quicker: 0.3099571305282724\n",
      "cbow: long, longer, quick, quicker: 0.0943593590121146\n",
      "cbow_dense: long, longer, quick, quicker: 0.07346261607150895\n",
      "skipgram: long, longer, quick, quicker: 0.0115626943979101\n",
      "skipgram_dense: long, longer, quick, quicker: 0.056064160828123114\n",
      "\n",
      "w2v: long, longer, small, smaller: 0.2388453687212515\n",
      "glove: long, longer, small, smaller: 0.45009013669954157\n",
      "cbow: long, longer, small, smaller: 0.06089632592231342\n",
      "cbow_dense: long, longer, small, smaller: 0.13468641151074714\n",
      "skipgram: long, longer, small, smaller: 0.0528596179085847\n",
      "skipgram_dense: long, longer, small, smaller: -0.033260428707802096\n",
      "\n",
      "w2v: long, longer, bad, worse: 0.2819685717087468\n",
      "glove: long, longer, bad, worse: 0.40626429575998596\n",
      "cbow: long, longer, bad, worse: 0.27068072880154254\n",
      "cbow_dense: long, longer, bad, worse: 0.17876813435616165\n",
      "skipgram: long, longer, bad, worse: -0.06755097366498111\n",
      "skipgram_dense: long, longer, bad, worse: 0.035440468745773696\n",
      "\n",
      "w2v: go, going, look, looking: 0.4437767197062708\n",
      "glove: go, going, look, looking: 0.6695781772903449\n",
      "cbow: go, going, look, looking: 0.0013563966233273222\n",
      "cbow_dense: go, going, look, looking: 0.16969527138432886\n",
      "skipgram: go, going, look, looking: 0.0857564110649774\n",
      "skipgram_dense: go, going, look, looking: 0.03098357758763984\n",
      "\n",
      "w2v: listen, listening, look, looking: 0.21896109979844153\n",
      "glove: listen, listening, look, looking: 0.4071841107300849\n",
      "cbow: listen, listening, look, looking: 0.00960132291661633\n",
      "cbow_dense: listen, listening, look, looking: 0.11961182797045093\n",
      "skipgram: listen, listening, look, looking: -0.0841335857178494\n",
      "skipgram_dense: listen, listening, look, looking: -0.037008750133731456\n",
      "\n",
      "w2v: swim, swimming, sit, sitting: 0.15879369259853693\n",
      "glove: swim, swimming, sit, sitting: 0.24853629762099705\n",
      "cbow: swim, swimming, sit, sitting: 0.08881370361878012\n",
      "cbow_dense: swim, swimming, sit, sitting: 0.10508880049784433\n",
      "skipgram: swim, swimming, sit, sitting: 0.035596227610360805\n",
      "skipgram_dense: swim, swimming, sit, sitting: -0.0007167028681988707\n",
      "\n",
      "w2v: run, running, listen, listening: 0.09999018014940893\n",
      "glove: run, running, listen, listening: 0.15729225189857585\n",
      "cbow: run, running, listen, listening: -0.03861840452743019\n",
      "cbow_dense: run, running, listen, listening: 0.035063353384430895\n",
      "skipgram: run, running, listen, listening: 0.020743869849819133\n",
      "skipgram_dense: run, running, listen, listening: 0.07100698165185709\n",
      "\n",
      "w2v: think, thinking, read, reading: 0.3014273799197356\n",
      "glove: think, thinking, read, reading: 0.411297884485633\n",
      "cbow: think, thinking, read, reading: 0.019524453048346102\n",
      "cbow_dense: think, thinking, read, reading: -0.04085618348009827\n",
      "skipgram: think, thinking, read, reading: 0.05569960656005628\n",
      "skipgram_dense: think, thinking, read, reading: 0.01165496543557438\n",
      "\n",
      "w2v: up, down, close, far: 0.31230518986077627\n",
      "glove: up, down, close, far: 0.6523693175355175\n",
      "cbow: up, down, close, far: -0.05967624532666053\n",
      "cbow_dense: up, down, close, far: -0.014005814944815826\n",
      "skipgram: up, down, close, far: 0.10219964024800547\n",
      "skipgram_dense: up, down, close, far: -0.03616082031316734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#w2v and glove analogy performance\n",
    "w2vanalogylist=[]\n",
    "gloveanalogylist=[]\n",
    "cbowanalogylist=[]\n",
    "cbowdenseanalogylist=[]\n",
    "skipgramanalogylist=[]\n",
    "skipgramdenseanalogylist=[]\n",
    "\n",
    "for analogy in analogy_list:\n",
    "    if(analogy[0] in tokenizer.word_index and analogy[1] in tokenizer.word_index and analogy[2] in tokenizer.word_index and analogy[3] in tokenizer.word_index):\n",
    "        try:\n",
    "            w2vanalogy=word2vec.n_similarity([analogy[0],analogy[1]],[analogy[2],analogy[3]])\n",
    "            gloveanalogy=glove.n_similarity([analogy[0],analogy[1]],[analogy[2],analogy[3]])\n",
    "            cbowanalogy=analogy_check(analogy,embed_cbow_300, tokenizer.word_index )\n",
    "            cbowdenseanalogy=analogy_check(analogy,embed_cbow_300_dense, tokenizer.word_index )\n",
    "            skipgramanalogy=analogy_check(analogy,embed_skipgram_300, tokenizer.word_index )\n",
    "            skipgramdenseanalogy=analogy_check(analogy,embed_skipgram_300_dense, tokenizer.word_index )\n",
    "            \n",
    "            print(\"w2v: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],w2vanalogy))\n",
    "            print(\"glove: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],gloveanalogy))\n",
    "            print(\"cbow: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],cbowanalogy))\n",
    "            print(\"cbow_dense: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],cbowdenseanalogy))\n",
    "            print(\"skipgram: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],skipgramanalogy))\n",
    "            print(\"skipgram_dense: {}, {}, {}, {}: {}\".format(analogy[0],analogy[1],analogy[2],analogy[3],skipgramdenseanalogy))\n",
    "            \n",
    "            \n",
    "            w2vanalogylist.append(w2vanalogy)\n",
    "            gloveanalogylist.append(gloveanalogy)\n",
    "            cbowanalogylist.append(cbowanalogy)\n",
    "            cbowdenseanalogylist.append(cbowdenseanalogy)\n",
    "            skipgramanalogylist.append(skipgramanalogy)\n",
    "            skipgramdenseanalogylist.append(skipgramdenseanalogy)\n",
    "            \n",
    "            \n",
    "            print()\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings\n",
    "def draw_word_vecs(X,strings):\n",
    "    col = ['r','b','g','y']\n",
    "    if len(X)!= len(strings):\n",
    "        print(\"mismatch in lengths between labels and vectors\")\n",
    "        \n",
    "    X_embedded = TSNE(n_components=4,method='exact').fit_transform(X)\n",
    "    maxt = np.amax(X_embedded)\n",
    "    mint = np.amin(X_embedded)\n",
    "    print(maxt,mint)\n",
    "    maxt = max(maxt,mint*-1)\n",
    "    for i in range(len(X_embedded)):\n",
    "        plt.quiver(X_embedded[i][0]/maxt,X_embedded[i][1]/maxt,X_embedded[i][2]/maxt,X_embedded[i][3]/maxt,angles='xy', scale_units='xy',color=col[i], scale=1,label=strings[i])\n",
    "        for val in X_embedded[i]:\n",
    "            print(val/maxt)\n",
    "    print(X_embedded)\n",
    "    plt.ylim(-1.2,1.2)\n",
    "    plt.xlim(-1.2,1.2)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'she': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'it': 7,\n",
       " 'of': 8,\n",
       " 'was': 9,\n",
       " 'in': 10,\n",
       " 'alice': 11,\n",
       " 'you': 12,\n",
       " 'that': 13,\n",
       " 'her': 14,\n",
       " 'as': 15,\n",
       " 'said': 16,\n",
       " 'had': 17,\n",
       " 'for': 18,\n",
       " 'but': 19,\n",
       " 'be': 20,\n",
       " 'on': 21,\n",
       " 'all': 22,\n",
       " 'with': 23,\n",
       " 'little': 24,\n",
       " 'mouse': 25,\n",
       " 'down': 26,\n",
       " 'very': 27,\n",
       " 'this': 28,\n",
       " 'not': 29,\n",
       " 'so': 30,\n",
       " 'out': 31,\n",
       " 'if': 32,\n",
       " 'is': 33,\n",
       " 'at': 34,\n",
       " 't': 35,\n",
       " 's': 36,\n",
       " 'll': 37,\n",
       " 'how': 38,\n",
       " 'they': 39,\n",
       " 'about': 40,\n",
       " 'herself': 41,\n",
       " 'me': 42,\n",
       " 'up': 43,\n",
       " 'what': 44,\n",
       " 'way': 45,\n",
       " 'when': 46,\n",
       " 'like': 47,\n",
       " 'one': 48,\n",
       " 'do': 49,\n",
       " 'no': 50,\n",
       " 'oh': 51,\n",
       " 'went': 52,\n",
       " 'thought': 53,\n",
       " 'again': 54,\n",
       " 'there': 55,\n",
       " 'see': 56,\n",
       " 'or': 57,\n",
       " 'could': 58,\n",
       " 'would': 59,\n",
       " 'think': 60,\n",
       " 'them': 61,\n",
       " 'know': 62,\n",
       " 'rabbit': 63,\n",
       " 'dear': 64,\n",
       " 'were': 65,\n",
       " 'time': 66,\n",
       " 'get': 67,\n",
       " 'here': 68,\n",
       " 'must': 69,\n",
       " 'my': 70,\n",
       " 'by': 71,\n",
       " 'into': 72,\n",
       " 'found': 73,\n",
       " 'such': 74,\n",
       " 'began': 75,\n",
       " 'soon': 76,\n",
       " 'm': 77,\n",
       " 'quite': 78,\n",
       " 'then': 79,\n",
       " 'off': 80,\n",
       " 'now': 81,\n",
       " 'go': 82,\n",
       " 'say': 83,\n",
       " 'have': 84,\n",
       " 'which': 85,\n",
       " 'come': 86,\n",
       " 'dinah': 87,\n",
       " 'your': 88,\n",
       " 'thing': 89,\n",
       " 'dodo': 90,\n",
       " 'much': 91,\n",
       " 'shall': 92,\n",
       " 'things': 93,\n",
       " 'long': 94,\n",
       " 'door': 95,\n",
       " 'who': 96,\n",
       " 'can': 97,\n",
       " 'once': 98,\n",
       " 'did': 99,\n",
       " 'over': 100,\n",
       " 'feet': 101,\n",
       " 'after': 102,\n",
       " 'wonder': 103,\n",
       " 'first': 104,\n",
       " 'let': 105,\n",
       " 'are': 106,\n",
       " 'cats': 107,\n",
       " 'round': 108,\n",
       " 'poor': 109,\n",
       " 'back': 110,\n",
       " 'he': 111,\n",
       " 'nothing': 112,\n",
       " 'seemed': 113,\n",
       " 'its': 114,\n",
       " 'never': 115,\n",
       " 'going': 116,\n",
       " 'great': 117,\n",
       " 'got': 118,\n",
       " 'ever': 119,\n",
       " 'table': 120,\n",
       " 'any': 121,\n",
       " 'only': 122,\n",
       " 'more': 123,\n",
       " 'cried': 124,\n",
       " 'gloves': 125,\n",
       " 'fan': 126,\n",
       " 'well': 127,\n",
       " 'white': 128,\n",
       " 'looked': 129,\n",
       " 'before': 130,\n",
       " 'moment': 131,\n",
       " 'why': 132,\n",
       " 'an': 133,\n",
       " 'eat': 134,\n",
       " 'however': 135,\n",
       " 'sure': 136,\n",
       " 'pool': 137,\n",
       " 'getting': 138,\n",
       " 'took': 139,\n",
       " 'either': 140,\n",
       " 'just': 141,\n",
       " 'upon': 142,\n",
       " 've': 143,\n",
       " 'through': 144,\n",
       " 'wish': 145,\n",
       " 'tell': 146,\n",
       " 'hall': 147,\n",
       " 'key': 148,\n",
       " 'half': 149,\n",
       " 'find': 150,\n",
       " 'his': 151,\n",
       " 'use': 152,\n",
       " 'large': 153,\n",
       " 'look': 154,\n",
       " 'too': 155,\n",
       " 'house': 156,\n",
       " 'good': 157,\n",
       " 'right': 158,\n",
       " 'words': 159,\n",
       " 'am': 160,\n",
       " 'might': 161,\n",
       " 'question': 162,\n",
       " 'came': 163,\n",
       " 'dry': 164,\n",
       " 'away': 165,\n",
       " 'been': 166,\n",
       " 'small': 167,\n",
       " 'high': 168,\n",
       " 'garden': 169,\n",
       " 'tears': 170,\n",
       " 'don': 171,\n",
       " 'won': 172,\n",
       " 'doesn': 173,\n",
       " 'without': 174,\n",
       " 'suddenly': 175,\n",
       " 'eyes': 176,\n",
       " 'some': 177,\n",
       " 'put': 178,\n",
       " 'sort': 179,\n",
       " 'their': 180,\n",
       " 'rather': 181,\n",
       " 'talking': 182,\n",
       " 'saying': 183,\n",
       " 'felt': 184,\n",
       " 'hand': 185,\n",
       " 'low': 186,\n",
       " 'trying': 187,\n",
       " 'golden': 188,\n",
       " 'head': 189,\n",
       " 'indeed': 190,\n",
       " 'will': 191,\n",
       " 'while': 192,\n",
       " 'same': 193,\n",
       " 'try': 194,\n",
       " 'beg': 195,\n",
       " 'tone': 196,\n",
       " 'our': 197,\n",
       " 'd': 198,\n",
       " 'called': 199,\n",
       " 'birds': 200,\n",
       " 'lory': 201,\n",
       " 'race': 202,\n",
       " 'looking': 203,\n",
       " 'course': 204,\n",
       " 'book': 205,\n",
       " 'made': 206,\n",
       " 'ran': 207,\n",
       " 'seen': 208,\n",
       " 'fell': 209,\n",
       " 'next': 210,\n",
       " 'tried': 211,\n",
       " 'make': 212,\n",
       " 'anything': 213,\n",
       " 'four': 214,\n",
       " 'nice': 215,\n",
       " 'people': 216,\n",
       " 'please': 217,\n",
       " 'should': 218,\n",
       " 'remember': 219,\n",
       " 'turned': 220,\n",
       " 'other': 221,\n",
       " 'glass': 222,\n",
       " 'than': 223,\n",
       " 'marked': 224,\n",
       " 'best': 225,\n",
       " 'sat': 226,\n",
       " 'two': 227,\n",
       " 'english': 228,\n",
       " 'pair': 229,\n",
       " 'heard': 230,\n",
       " 'kid': 231,\n",
       " 'ready': 232,\n",
       " 'voice': 233,\n",
       " 'queer': 234,\n",
       " 'changed': 235,\n",
       " 'swam': 236,\n",
       " 'talk': 237,\n",
       " 'offended': 238,\n",
       " 'we': 239,\n",
       " 'party': 240,\n",
       " 'prizes': 241,\n",
       " 'chapter': 242,\n",
       " 'hole': 243,\n",
       " 'tired': 244,\n",
       " 'having': 245,\n",
       " 'pictures': 246,\n",
       " 'own': 247,\n",
       " 'mind': 248,\n",
       " 'hot': 249,\n",
       " 'day': 250,\n",
       " 'pocket': 251,\n",
       " 'hurried': 252,\n",
       " 'take': 253,\n",
       " 'another': 254,\n",
       " 'coming': 255,\n",
       " 'from': 256,\n",
       " 'fall': 257,\n",
       " 'even': 258,\n",
       " 'near': 259,\n",
       " 'idea': 260,\n",
       " 'among': 261,\n",
       " 'didn': 262,\n",
       " 'ask': 263,\n",
       " 'air': 264,\n",
       " 'perhaps': 265,\n",
       " 'else': 266,\n",
       " 'cat': 267,\n",
       " 'afraid': 268,\n",
       " 'bats': 269,\n",
       " 'rate': 270,\n",
       " 'those': 271,\n",
       " 'really': 272,\n",
       " 'bottle': 273,\n",
       " 'children': 274,\n",
       " 'enough': 275,\n",
       " 'anxiously': 276,\n",
       " 'surprised': 277,\n",
       " 'speak': 278,\n",
       " 'hastily': 279,\n",
       " 'duchess': 280,\n",
       " 'kept': 281,\n",
       " 'everything': 282,\n",
       " 'mabel': 283,\n",
       " 'capital': 284,\n",
       " 'tail': 285,\n",
       " 'something': 286,\n",
       " 'william': 287,\n",
       " 'pardon': 288,\n",
       " 'crowded': 289,\n",
       " 'whole': 290,\n",
       " 'last': 291,\n",
       " 'old': 292,\n",
       " 'him': 293,\n",
       " 'thimble': 294,\n",
       " 'close': 295,\n",
       " 'hear': 296,\n",
       " 'itself': 297,\n",
       " 'late': 298,\n",
       " 'ought': 299,\n",
       " 'under': 300,\n",
       " 'world': 301,\n",
       " 'deep': 302,\n",
       " 'slowly': 303,\n",
       " 'happen': 304,\n",
       " 'dark': 305,\n",
       " 'noticed': 306,\n",
       " 'somebody': 307,\n",
       " 'home': 308,\n",
       " 'end': 309,\n",
       " 'many': 310,\n",
       " 'fallen': 311,\n",
       " 'several': 312,\n",
       " 'lessons': 313,\n",
       " 'still': 314,\n",
       " 'distance': 315,\n",
       " 'seem': 316,\n",
       " 'walk': 317,\n",
       " 'heads': 318,\n",
       " 'name': 319,\n",
       " 'ma': 320,\n",
       " 'fancy': 321,\n",
       " 're': 322,\n",
       " 'manage': 323,\n",
       " 'night': 324,\n",
       " 'mice': 325,\n",
       " 'answer': 326,\n",
       " 'passage': 327,\n",
       " 'sight': 328,\n",
       " 'behind': 329,\n",
       " 'every': 330,\n",
       " 'three': 331,\n",
       " 'alas': 332,\n",
       " 'inches': 333,\n",
       " 'along': 334,\n",
       " 'bright': 335,\n",
       " 'happened': 336,\n",
       " 'few': 337,\n",
       " 'drink': 338,\n",
       " 'hurry': 339,\n",
       " 'poison': 340,\n",
       " 'finger': 341,\n",
       " 'almost': 342,\n",
       " 'certain': 343,\n",
       " 'finished': 344,\n",
       " 'curious': 345,\n",
       " 'face': 346,\n",
       " 'candle': 347,\n",
       " 'generally': 348,\n",
       " 'remembered': 349,\n",
       " 'box': 350,\n",
       " 'left': 351,\n",
       " 'person': 352,\n",
       " 'eye': 353,\n",
       " 'lying': 354,\n",
       " 'cake': 355,\n",
       " 'life': 356,\n",
       " 'deal': 357,\n",
       " 'dropped': 358,\n",
       " 'times': 359,\n",
       " 'seems': 360,\n",
       " 'stay': 361,\n",
       " 'being': 362,\n",
       " 'sudden': 363,\n",
       " 'cause': 364,\n",
       " 'water': 365,\n",
       " 'o': 366,\n",
       " 'history': 367,\n",
       " 'trembling': 368,\n",
       " 'always': 369,\n",
       " 'dogs': 370,\n",
       " 'eagerly': 371,\n",
       " 'fetch': 372,\n",
       " 'us': 373,\n",
       " 'duck': 374,\n",
       " 'eaglet': 375,\n",
       " 'caucus': 376,\n",
       " 'tale': 377,\n",
       " 'replied': 378,\n",
       " 'turning': 379,\n",
       " 'sister': 380,\n",
       " 'bank': 381,\n",
       " 'considering': 382,\n",
       " 'feel': 383,\n",
       " 'sleepy': 384,\n",
       " 'stupid': 385,\n",
       " 'whether': 386,\n",
       " 'making': 387,\n",
       " 'worth': 388,\n",
       " 'trouble': 389,\n",
       " 'remarkable': 390,\n",
       " 'natural': 391,\n",
       " 'watch': 392,\n",
       " 'waistcoat': 393,\n",
       " 'started': 394,\n",
       " 'across': 395,\n",
       " 'falling': 396,\n",
       " 'filled': 397,\n",
       " 'cupboards': 398,\n",
       " 'shelves': 399,\n",
       " 'saw': 400,\n",
       " 'jar': 401,\n",
       " 'fear': 402,\n",
       " 'managed': 403,\n",
       " 'wouldn': 404,\n",
       " 'top': 405,\n",
       " 'likely': 406,\n",
       " 'miles': 407,\n",
       " 'aloud': 408,\n",
       " 'somewhere': 409,\n",
       " 'earth': 410,\n",
       " 'though': 411,\n",
       " 'opportunity': 412,\n",
       " 'knowledge': 413,\n",
       " 'listen': 414,\n",
       " 'yes': 415,\n",
       " 'latitude': 416,\n",
       " 'longitude': 417,\n",
       " 'funny': 418,\n",
       " 'glad': 419,\n",
       " 'new': 420,\n",
       " 'spoke': 421,\n",
       " 'girl': 422,\n",
       " 'asking': 423,\n",
       " 'miss': 424,\n",
       " 'catch': 425,\n",
       " 'bat': 426,\n",
       " 'sometimes': 427,\n",
       " 'matter': 428,\n",
       " 'begun': 429,\n",
       " 'walking': 430,\n",
       " 'thump': 431,\n",
       " 'bit': 432,\n",
       " 'hurt': 433,\n",
       " 'lost': 434,\n",
       " 'corner': 435,\n",
       " 'ears': 436,\n",
       " 'whiskers': 437,\n",
       " 'row': 438,\n",
       " 'roof': 439,\n",
       " 'doors': 440,\n",
       " 'side': 441,\n",
       " 'walked': 442,\n",
       " 'sadly': 443,\n",
       " 'middle': 444,\n",
       " 'tiny': 445,\n",
       " 'opened': 446,\n",
       " 'led': 447,\n",
       " 'larger': 448,\n",
       " 'shut': 449,\n",
       " 'telescope': 450,\n",
       " 'knew': 451,\n",
       " 'waiting': 452,\n",
       " 'hoping': 453,\n",
       " 'rules': 454,\n",
       " 'shutting': 455,\n",
       " 'certainly': 456,\n",
       " 'beautifully': 457,\n",
       " 'hold': 458,\n",
       " 'usually': 459,\n",
       " 'forgotten': 460,\n",
       " 'taste': 461,\n",
       " 'finding': 462,\n",
       " 'fact': 463,\n",
       " 'feeling': 464,\n",
       " 'size': 465,\n",
       " 'waited': 466,\n",
       " 'minutes': 467,\n",
       " 'altogether': 468,\n",
       " 'reach': 469,\n",
       " 'crying': 470,\n",
       " 'sharply': 471,\n",
       " 'leave': 472,\n",
       " 'minute': 473,\n",
       " 'gave': 474,\n",
       " 'severely': 475,\n",
       " 'against': 476,\n",
       " 'child': 477,\n",
       " 'fond': 478,\n",
       " 'makes': 479,\n",
       " 'grow': 480,\n",
       " 'happens': 481,\n",
       " 'holding': 482,\n",
       " 'growing': 483,\n",
       " 'curiouser': 484,\n",
       " 'forgot': 485,\n",
       " 'far': 486,\n",
       " 'dears': 487,\n",
       " 'give': 488,\n",
       " 'sending': 489,\n",
       " 'foot': 490,\n",
       " 'nonsense': 491,\n",
       " 'nine': 492,\n",
       " 'cry': 493,\n",
       " 'yourself': 494,\n",
       " 'stop': 495,\n",
       " 'pattering': 496,\n",
       " 'trotting': 497,\n",
       " 'muttering': 498,\n",
       " 'help': 499,\n",
       " 'sir': 500,\n",
       " 'hard': 501,\n",
       " 'morning': 502,\n",
       " 'ah': 503,\n",
       " 'age': 504,\n",
       " 'hair': 505,\n",
       " 'ringlets': 506,\n",
       " 'mine': 507,\n",
       " 'sorts': 508,\n",
       " 'puzzling': 509,\n",
       " 'used': 510,\n",
       " 'paris': 511,\n",
       " 'rome': 512,\n",
       " 'doth': 513,\n",
       " 'hands': 514,\n",
       " 'alone': 515,\n",
       " 'done': 516,\n",
       " 'shrinking': 517,\n",
       " 'frightened': 518,\n",
       " 'change': 519,\n",
       " 'bad': 520,\n",
       " 'slipped': 521,\n",
       " 'salt': 522,\n",
       " 'sea': 523,\n",
       " 'case': 524,\n",
       " 'railway': 525,\n",
       " 'hadn': 526,\n",
       " 'suppose': 527,\n",
       " 'swimming': 528,\n",
       " 'speaking': 529,\n",
       " 'understand': 530,\n",
       " 'french': 531,\n",
       " 'conqueror': 532,\n",
       " 'lesson': 533,\n",
       " 'angry': 534,\n",
       " 'show': 535,\n",
       " 'paws': 536,\n",
       " 'nurse': 537,\n",
       " 'catching': 538,\n",
       " 'subject': 539,\n",
       " 'sit': 540,\n",
       " 'says': 541,\n",
       " 'useful': 542,\n",
       " 'shore': 543,\n",
       " 'hate': 544,\n",
       " 'animals': 545,\n",
       " 'fur': 546,\n",
       " 'wet': 547,\n",
       " 'better': 548,\n",
       " 'ring': 549,\n",
       " 'silence': 550,\n",
       " 'wanted': 551,\n",
       " 'edwin': 552,\n",
       " 'morcar': 553,\n",
       " 'earls': 554,\n",
       " 'mercia': 555,\n",
       " 'northumbria': 556,\n",
       " 'advisable': 557,\n",
       " 'meet': 558,\n",
       " 'melancholy': 559,\n",
       " 'solemnly': 560,\n",
       " 'explain': 561,\n",
       " 'running': 562,\n",
       " 'liked': 563,\n",
       " 'has': 564,\n",
       " 'chorus': 565,\n",
       " 'comfits': 566,\n",
       " 'speech': 567,\n",
       " 'caused': 568,\n",
       " 'sad': 569,\n",
       " 'fury': 570,\n",
       " 'trial': 571,\n",
       " 'jury': 572,\n",
       " 'judge': 573,\n",
       " 'finish': 574,\n",
       " 'story': 575,\n",
       " 'crab': 576,\n",
       " 'nobody': 577,\n",
       " 'ferrets': 578,\n",
       " 'hunting': 579,\n",
       " 'mary': 580,\n",
       " 'ann': 581,\n",
       " 'messages': 582,\n",
       " 'room': 583,\n",
       " 'beginning': 584,\n",
       " 'sitting': 585,\n",
       " 'twice': 586,\n",
       " 'peeped': 587,\n",
       " 'reading': 588,\n",
       " 'conversations': 589,\n",
       " 'pleasure': 590,\n",
       " 'daisy': 591,\n",
       " 'chain': 592,\n",
       " 'picking': 593,\n",
       " 'daisies': 594,\n",
       " 'pink': 595,\n",
       " 'nor': 596,\n",
       " 'afterwards': 597,\n",
       " 'occurred': 598,\n",
       " 'wondered': 599,\n",
       " 'actually': 600,\n",
       " 'flashed': 601,\n",
       " 'burning': 602,\n",
       " 'curiosity': 603,\n",
       " 'field': 604,\n",
       " 'fortunately': 605,\n",
       " 'pop': 606,\n",
       " 'hedge': 607,\n",
       " 'straight': 608,\n",
       " 'tunnel': 609,\n",
       " 'dipped': 610,\n",
       " 'stopping': 611,\n",
       " 'plenty': 612,\n",
       " 'sides': 613,\n",
       " 'maps': 614,\n",
       " 'hung': 615,\n",
       " 'pegs': 616,\n",
       " 'passed': 617,\n",
       " 'labelled': 618,\n",
       " 'orange': 619,\n",
       " 'marmalade': 620,\n",
       " 'disappointment': 621,\n",
       " 'empty': 622,\n",
       " 'drop': 623,\n",
       " 'killing': 624,\n",
       " 'past': 625,\n",
       " 'tumbling': 626,\n",
       " 'stairs': 627,\n",
       " 'brave': 628,\n",
       " 'true': 629,\n",
       " 'centre': 630,\n",
       " 'thousand': 631,\n",
       " 'learnt': 632,\n",
       " 'schoolroom': 633,\n",
       " 'showing': 634,\n",
       " 'practice': 635,\n",
       " 'grand': 636,\n",
       " 'presently': 637,\n",
       " 'downward': 638,\n",
       " 'antipathies': 639,\n",
       " 'listening': 640,\n",
       " 'sound': 641,\n",
       " 'word': 642,\n",
       " 'country': 643,\n",
       " 'zealand': 644,\n",
       " 'australia': 645,\n",
       " 'curtsey': 646,\n",
       " 'curtseying': 647,\n",
       " 'ignorant': 648,\n",
       " 'written': 649,\n",
       " 'hope': 650,\n",
       " 'saucer': 651,\n",
       " 'milk': 652,\n",
       " 'tea': 653,\n",
       " 'dreamy': 654,\n",
       " 'couldn': 655,\n",
       " 'dozing': 656,\n",
       " 'dream': 657,\n",
       " 'earnestly': 658,\n",
       " 'truth': 659,\n",
       " 'heap': 660,\n",
       " 'sticks': 661,\n",
       " 'leaves': 662,\n",
       " 'jumped': 663,\n",
       " 'overhead': 664,\n",
       " 'hurrying': 665,\n",
       " 'wind': 666,\n",
       " 'longer': 667,\n",
       " 'lit': 668,\n",
       " 'lamps': 669,\n",
       " 'hanging': 670,\n",
       " 'locked': 671,\n",
       " 'wondering': 672,\n",
       " 'legged': 673,\n",
       " 'solid': 674,\n",
       " 'except': 675,\n",
       " 'belong': 676,\n",
       " 'locks': 677,\n",
       " 'open': 678,\n",
       " 'second': 679,\n",
       " 'curtain': 680,\n",
       " 'fifteen': 681,\n",
       " 'lock': 682,\n",
       " 'delight': 683,\n",
       " 'fitted': 684,\n",
       " 'rat': 685,\n",
       " 'knelt': 686,\n",
       " 'loveliest': 687,\n",
       " 'longed': 688,\n",
       " 'wander': 689,\n",
       " 'beds': 690,\n",
       " 'flowers': 691,\n",
       " 'cool': 692,\n",
       " 'fountains': 693,\n",
       " 'doorway': 694,\n",
       " 'shoulders': 695,\n",
       " 'begin': 696,\n",
       " 'lately': 697,\n",
       " 'telescopes': 698,\n",
       " 'neck': 699,\n",
       " 'paper': 700,\n",
       " 'label': 701,\n",
       " 'printed': 702,\n",
       " 'wise': 703,\n",
       " 'read': 704,\n",
       " 'histories': 705,\n",
       " 'burnt': 706,\n",
       " 'eaten': 707,\n",
       " 'wild': 708,\n",
       " 'beasts': 709,\n",
       " 'unpleasant': 710,\n",
       " 'because': 711,\n",
       " 'simple': 712,\n",
       " 'friends': 713,\n",
       " 'taught': 714,\n",
       " 'red': 715,\n",
       " 'poker': 716,\n",
       " 'burn': 717,\n",
       " 'cut': 718,\n",
       " 'deeply': 719,\n",
       " 'knife': 720,\n",
       " 'bleeds': 721,\n",
       " 'disagree': 722,\n",
       " 'sooner': 723,\n",
       " 'later': 724,\n",
       " 'ventured': 725,\n",
       " 'mixed': 726,\n",
       " 'flavour': 727,\n",
       " 'cherry': 728,\n",
       " 'tart': 729,\n",
       " 'custard': 730,\n",
       " 'pine': 731,\n",
       " 'apple': 732,\n",
       " 'roast': 733,\n",
       " 'turkey': 734,\n",
       " 'toffee': 735,\n",
       " 'buttered': 736,\n",
       " 'toast': 737,\n",
       " 'ten': 738,\n",
       " 'brightened': 739,\n",
       " 'lovely': 740,\n",
       " 'shrink': 741,\n",
       " 'further': 742,\n",
       " 'nervous': 743,\n",
       " 'flame': 744,\n",
       " 'blown': 745,\n",
       " 'decided': 746,\n",
       " 'possibly': 747,\n",
       " 'plainly': 748,\n",
       " 'climb': 749,\n",
       " 'legs': 750,\n",
       " 'slippery': 751,\n",
       " 'advise': 752,\n",
       " 'advice': 753,\n",
       " 'seldom': 754,\n",
       " 'followed': 755,\n",
       " 'scolded': 756,\n",
       " 'bring': 757,\n",
       " 'cheated': 758,\n",
       " 'game': 759,\n",
       " 'croquet': 760,\n",
       " 'playing': 761,\n",
       " 'pretending': 762,\n",
       " 'pretend': 763,\n",
       " 'hardly': 764,\n",
       " 'respectable': 765,\n",
       " 'currants': 766,\n",
       " 'smaller': 767,\n",
       " 'creep': 768,\n",
       " 'care': 769,\n",
       " 'ate': 770,\n",
       " 'remained': 771,\n",
       " 'eats': 772,\n",
       " 'expecting': 773,\n",
       " 'dull': 774,\n",
       " 'common': 775,\n",
       " 'set': 776,\n",
       " 'work': 777,\n",
       " 'ii': 778,\n",
       " 'opening': 779,\n",
       " 'largest': 780,\n",
       " 'bye': 781,\n",
       " 'shoes': 782,\n",
       " 'stockings': 783,\n",
       " 'shan': 784,\n",
       " 'able': 785,\n",
       " 'myself': 786,\n",
       " 'kind': 787,\n",
       " 'want': 788,\n",
       " 'boots': 789,\n",
       " 'christmas': 790,\n",
       " 'planning': 791,\n",
       " 'carrier': 792,\n",
       " 'presents': 793,\n",
       " 'odd': 794,\n",
       " 'directions': 795,\n",
       " 'esq': 796,\n",
       " 'hearthrug': 797,\n",
       " 'fender': 798,\n",
       " 'love': 799,\n",
       " 'struck': 800,\n",
       " 'hopeless': 801,\n",
       " 'ashamed': 802,\n",
       " 'shedding': 803,\n",
       " 'gallons': 804,\n",
       " 'until': 805,\n",
       " 'reaching': 806,\n",
       " 'dried': 807,\n",
       " 'returning': 808,\n",
       " 'splendidly': 809,\n",
       " 'dressed': 810,\n",
       " 'himself': 811,\n",
       " 'savage': 812,\n",
       " 'desperate': 813,\n",
       " 'timid': 814,\n",
       " 'violently': 815,\n",
       " 'skurried': 816,\n",
       " 'darkness': 817,\n",
       " 'fanning': 818,\n",
       " 'yesterday': 819,\n",
       " 'usual': 820,\n",
       " 'different': 821,\n",
       " 'puzzle': 822,\n",
       " 'thinking': 823,\n",
       " 'ada': 824,\n",
       " 'goes': 825,\n",
       " 'knows': 826,\n",
       " 'besides': 827,\n",
       " 'five': 828,\n",
       " 'twelve': 829,\n",
       " 'six': 830,\n",
       " 'thirteen': 831,\n",
       " 'seven': 832,\n",
       " 'twenty': 833,\n",
       " 'multiplication': 834,\n",
       " 'signify': 835,\n",
       " 'geography': 836,\n",
       " 'london': 837,\n",
       " 'wrong': 838,\n",
       " 'crossed': 839,\n",
       " 'lap': 840,\n",
       " 'repeat': 841,\n",
       " 'sounded': 842,\n",
       " 'hoarse': 843,\n",
       " 'strange': 844,\n",
       " 'crocodile': 845,\n",
       " 'improve': 846,\n",
       " 'shining': 847,\n",
       " 'pour': 848,\n",
       " 'waters': 849,\n",
       " 'nile': 850,\n",
       " 'scale': 851,\n",
       " 'cheerfully': 852,\n",
       " 'grin': 853,\n",
       " 'neatly': 854,\n",
       " 'spread': 855,\n",
       " 'claws': 856,\n",
       " 'welcome': 857,\n",
       " 'fishes': 858,\n",
       " 'gently': 859,\n",
       " 'smiling': 860,\n",
       " 'jaws': 861,\n",
       " 'live': 862,\n",
       " 'poky': 863,\n",
       " 'toys': 864,\n",
       " 'play': 865,\n",
       " 'learn': 866,\n",
       " 'putting': 867,\n",
       " 'till': 868,\n",
       " 'burst': 869,\n",
       " 'measure': 870,\n",
       " 'nearly': 871,\n",
       " 'guess': 872,\n",
       " 'rapidly': 873,\n",
       " 'avoid': 874,\n",
       " 'narrow': 875,\n",
       " 'escape': 876,\n",
       " 'existence': 877,\n",
       " 'speed': 878,\n",
       " 'worse': 879,\n",
       " 'declare': 880,\n",
       " 'these': 881,\n",
       " 'splash': 882,\n",
       " 'chin': 883,\n",
       " 'somehow': 884,\n",
       " 'seaside': 885,\n",
       " 'general': 886,\n",
       " 'conclusion': 887,\n",
       " 'wherever': 888,\n",
       " 'coast': 889,\n",
       " 'number': 890,\n",
       " 'bathing': 891,\n",
       " 'machines': 892,\n",
       " 'digging': 893,\n",
       " 'sand': 894,\n",
       " 'wooden': 895,\n",
       " 'spades': 896,\n",
       " 'lodging': 897,\n",
       " 'houses': 898,\n",
       " 'station': 899,\n",
       " 'wept': 900,\n",
       " 'punished': 901,\n",
       " 'drowned': 902,\n",
       " 'splashing': 903,\n",
       " 'nearer': 904,\n",
       " 'walrus': 905,\n",
       " 'hippopotamus': 906,\n",
       " 'harm': 907,\n",
       " 'brother': 908,\n",
       " 'latin': 909,\n",
       " 'grammar': 910,\n",
       " 'inquisitively': 911,\n",
       " 'wink': 912,\n",
       " 'daresay': 913,\n",
       " 'clear': 914,\n",
       " 'notion': 915,\n",
       " 'ago': 916,\n",
       " 'ou': 917,\n",
       " 'est': 918,\n",
       " 'chatte': 919,\n",
       " 'sentence': 920,\n",
       " 'leap': 921,\n",
       " 'quiver': 922,\n",
       " 'fright': 923,\n",
       " 'animal': 924,\n",
       " 'feelings': 925,\n",
       " 'shrill': 926,\n",
       " 'passionate': 927,\n",
       " 'soothing': 928,\n",
       " 'yet': 929,\n",
       " 'quiet': 930,\n",
       " 'lazily': 931,\n",
       " 'sits': 932,\n",
       " 'purring': 933,\n",
       " 'nicely': 934,\n",
       " 'fire': 935,\n",
       " 'licking': 936,\n",
       " 'washing': 937,\n",
       " 'soft': 938,\n",
       " 'bristling': 939,\n",
       " 'family': 940,\n",
       " 'hated': 941,\n",
       " 'nasty': 942,\n",
       " 'vulgar': 943,\n",
       " 'conversation': 944,\n",
       " 'dog': 945,\n",
       " 'eyed': 946,\n",
       " 'terrier': 947,\n",
       " 'curly': 948,\n",
       " 'brown': 949,\n",
       " 'throw': 950,\n",
       " 'dinner': 951,\n",
       " 'belongs': 952,\n",
       " 'farmer': 953,\n",
       " 'hundred': 954,\n",
       " 'pounds': 955,\n",
       " 'kills': 956,\n",
       " 'rats': 957,\n",
       " 'sorrowful': 958,\n",
       " 'commotion': 959,\n",
       " 'softly': 960,\n",
       " 'pale': 961,\n",
       " 'passion': 962,\n",
       " 'creatures': 963,\n",
       " 'iii': 964,\n",
       " 'assembled': 965,\n",
       " 'draggled': 966,\n",
       " 'feathers': 967,\n",
       " 'clinging': 968,\n",
       " 'dripping': 969,\n",
       " 'cross': 970,\n",
       " 'uncomfortable': 971,\n",
       " 'consultation': 972,\n",
       " 'familiarly': 973,\n",
       " 'known': 974,\n",
       " 'argument': 975,\n",
       " 'sulky': 976,\n",
       " 'older': 977,\n",
       " 'allow': 978,\n",
       " 'knowing': 979,\n",
       " 'positively': 980,\n",
       " 'refused': 981,\n",
       " 'authority': 982,\n",
       " 'fixed': 983,\n",
       " 'cold': 984,\n",
       " 'ahem': 985,\n",
       " 'important': 986,\n",
       " 'driest': 987,\n",
       " 'whose': 988,\n",
       " 'favoured': 989,\n",
       " 'pope': 990,\n",
       " 'submitted': 991,\n",
       " 'leaders': 992,\n",
       " 'accustomed': 993,\n",
       " 'usurpation': 994,\n",
       " 'conquest': 995,\n",
       " 'ugh': 996,\n",
       " 'shiver': 997,\n",
       " 'frowning': 998,\n",
       " 'politely': 999,\n",
       " 'proceed': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pleasanter' in tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
