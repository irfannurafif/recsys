{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj4T8PEHGbMF"
   },
   "source": [
    "# Assignment 3\n",
    "## Question 1: Siamese networks & one-shot learning (8pt)\n",
    "The Cifar-100 dataset is similar to the Cifar-10 dataset. It also consists of 60,000 32x32 RGB images, but they are distributed over 100 classes instead of 10. Thus, each class has much less examples, only 500 training images and 100 testing images per class. For more info about the dataset, see https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "*HINT: Import the Cifar-100 dataset directly from Keras, no need to download it from the website. Use* `label_mode=\"fine\"`\n",
    "\n",
    "### Task 1.1: Siamese network\n",
    "**a)**\n",
    "* Train a Siamese Network on the first 80 classes of (the training set of) Cifar-100, i.e. let the network predict the probability that two input images are from the same class. Use 1 as a target for pairs of images from the same class (positive pairs), and 0 for pairs of images from different classes (negative pairs). Randomly select image pairs from Cifar-100, but make sure you train on as many positive pairs as negative pairs.\n",
    "\n",
    "* Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\n",
    "\n",
    "For this question you may ignore the test set of Cifar-100; it suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
    "\n",
    "*HINT: First sort the data by their labels (see e.g.* `numpy.argsort()`*), then reshape the data to a shape of* `(n_classes, n_examples, width, height, depth)`*, similar to the Omniglot data in Practical 4. It is then easier to split the data by class, and to sample positive and negative images pairs for training the Siamese network.*\n",
    "\n",
    "*NOTE: do not expect the one-shot accuracy for Cifar-100 to be similar to that accuracy for Omniglot; a lower accuracy can be expected. However, accuracy higher than random guess is certainly achievable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# === add code here ===\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19],\n",
       "       [29],\n",
       "       [ 0],\n",
       "       ...,\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [73]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"X_test shape:\", x_test.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_p=np.argsort(y_train.flatten(),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sam=[]\n",
    "cnt=0\n",
    "lst_id=0\n",
    "for i in range(50000):\n",
    "    if(y_train[sort_p[i]]!=lst_id):\n",
    "        n_sam.append(cnt)\n",
    "        lst_id=y_train[sort_p[i]]\n",
    "        cnt=1\n",
    "    else:\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[80]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n",
      "[81]\n"
     ]
    }
   ],
   "source": [
    "for i in range(40000,41000):\n",
    "    print(y_train[sort_p[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new=[x_train[sort_p[i]].flatten() for i in range(50000)]\n",
    "y_train_new=[y_train[sort_p[i]].flatten() for i in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new2=np.reshape(x_train_new,(100, 500, 32, 32, 3))\n",
    "y_train_new2=np.reshape(y_train_new,(100, 500, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[170 174 173]\n",
      "  [171 175 174]\n",
      "  [174 178 177]\n",
      "  ...\n",
      "  [209 209 209]\n",
      "  [209 209 209]\n",
      "  [208 208 208]]\n",
      "\n",
      " [[170 174 173]\n",
      "  [171 175 174]\n",
      "  [174 178 177]\n",
      "  ...\n",
      "  [207 207 207]\n",
      "  [207 207 207]\n",
      "  [206 206 206]]\n",
      "\n",
      " [[172 176 175]\n",
      "  [173 177 176]\n",
      "  [176 180 179]\n",
      "  ...\n",
      "  [210 210 210]\n",
      "  [209 209 209]\n",
      "  [208 208 208]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[175 175 175]\n",
      "  [175 177 176]\n",
      "  [167 172 168]\n",
      "  ...\n",
      "  [183 155 158]\n",
      "  [219 222 220]\n",
      "  [218 218 218]]\n",
      "\n",
      " [[170 170 170]\n",
      "  [168 171 171]\n",
      "  [172 177 176]\n",
      "  ...\n",
      "  [218 216 217]\n",
      "  [220 217 218]\n",
      "  [215 214 214]]\n",
      "\n",
      " [[169 169 169]\n",
      "  [170 171 171]\n",
      "  [173 175 175]\n",
      "  ...\n",
      "  [219 218 218]\n",
      "  [216 216 216]\n",
      "  [213 214 214]]]\n",
      "====\n",
      "[[[170 174 173]\n",
      "  [171 175 174]\n",
      "  [174 178 177]\n",
      "  ...\n",
      "  [209 209 209]\n",
      "  [209 209 209]\n",
      "  [208 208 208]]\n",
      "\n",
      " [[170 174 173]\n",
      "  [171 175 174]\n",
      "  [174 178 177]\n",
      "  ...\n",
      "  [207 207 207]\n",
      "  [207 207 207]\n",
      "  [206 206 206]]\n",
      "\n",
      " [[172 176 175]\n",
      "  [173 177 176]\n",
      "  [176 180 179]\n",
      "  ...\n",
      "  [210 210 210]\n",
      "  [209 209 209]\n",
      "  [208 208 208]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[175 175 175]\n",
      "  [175 177 176]\n",
      "  [167 172 168]\n",
      "  ...\n",
      "  [183 155 158]\n",
      "  [219 222 220]\n",
      "  [218 218 218]]\n",
      "\n",
      " [[170 170 170]\n",
      "  [168 171 171]\n",
      "  [172 177 176]\n",
      "  ...\n",
      "  [218 216 217]\n",
      "  [220 217 218]\n",
      "  [215 214 214]]\n",
      "\n",
      " [[169 169 169]\n",
      "  [170 171 171]\n",
      "  [173 175 175]\n",
      "  ...\n",
      "  [219 218 218]\n",
      "  [216 216 216]\n",
      "  [213 214 214]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[sort_p[0]])\n",
    "print(\"====\")\n",
    "print(x_train_new2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final=x_train_new2[0:80]\n",
    "y_train_final=y_train_new2[0:80]\n",
    "x_test_final=x_train_new2[80:]\n",
    "y_test_final=y_train_new2[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 44  49  49]\n",
      "   [ 46  50  47]\n",
      "   [ 49  51  50]\n",
      "   ...\n",
      "   [ 57  58  54]\n",
      "   [ 51  51  52]\n",
      "   [ 23  24  28]]\n",
      "\n",
      "  [[ 47  48  46]\n",
      "   [ 52  52  47]\n",
      "   [ 55  53  50]\n",
      "   ...\n",
      "   [ 49  50  46]\n",
      "   [ 42  41  41]\n",
      "   [ 22  22  25]]\n",
      "\n",
      "  [[ 52  50  44]\n",
      "   [ 63  60  53]\n",
      "   [ 67  62  56]\n",
      "   ...\n",
      "   [ 62  61  58]\n",
      "   [ 49  47  46]\n",
      "   [ 18  19  21]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[122 104  91]\n",
      "   [123 107  93]\n",
      "   [144 128 115]\n",
      "   ...\n",
      "   [104  96  80]\n",
      "   [110  97  84]\n",
      "   [110  93  80]]\n",
      "\n",
      "  [[123 109  98]\n",
      "   [134 119 108]\n",
      "   [149 135 124]\n",
      "   ...\n",
      "   [107  99  83]\n",
      "   [112  99  85]\n",
      "   [122 104  92]]\n",
      "\n",
      "  [[102  95  89]\n",
      "   [121 113 106]\n",
      "   [133 124 117]\n",
      "   ...\n",
      "   [113 105  88]\n",
      "   [112  99  86]\n",
      "   [124 106  94]]]\n",
      "\n",
      "\n",
      " [[[ 46  50  59]\n",
      "   [ 48  52  63]\n",
      "   [ 51  53  65]\n",
      "   ...\n",
      "   [213 210 204]\n",
      "   [207 202 193]\n",
      "   [193 188 180]]\n",
      "\n",
      "  [[ 47  51  62]\n",
      "   [ 52  55  67]\n",
      "   [ 53  56  68]\n",
      "   ...\n",
      "   [219 214 206]\n",
      "   [213 206 199]\n",
      "   [207 200 193]]\n",
      "\n",
      "  [[ 51  55  66]\n",
      "   [ 53  57  68]\n",
      "   [ 55  58  69]\n",
      "   ...\n",
      "   [218 211 204]\n",
      "   [210 203 195]\n",
      "   [206 199 192]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 42  42  50]\n",
      "   [ 41  42  49]\n",
      "   [ 44  44  51]\n",
      "   ...\n",
      "   [201 194 181]\n",
      "   [206 200 188]\n",
      "   [202 194 182]]\n",
      "\n",
      "  [[ 41  41  48]\n",
      "   [ 42  43  49]\n",
      "   [ 35  35  40]\n",
      "   ...\n",
      "   [200 193 181]\n",
      "   [199 194 183]\n",
      "   [198 192 180]]\n",
      "\n",
      "  [[ 40  41  46]\n",
      "   [ 35  36  39]\n",
      "   [ 19  19  20]\n",
      "   ...\n",
      "   [200 193 180]\n",
      "   [189 184 172]\n",
      "   [188 183 171]]]\n",
      "\n",
      "\n",
      " [[[184 186 165]\n",
      "   [185 187 166]\n",
      "   [191 193 171]\n",
      "   ...\n",
      "   [182 185 166]\n",
      "   [193 196 177]\n",
      "   [200 203 184]]\n",
      "\n",
      "  [[177 179 158]\n",
      "   [173 175 154]\n",
      "   [179 181 160]\n",
      "   ...\n",
      "   [177 180 161]\n",
      "   [183 186 167]\n",
      "   [191 194 175]]\n",
      "\n",
      "  [[176 178 157]\n",
      "   [175 177 156]\n",
      "   [180 182 161]\n",
      "   ...\n",
      "   [180 183 164]\n",
      "   [182 185 166]\n",
      "   [190 193 174]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175 175 150]\n",
      "   [175 175 151]\n",
      "   [178 178 154]\n",
      "   ...\n",
      "   [200 201 183]\n",
      "   [190 191 173]\n",
      "   [181 182 164]]\n",
      "\n",
      "  [[172 172 147]\n",
      "   [173 173 148]\n",
      "   [175 175 150]\n",
      "   ...\n",
      "   [189 190 172]\n",
      "   [192 193 175]\n",
      "   [187 188 170]]\n",
      "\n",
      "  [[184 183 162]\n",
      "   [182 182 160]\n",
      "   [182 181 160]\n",
      "   ...\n",
      "   [179 180 162]\n",
      "   [188 189 170]\n",
      "   [191 192 174]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[249 250 245]\n",
      "   [251 252 247]\n",
      "   [251 252 247]\n",
      "   ...\n",
      "   [229 230 222]\n",
      "   [248 251 242]\n",
      "   [247 252 245]]\n",
      "\n",
      "  [[245 246 241]\n",
      "   [248 249 244]\n",
      "   [248 249 244]\n",
      "   ...\n",
      "   [217 218 206]\n",
      "   [245 248 237]\n",
      "   [244 249 242]]\n",
      "\n",
      "  [[246 247 242]\n",
      "   [248 249 244]\n",
      "   [248 249 244]\n",
      "   ...\n",
      "   [216 218 203]\n",
      "   [246 249 237]\n",
      "   [244 248 241]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[241 246 241]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [246 249 243]\n",
      "   [246 248 241]\n",
      "   [241 245 239]]\n",
      "\n",
      "  [[241 246 240]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [246 249 242]\n",
      "   [246 249 242]\n",
      "   [241 246 239]]\n",
      "\n",
      "  [[242 247 241]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [245 249 242]\n",
      "   [245 249 242]\n",
      "   [242 247 240]]]\n",
      "\n",
      "\n",
      " [[[239 238 234]\n",
      "   [251 250 246]\n",
      "   [251 250 246]\n",
      "   ...\n",
      "   [249 250 244]\n",
      "   [249 250 244]\n",
      "   [249 250 244]]\n",
      "\n",
      "  [[238 237 233]\n",
      "   [249 248 244]\n",
      "   [249 248 244]\n",
      "   ...\n",
      "   [247 248 242]\n",
      "   [247 248 242]\n",
      "   [248 249 243]]\n",
      "\n",
      "  [[238 237 233]\n",
      "   [250 249 245]\n",
      "   [249 248 244]\n",
      "   ...\n",
      "   [247 248 242]\n",
      "   [248 249 243]\n",
      "   [248 249 243]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[237 233 224]\n",
      "   [250 247 239]\n",
      "   [249 247 241]\n",
      "   ...\n",
      "   [245 245 237]\n",
      "   [244 244 236]\n",
      "   [244 244 236]]\n",
      "\n",
      "  [[238 233 224]\n",
      "   [248 245 237]\n",
      "   [246 244 238]\n",
      "   ...\n",
      "   [245 245 237]\n",
      "   [244 245 236]\n",
      "   [244 244 236]]\n",
      "\n",
      "  [[237 232 225]\n",
      "   [248 243 237]\n",
      "   [247 244 238]\n",
      "   ...\n",
      "   [247 247 239]\n",
      "   [248 247 240]\n",
      "   [247 246 239]]]\n",
      "\n",
      "\n",
      " [[[179 152  81]\n",
      "   [177 151  80]\n",
      "   [179 153  82]\n",
      "   ...\n",
      "   [173 150  86]\n",
      "   [172 149  85]\n",
      "   [169 147  83]]\n",
      "\n",
      "  [[180 153  77]\n",
      "   [176 150  74]\n",
      "   [175 149  74]\n",
      "   ...\n",
      "   [177 155  91]\n",
      "   [175 153  88]\n",
      "   [171 151  86]]\n",
      "\n",
      "  [[177 151  73]\n",
      "   [172 146  68]\n",
      "   [168 143  64]\n",
      "   ...\n",
      "   [181 160  97]\n",
      "   [176 155  92]\n",
      "   [172 151  89]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 147  80]\n",
      "   [175 148  82]\n",
      "   [184 157  90]\n",
      "   ...\n",
      "   [158 133  77]\n",
      "   [155 130  74]\n",
      "   [149 124  70]]\n",
      "\n",
      "  [[174 147  79]\n",
      "   [176 149  82]\n",
      "   [185 158  91]\n",
      "   ...\n",
      "   [159 134  78]\n",
      "   [156 131  75]\n",
      "   [150 125  70]]\n",
      "\n",
      "  [[170 143  76]\n",
      "   [173 146  80]\n",
      "   [181 154  88]\n",
      "   ...\n",
      "   [157 132  77]\n",
      "   [154 129  74]\n",
      "   [147 122  68]]]]\n",
      "============================\n",
      "[[[[ 44  49  49]\n",
      "   [ 46  50  47]\n",
      "   [ 49  51  50]\n",
      "   ...\n",
      "   [ 57  58  54]\n",
      "   [ 51  51  52]\n",
      "   [ 23  24  28]]\n",
      "\n",
      "  [[ 47  48  46]\n",
      "   [ 52  52  47]\n",
      "   [ 55  53  50]\n",
      "   ...\n",
      "   [ 49  50  46]\n",
      "   [ 42  41  41]\n",
      "   [ 22  22  25]]\n",
      "\n",
      "  [[ 52  50  44]\n",
      "   [ 63  60  53]\n",
      "   [ 67  62  56]\n",
      "   ...\n",
      "   [ 62  61  58]\n",
      "   [ 49  47  46]\n",
      "   [ 18  19  21]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[122 104  91]\n",
      "   [123 107  93]\n",
      "   [144 128 115]\n",
      "   ...\n",
      "   [104  96  80]\n",
      "   [110  97  84]\n",
      "   [110  93  80]]\n",
      "\n",
      "  [[123 109  98]\n",
      "   [134 119 108]\n",
      "   [149 135 124]\n",
      "   ...\n",
      "   [107  99  83]\n",
      "   [112  99  85]\n",
      "   [122 104  92]]\n",
      "\n",
      "  [[102  95  89]\n",
      "   [121 113 106]\n",
      "   [133 124 117]\n",
      "   ...\n",
      "   [113 105  88]\n",
      "   [112  99  86]\n",
      "   [124 106  94]]]\n",
      "\n",
      "\n",
      " [[[ 46  50  59]\n",
      "   [ 48  52  63]\n",
      "   [ 51  53  65]\n",
      "   ...\n",
      "   [213 210 204]\n",
      "   [207 202 193]\n",
      "   [193 188 180]]\n",
      "\n",
      "  [[ 47  51  62]\n",
      "   [ 52  55  67]\n",
      "   [ 53  56  68]\n",
      "   ...\n",
      "   [219 214 206]\n",
      "   [213 206 199]\n",
      "   [207 200 193]]\n",
      "\n",
      "  [[ 51  55  66]\n",
      "   [ 53  57  68]\n",
      "   [ 55  58  69]\n",
      "   ...\n",
      "   [218 211 204]\n",
      "   [210 203 195]\n",
      "   [206 199 192]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 42  42  50]\n",
      "   [ 41  42  49]\n",
      "   [ 44  44  51]\n",
      "   ...\n",
      "   [201 194 181]\n",
      "   [206 200 188]\n",
      "   [202 194 182]]\n",
      "\n",
      "  [[ 41  41  48]\n",
      "   [ 42  43  49]\n",
      "   [ 35  35  40]\n",
      "   ...\n",
      "   [200 193 181]\n",
      "   [199 194 183]\n",
      "   [198 192 180]]\n",
      "\n",
      "  [[ 40  41  46]\n",
      "   [ 35  36  39]\n",
      "   [ 19  19  20]\n",
      "   ...\n",
      "   [200 193 180]\n",
      "   [189 184 172]\n",
      "   [188 183 171]]]\n",
      "\n",
      "\n",
      " [[[184 186 165]\n",
      "   [185 187 166]\n",
      "   [191 193 171]\n",
      "   ...\n",
      "   [182 185 166]\n",
      "   [193 196 177]\n",
      "   [200 203 184]]\n",
      "\n",
      "  [[177 179 158]\n",
      "   [173 175 154]\n",
      "   [179 181 160]\n",
      "   ...\n",
      "   [177 180 161]\n",
      "   [183 186 167]\n",
      "   [191 194 175]]\n",
      "\n",
      "  [[176 178 157]\n",
      "   [175 177 156]\n",
      "   [180 182 161]\n",
      "   ...\n",
      "   [180 183 164]\n",
      "   [182 185 166]\n",
      "   [190 193 174]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175 175 150]\n",
      "   [175 175 151]\n",
      "   [178 178 154]\n",
      "   ...\n",
      "   [200 201 183]\n",
      "   [190 191 173]\n",
      "   [181 182 164]]\n",
      "\n",
      "  [[172 172 147]\n",
      "   [173 173 148]\n",
      "   [175 175 150]\n",
      "   ...\n",
      "   [189 190 172]\n",
      "   [192 193 175]\n",
      "   [187 188 170]]\n",
      "\n",
      "  [[184 183 162]\n",
      "   [182 182 160]\n",
      "   [182 181 160]\n",
      "   ...\n",
      "   [179 180 162]\n",
      "   [188 189 170]\n",
      "   [191 192 174]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[249 250 245]\n",
      "   [251 252 247]\n",
      "   [251 252 247]\n",
      "   ...\n",
      "   [229 230 222]\n",
      "   [248 251 242]\n",
      "   [247 252 245]]\n",
      "\n",
      "  [[245 246 241]\n",
      "   [248 249 244]\n",
      "   [248 249 244]\n",
      "   ...\n",
      "   [217 218 206]\n",
      "   [245 248 237]\n",
      "   [244 249 242]]\n",
      "\n",
      "  [[246 247 242]\n",
      "   [248 249 244]\n",
      "   [248 249 244]\n",
      "   ...\n",
      "   [216 218 203]\n",
      "   [246 249 237]\n",
      "   [244 248 241]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[241 246 241]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [246 249 243]\n",
      "   [246 248 241]\n",
      "   [241 245 239]]\n",
      "\n",
      "  [[241 246 240]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [246 249 242]\n",
      "   [246 249 242]\n",
      "   [241 246 239]]\n",
      "\n",
      "  [[242 247 241]\n",
      "   [243 248 242]\n",
      "   [244 249 243]\n",
      "   ...\n",
      "   [245 249 242]\n",
      "   [245 249 242]\n",
      "   [242 247 240]]]\n",
      "\n",
      "\n",
      " [[[239 238 234]\n",
      "   [251 250 246]\n",
      "   [251 250 246]\n",
      "   ...\n",
      "   [249 250 244]\n",
      "   [249 250 244]\n",
      "   [249 250 244]]\n",
      "\n",
      "  [[238 237 233]\n",
      "   [249 248 244]\n",
      "   [249 248 244]\n",
      "   ...\n",
      "   [247 248 242]\n",
      "   [247 248 242]\n",
      "   [248 249 243]]\n",
      "\n",
      "  [[238 237 233]\n",
      "   [250 249 245]\n",
      "   [249 248 244]\n",
      "   ...\n",
      "   [247 248 242]\n",
      "   [248 249 243]\n",
      "   [248 249 243]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[237 233 224]\n",
      "   [250 247 239]\n",
      "   [249 247 241]\n",
      "   ...\n",
      "   [245 245 237]\n",
      "   [244 244 236]\n",
      "   [244 244 236]]\n",
      "\n",
      "  [[238 233 224]\n",
      "   [248 245 237]\n",
      "   [246 244 238]\n",
      "   ...\n",
      "   [245 245 237]\n",
      "   [244 245 236]\n",
      "   [244 244 236]]\n",
      "\n",
      "  [[237 232 225]\n",
      "   [248 243 237]\n",
      "   [247 244 238]\n",
      "   ...\n",
      "   [247 247 239]\n",
      "   [248 247 240]\n",
      "   [247 246 239]]]\n",
      "\n",
      "\n",
      " [[[179 152  81]\n",
      "   [177 151  80]\n",
      "   [179 153  82]\n",
      "   ...\n",
      "   [173 150  86]\n",
      "   [172 149  85]\n",
      "   [169 147  83]]\n",
      "\n",
      "  [[180 153  77]\n",
      "   [176 150  74]\n",
      "   [175 149  74]\n",
      "   ...\n",
      "   [177 155  91]\n",
      "   [175 153  88]\n",
      "   [171 151  86]]\n",
      "\n",
      "  [[177 151  73]\n",
      "   [172 146  68]\n",
      "   [168 143  64]\n",
      "   ...\n",
      "   [181 160  97]\n",
      "   [176 155  92]\n",
      "   [172 151  89]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 147  80]\n",
      "   [175 148  82]\n",
      "   [184 157  90]\n",
      "   ...\n",
      "   [158 133  77]\n",
      "   [155 130  74]\n",
      "   [149 124  70]]\n",
      "\n",
      "  [[174 147  79]\n",
      "   [176 149  82]\n",
      "   [185 158  91]\n",
      "   ...\n",
      "   [159 134  78]\n",
      "   [156 131  75]\n",
      "   [150 125  70]]\n",
      "\n",
      "  [[170 143  76]\n",
      "   [173 146  80]\n",
      "   [181 154  88]\n",
      "   ...\n",
      "   [157 132  77]\n",
      "   [154 129  74]\n",
      "   [147 122  68]]]]\n",
      "============================\n",
      "[[79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]]\n",
      "============================\n",
      "[[79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]\n",
      " [79]]\n",
      "(80, 500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_new2[79])\n",
    "print('============================')\n",
    "print(x_train_final[79])\n",
    "print('============================')\n",
    "print(y_train_new2[79])\n",
    "print('============================')\n",
    "print(y_train_final[79])\n",
    "print(x_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80, 500, 32, 32, 3)\n",
      "\n",
      "training alphabets\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c_train={a:[500*(a), 500*(a+1)-1] for a in range(80)}\n",
    "c_train={a:a for a in range(80)}\n",
    "c_test={a:a for a in range(80,100)}\n",
    "\n",
    "print(\"X_train shape:\", x_train_final.shape)\n",
    "print(\"\")\n",
    "print(\"training alphabets\")\n",
    "print([key for key in c_train.keys()])\n",
    "\n",
    "c_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, X):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    #n_examples, w, h, d = X.shape\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n",
    "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n",
    "    return pairs, targets\n",
    "\n",
    "def batch_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, X)\n",
    "        yield (pairs, targets)\n",
    "\n",
    "def train(model, X_train, batch_size=32, steps_per_epoch=100, epochs=1):\n",
    "    model.fit_generator(batch_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "=================================================================\n",
      "Total params: 1,330,688\n",
      "Trainable params: 1,329,536\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         1330688     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,334,785\n",
      "Trainable params: 1,333,633\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(128, (2,2), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(256, (2,2), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "convnet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = Lambda(L1_distance)([encoded_l, encoded_r])\n",
    "prediction = Dense(1, activation='sigmoid')(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n",
    "\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    #if language is not None:\n",
    "        #low, high = c[language]\n",
    "        #if N > high - low:\n",
    "        #    raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "    #    categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    #else:  # if no language specified just pick a bunch of random letters\n",
    "    categories = np.random.choice(range(n_classes), size=(N,), replace=False)    \n",
    "    #print(categories)\n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc,h,w,_ = X.shape\n",
    "    X = X.reshape(nc,h,w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n",
    "    fig,(ax1,ax2) = plt.subplots(2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105),cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, targets = make_oneshot_task(20, x_train_final, c_train, language=5)\n",
    "#plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....................................................x...............................s...................................x....................................s...s......ss.ss............................................................................ss...................ssssss..............................F..................................x....x........................x.....x.................................................ssss.........F....\n",
      "======================================================================\n",
      "FAIL: test_out_of_order_offsets (h5py.tests.old.test_h5t.TestCompound)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\illia\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\tests\\old\\test_h5t.py\", line 61, in test_out_of_order_offsets\n",
      "    self.assertEqual(tid.dtype, expected_dtype)\n",
      "AssertionError: dtype[13 chars]1','f3','f2'], 'formats':['<f4','<f8','<i4'], [30 chars]:20}) != dtype[13 chars]1','f2','f3'], 'formats':['<f4','<i4','<f8'], [30 chars]:20})\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_out_of_order_offsets (h5py.tests.hl.test_datatype.TestOffsets)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\illia\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\tests\\hl\\test_datatype.py\", line 198, in test_out_of_order_offsets\n",
      "    self.assertArrayEqual(fd['data'], data)\n",
      "  File \"c:\\users\\illia\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\tests\\common.py\", line 124, in assertArrayEqual\n",
      "    \"Dtype mismatch (%s vs %s)%s\" % (dset.dtype, arr.dtype, message)\n",
      "AssertionError: False is not true : Dtype mismatch ({'names':['f1','f3','f2'], 'formats':['<f4','<f8','<i4'], 'offsets':[0,8,16], 'itemsize':20} vs {'names':['f1','f2','f3'], 'formats':['<f4','<i4','<f8'], 'offsets':[0,16,8], 'itemsize':20})\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 446 tests in 2.164s\n",
      "\n",
      "FAILED (failures=2, skipped=19, expected failures=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=446 errors=0 failures=2>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "#import h5py_cache as h5c\n",
    "h5py.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training loop 1 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.6819\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 2 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6797\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 3 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6899\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 4 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6803\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 5 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6858\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 6 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6858\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 7 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6934\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 8 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6735\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 9 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6800\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 10 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6755\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 11 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6786\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 12 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6813\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 13 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6820\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 14 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6893\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 15 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6795: \n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 20.8% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 16 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6823\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 17 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6809\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 18 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6764\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 19 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6869\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 20 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6855\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 21 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6943\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 22 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6879\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 23 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6867\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 24 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6940\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 25 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6834\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 26 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6751\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 27 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6940\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 28 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6689\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 29 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6733\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 30 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6792\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 31 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6775\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 32 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6807\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 33 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6896\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 34 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6721: 0s - loss: 0.672\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 35 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6746\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 36 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6817\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 37 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6812\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 38 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6899\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 39 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6890\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 40 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6742\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 41 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6740\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 42 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6828\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 43 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6878\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 44 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6744\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 45 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6761\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 46 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6754\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 47 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6765\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 48 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6759\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 49 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6808\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 50 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6841\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 51 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6847\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 52 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6727\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 53 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6797\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 54 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6846\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 55 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6714\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 56 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6640\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 57 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6702\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 58 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6727\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 59 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6730\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 60 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6847\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 61 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6856\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 62 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6807\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 63 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6780\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 64 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6726\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 65 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6775\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 66 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6774\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 67 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6800\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 68 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6750\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 69 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6733\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 70 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6715\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 71 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6658\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 72 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6769\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 73 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6838\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 74 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6697\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 75 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6813\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 76 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6729\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 77 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6660\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 78 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6827\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 79 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6695\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 80 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6798\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 81 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6832: 1s - loss - ETA: 0s - l - ETA: 0s - loss: 0.6\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 82 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6753\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 83 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6821\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 84 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6776\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 85 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6744\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 86 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6773\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 87 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6699\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 88 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6778\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 89 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6759\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 90 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6788\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 91 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6846\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 92 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6834\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 93 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6778\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 94 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6796\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 95 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6827\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 96 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6804\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 97 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6811\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 98 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6717\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 99 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6820\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 100 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6734\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 101 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6841\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 102 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6783\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 103 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6716\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 104 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6757\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 105 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6756\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 106 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6858\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 107 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6732\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 108 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6656\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 109 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6698\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 110 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6662\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 111 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6731\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 112 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6759\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 113 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6768\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 114 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6848\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 115 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6829\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 116 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6826\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 117 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6776\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 118 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6781\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 119 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6687\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 120 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6720\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 121 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6554\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 122 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6838\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 123 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6685\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 124 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6681\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 125 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6754\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 126 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6729\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 127 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6640\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 128 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6697\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 129 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6730\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 130 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6678\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 131 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6798\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 132 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6756\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 133 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6692\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 134 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6716\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 135 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6802\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 136 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6758\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 137 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6727\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 138 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6686\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 139 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6695\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 140 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6758\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 141 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6784\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 142 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6665\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 143 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6681\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 144 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6780\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 145 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6736\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 146 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6706\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 147 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6774\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 148 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6770\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 149 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6858\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 150 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6739\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 151 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6788\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 152 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6664\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 153 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6819\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 154 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6766\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 22.4% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 155 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6634\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 156 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6751\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 157 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6668\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 158 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6720\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 159 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6741\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 160 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6766\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 161 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6841\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 162 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6794\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 163 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6733\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 164 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6743\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 165 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6665\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 166 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6697\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 167 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6660\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 168 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6687\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 169 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6688\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 170 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6722\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 171 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6740\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 172 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6750\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 173 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6728\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 174 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6615\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 175 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6650\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 176 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6807\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 177 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6729\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 178 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6716\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 179 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6674\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 180 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6714\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 181 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6751\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 182 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6697\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 183 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6683\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 184 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6681\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 185 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6566\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 186 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6783\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 187 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6713\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 188 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6748\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 189 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6643\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 190 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6650\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 191 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6695\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 192 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6649\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 193 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6658\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 194 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6665\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 195 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6670\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 196 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6638\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 197 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6832\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 198 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6745\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 199 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6740\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 200 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6730\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 201 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6611\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 202 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6636\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 11.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 203 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6719\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 204 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6589\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 205 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6728\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 206 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6680\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 207 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6710\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 208 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6681\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 209 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6633\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 210 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6667\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 211 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6640\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 212 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6604\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 213 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6701\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 214 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6714\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 215 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6633\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 216 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6600\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 217 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6656: 0s - loss: 0.66\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 218 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6719\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 219 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6710\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 17.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 220 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6636\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 221 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6759\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 222 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6896\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 223 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6887: 0s - loss: 0.68\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 224 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6681\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 225 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6637\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 226 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6728\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 227 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6727\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 228 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6607\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 229 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6637\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 230 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6619\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 231 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6666\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 232 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6692\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 233 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6718\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 234 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6759\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 235 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6701\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 19.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 236 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6554\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 18.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 237 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6713\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 238 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6683\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 239 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6577\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 20.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 240 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6586\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 241 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6777\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 242 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6651\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 243 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6605\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 244 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6749\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 245 ===\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6685\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 246 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6655\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 17.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 247 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6694\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 14.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 248 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6689\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 249 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6604\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 250 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6692\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 15.6% accuracy for 20-way one-shot learning\n"
     ]
    }
   ],
   "source": [
    "loops = 250\n",
    "best_acc = 0\n",
    "for i in range(loops):\n",
    "    print(\"=== Training loop {} ===\".format(i+1))\n",
    "    train(siamese_net, x_train_final)\n",
    "    test_acc = test_oneshot(siamese_net, x_test_final, c_test)\n",
    "    if test_acc >= best_acc:\n",
    "        print(\"New best one-shot accuracy, saving model ...\")\n",
    "        siamese_net.save(os.path.join(\"models\", \"siamese_omniglot.h5\"))\n",
    "        best_acc = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc:  22.4\n"
     ]
    }
   ],
   "source": [
    "print('Best acc: ',best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaivGyk_AVTY"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate your model's architecture, as well as its performance. What accuracy would random guessing achieve (on average)?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQVd8vHrAYEX"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COiAqXWDAgCe"
   },
   "source": [
    "***\n",
    "\n",
    "**c)** Compare the performance of your Siamese network for Cifar-100 to the Siamese network from Practical 4 for Omniglot. Name three fundamental differences between the Cifar-100 and Omniglot datasets. How do these differences influence the difference in one-shot accuracy?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIHkoQ0PBWuB"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWpFF_5-Bf4B"
   },
   "source": [
    "***\n",
    "\n",
    "### Task 1.2: One-shot learning with neural codes\n",
    "**a)**\n",
    "* Train a CNN classifier on the first 80 classes of Cifar-100. Make sure it achieves at least 40% classification accuracy on those 80 classes (use the test set to validate this accuracy).\n",
    "* Then use neural codes from one of the later hidden layers of the CNN with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 80)                10320     \n",
      "=================================================================\n",
      "Total params: 1,636,368\n",
      "Trainable params: 1,635,920\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "n_classes=80\n",
    "# build convnet to use in each siamese 'leg'\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train_cnn=x_train_final.reshape(40000,32,32,3)\n",
    "y_train_cnn=y_train_final.reshape(40000,1)\n",
    "x_test_cnn=x_test_final.reshape(10000,32,32,3)\n",
    "y_test_cnn=y_test_final.reshape(10000,1)\n",
    "\n",
    "y_train_cnn = to_categorical(y_train_cnn, 80)\n",
    "#y_test_cnn = to_categorical(y_test_cnn, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  1,   1,   1],\n",
       "        [  1,   1,   1],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 10,  10,  10],\n",
       "        [  4,   4,   4],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 20,  20,  20],\n",
       "        [ 11,  11,  11],\n",
       "        [  2,   2,   2]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[252, 252, 252],\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        ...,\n",
       "        [205, 205, 205],\n",
       "        [201, 201, 201],\n",
       "        [194, 194, 194]],\n",
       "\n",
       "       [[252, 252, 252],\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        ...,\n",
       "        [213, 213, 213],\n",
       "        [214, 214, 214],\n",
       "        [206, 206, 206]],\n",
       "\n",
       "       [[250, 250, 250],\n",
       "        [254, 254, 254],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [193, 193, 193],\n",
       "        [197, 197, 197],\n",
       "        [184, 184, 184]]], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cnn[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  1,   1,   1],\n",
       "        [  1,   1,   1],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 10,  10,  10],\n",
       "        [  4,   4,   4],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 20,  20,  20],\n",
       "        [ 11,  11,  11],\n",
       "        [  2,   2,   2]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[252, 252, 252],\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        ...,\n",
       "        [205, 205, 205],\n",
       "        [201, 201, 201],\n",
       "        [194, 194, 194]],\n",
       "\n",
       "       [[252, 252, 252],\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        ...,\n",
       "        [213, 213, 213],\n",
       "        [214, 214, 214],\n",
       "        [206, 206, 206]],\n",
       "\n",
       "       [[250, 250, 250],\n",
       "        [254, 254, 254],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [193, 193, 193],\n",
       "        [197, 197, 197],\n",
       "        [184, 184, 184]]], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_final[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/20\n",
      "36000/36000 [==============================] - 18s 495us/step - loss: 3.8411 - acc: 0.1263 - val_loss: 5.7525 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "36000/36000 [==============================] - 16s 457us/step - loss: 3.0993 - acc: 0.2307 - val_loss: 6.0325 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "36000/36000 [==============================] - 16s 457us/step - loss: 2.7936 - acc: 0.2905 - val_loss: 7.0301 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "36000/36000 [==============================] - 16s 457us/step - loss: 2.6211 - acc: 0.3226 - val_loss: 7.2646 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "36000/36000 [==============================] - 17s 458us/step - loss: 2.4751 - acc: 0.3557 - val_loss: 7.6582 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "36000/36000 [==============================] - 16s 457us/step - loss: 2.3701 - acc: 0.3765 - val_loss: 8.1416 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "36000/36000 [==============================] - 17s 459us/step - loss: 2.2969 - acc: 0.3908 - val_loss: 8.0779 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "36000/36000 [==============================] - 17s 458us/step - loss: 2.2317 - acc: 0.4064 - val_loss: 8.4212 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "36000/36000 [==============================] - 16s 458us/step - loss: 2.1535 - acc: 0.4202 - val_loss: 7.9793 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "36000/36000 [==============================] - 17s 460us/step - loss: 2.1132 - acc: 0.4284 - val_loss: 7.9915 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "36000/36000 [==============================] - 17s 460us/step - loss: 2.0560 - acc: 0.4423 - val_loss: 8.8793 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "36000/36000 [==============================] - 17s 463us/step - loss: 2.0084 - acc: 0.4539 - val_loss: 9.6160 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "36000/36000 [==============================] - 17s 461us/step - loss: 1.9795 - acc: 0.4634 - val_loss: 8.9373 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "36000/36000 [==============================] - 17s 461us/step - loss: 1.9509 - acc: 0.4700 - val_loss: 9.0483 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "36000/36000 [==============================] - 17s 462us/step - loss: 1.9028 - acc: 0.4798 - val_loss: 9.6274 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "36000/36000 [==============================] - 17s 461us/step - loss: 1.8885 - acc: 0.4792 - val_loss: 9.5254 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "36000/36000 [==============================] - 17s 462us/step - loss: 1.8389 - acc: 0.4914 - val_loss: 9.5925 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "36000/36000 [==============================] - 17s 464us/step - loss: 1.8274 - acc: 0.4964 - val_loss: 10.1098 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "36000/36000 [==============================] - 17s 467us/step - loss: 1.7961 - acc: 0.5038 - val_loss: 9.6142 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "36000/36000 [==============================] - 17s 466us/step - loss: 1.7749 - acc: 0.5057 - val_loss: 10.3411 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251b66a9ba8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "model.fit(x_train_cnn, y_train_cnn,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=np.asarray([x_test[i] for i in range(10000) if y_test[i]<80])\n",
    "y_val=np.asarray([y_test[i] for i in range(10000) if y_test[i]<80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 32, 32, 3)\n",
      "(8000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = to_categorical(y_val, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.070949311256409\n",
      "Test accuracy: 0.374625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(\"models\", \"cnn_80_03745.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 5, 5, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 80)                20560     \n",
      "=================================================================\n",
      "Total params: 298,576\n",
      "Trainable params: 297,424\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 5, 5, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 80)                20560     \n",
      "=================================================================\n",
      "Total params: 298,576\n",
      "Trainable params: 297,424\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "\n",
    "\n",
    "# build model to use in each siamese 'leg'\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (2,2), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(256, (2,2), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(80, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/20\n",
      "36000/36000 [==============================] - 15s 422us/step - loss: 4.1107 - acc: 0.0871 - val_loss: 8.0275 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "36000/36000 [==============================] - 12s 346us/step - loss: 3.6313 - acc: 0.1593 - val_loss: 8.4507 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 3.1829 - acc: 0.2383 - val_loss: 8.3306 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.9109 - acc: 0.2988 - val_loss: 8.2229 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.7229 - acc: 0.3431 - val_loss: 8.5603 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.5955 - acc: 0.3747 - val_loss: 8.4642 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.5128 - acc: 0.3929 - val_loss: 8.7960 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.4510 - acc: 0.4077 - val_loss: 9.0131 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.3876 - acc: 0.4264 - val_loss: 8.5647 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "36000/36000 [==============================] - 13s 352us/step - loss: 2.3451 - acc: 0.4381 - val_loss: 8.7419 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.3019 - acc: 0.4470 - val_loss: 9.0391 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "36000/36000 [==============================] - 12s 347us/step - loss: 2.2687 - acc: 0.4587 - val_loss: 8.9847 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "36000/36000 [==============================] - 12s 344us/step - loss: 2.2367 - acc: 0.4679 - val_loss: 8.9684 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.2074 - acc: 0.4763 - val_loss: 9.0875 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.1920 - acc: 0.4777 - val_loss: 9.1366 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "36000/36000 [==============================] - 12s 346us/step - loss: 2.1556 - acc: 0.4865 - val_loss: 9.1074 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.1393 - acc: 0.4925 - val_loss: 9.1729 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "36000/36000 [==============================] - 13s 347us/step - loss: 2.1254 - acc: 0.4977 - val_loss: 9.2963 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.1065 - acc: 0.5014 - val_loss: 9.4607 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "36000/36000 [==============================] - 13s 348us/step - loss: 2.0947 - acc: 0.5069 - val_loss: 9.2373 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251d67bd1d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "model.fit(x_train_cnn, y_train_cnn,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.0494189372062683\n",
      "Test accuracy: 0.418625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(\"models\", \"cnn_80_second_0418.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 5, 5, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 80)                20560     \n",
      "=================================================================\n",
      "Total params: 298,576\n",
      "Trainable params: 297,424\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# additional imports\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "model = load_model(os.path.join(\"models\", \"cnn_80_second_0418.h5\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1, 1, 256)\n",
      "(10000, 1, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "# load the previously trained and saved models\n",
    "model = load_model(os.path.join(\"models\", \"cnn_80_second_0418.h5\"))\n",
    "    \n",
    "# NOTE: change the name \"neural codes\" if the layer from which you wish to retrieve neural codes has a different name\n",
    "model_nc = Model(inputs=model.input, outputs=model.get_layer(\"conv2d_20\").output)\n",
    "\n",
    "# obtain flat representations of the data\n",
    "#x_train_r_flat = x_train_r.reshape((x_train_r.shape[0], -1))\n",
    "#x_test_r_flat = x_test_r.reshape((x_test_r.shape[0], -1))\n",
    "\n",
    "# train PCA on the retrieval set\n",
    "#pca = PCA(n_components=128)\n",
    "#pca.fit(x_train_r_flat)\n",
    "\n",
    "# obtain 128-dimensional representations\n",
    "nc_train = model_nc.predict(x_train_cnn)\n",
    "nc_test = model_nc.predict(x_test_cnn)\n",
    "\n",
    "\n",
    "# print the shapes to confirm all features are 128-dimensional\n",
    "print(nc_train.shape)\n",
    "print(nc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random selection of n query images/indices, the same for all experiments\n",
    "n = 200\n",
    "n_examples = 5000  # the retrieval test set has 5000 images\n",
    "indices = np.random.choice(range(n_examples), size=n, replace=False)\n",
    "result={}\n",
    "\n",
    "representations = [\n",
    "    (nc_train, nc_test)\n",
    "]\n",
    "\n",
    "it=0\n",
    "idt=['cnn']\n",
    "for (nc_train, nc_test) in representations:\n",
    "    # === SOLUTION: ===\n",
    "    #print(nc_test.shape)\n",
    "# iterate over two data representations (make sure these two files exist in the \"data\" subfolder first)\n",
    "#for datapath in (\"caltech101_VGG16_fc1.p\", \"caltech101_VGG16_fc2.p\"):\n",
    "    # load the dataset\n",
    "    #with open(os.path.join(\"data\", datapath), \"rb\") as f:\n",
    "        #X_train_digits_r, y_train_digits_r, X_test_digits_r, y_test_digits_r = pickle.load(f)\n",
    "        #X_fc, y, X_paths, classes = pickle.load(f)\n",
    "\n",
    "    #nsamples, nx, ny = nc_test.shape\n",
    "    # === SOLUTION: ===\n",
    "    neigh = NearestNeighbors(n_neighbors=6, p=2)\n",
    "    #nc_test_rs=nc_test.reshape((n_examples,nx*ny))\n",
    "    neigh.fit(nc_train)\n",
    "    X = nc_train[indices]\n",
    "    nn = neigh.kneighbors(X)\n",
    "    #print(nn)\n",
    "    result[idt[it] +\"_distance\"] = nn[0]\n",
    "    result[idt[it]+\"_comparables\"] = nn[1]\n",
    "    it+=1    \n",
    "        \n",
    "score = {}\n",
    "for key,val in result.items():\n",
    "    if \"comparables\" in key:\n",
    "        print(\"_______{}______\".format(key))\n",
    "        total = 0\n",
    "        for neigh in val:\n",
    "            not_equal = [x for x in neigh if y_train_digits_r[x] != y_train_digits_r[neigh[0]]]\n",
    "            amnt = len(not_equal)\n",
    "            total += amnt\n",
    "        #print(\"amount of samples {}\".format(len(val)))\n",
    "        print(\"total unequal: {}, average unequal: {}\".format(total,total/len(val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1BDzdPAz26B"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate your CNN architecture, and discuss the difference in one-shot accuracy between the Siamese network approach and the CNN neural codes approach.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRpVm956FR8P"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Assignment 3 question 1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
