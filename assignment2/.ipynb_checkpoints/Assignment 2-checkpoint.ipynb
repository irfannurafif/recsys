{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "***\n",
    "\n",
    "## Question 1: Neural Codes & Nearest Neighbor retrieval (7.5pt)\n",
    "The Caltech101 dataset consists of images of 101 different objects. In this question you will develop an image retrieval system using image representations (neural codes) learned with a deep convolutional neural network and a given distance metric.\n",
    "\n",
    "In the tasks below you will need to implement the following steps:\n",
    "\n",
    "* Retrieval for $n$ selected (distinct) query images from the dataset\n",
    "    * For each query image, obtain the 5 most similar images (excluding the query image itself!)\n",
    "* Evaluation of the quality of the retrieval \n",
    "    * The Caltech101 images are annotated with their object class. Use these annotations to evaluate the accuracy of the retrieval task.\n",
    "    * For each query image, count the number of images whose class corresponds to the one from the query. The score of the retrieval for that image then ranges between:\n",
    "        * **5** *all* retrieved images' classes agree with the query image class\n",
    "        * **0** *none* of the images' classes agree with the query image class\n",
    "    * Compute the average of all $n$ queries\n",
    "\n",
    "***\n",
    "\n",
    "### Task 1.1:  Neural codes image retrieval\n",
    "**a)** Implement the retrieval task and evaluate the results for $n=200$ images. Use the provided VGG16 network pre-trained on ImageNet to compute \"neural codes\" and L2-distance. Specifically use the codes produces by the following layers of the model: \n",
    "1. the \"fc1\"-layer\n",
    "2. the \"fc2\"-layer\n",
    "\n",
    "Provide the retrieval evaluation scores for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll need these imports:\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\caltech101_VGG16_fc1.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-766067024b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"caltech101_VGG16_fc1.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"caltech101_VGG16_fc2.p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mX_fc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\caltech101_VGG16_fc1.p'"
     ]
    }
   ],
   "source": [
    "# NOTE: you will first need to apply some changes to \"Practical-3.3.0_preprocess-caltech101.ipynb\" and run it\n",
    "#       to obtain a pickle file with \"fc1\"-features. You don't need to show these changes here.\n",
    "\n",
    "\n",
    "# make random selection of n query images/indices, the same for all experiments\n",
    "n = 200\n",
    "n_examples = 8677  # the dataset has 8677 images\n",
    "indices = np.random.choice(range(n_examples), size=n, replace=False)\n",
    "\n",
    "# iterate over two data representations (make sure these two files exist in the \"data\" subfolder first)\n",
    "for datapath in (\"caltech101_VGG16_fc1.p\", \"caltech101_VGG16_fc2.p\"):\n",
    "    # load the dataset\n",
    "    with open(os.path.join(\"data\", datapath), \"rb\") as f:\n",
    "        X_fc, y, X_paths, classes = pickle.load(f)\n",
    "\n",
    "\n",
    "    # === SOLUTION: ===\n",
    "    # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Which representation (\"neural code\") provided better features for the given retrieval task? \n",
    "Justify your answer and discuss possible reasons for the observed results. Relate your answer to the conclusions in the paper \"Neural Codes for Image Retrieval\".\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 1.2: Detailed evaluation\n",
    "**a)** The retrieval scores can vary from one query image to another. Some images are quite representative and for them retrieval works well, some are not so much.\n",
    "For the same retrieval task given above using \"fc2\"-features, find (if possible) six query images such that they range from excellent to poor retrieval performance. More specifically find example query images that result in query scores of exactly 0, 1, 2, 3, 4, and 5.\n",
    "\n",
    "Visualise the six (or less) resulting query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll need these extra imports:\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\caltech101_VGG16_fc2.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-865ae8d746ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"caltech101_VGG16_fc2.p\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mX_fc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\caltech101_VGG16_fc2.p'"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "datapath = \"caltech101_VGG16_fc2.p\"\n",
    "with open(os.path.join(\"data\", datapath), \"rb\") as f:\n",
    "    X_fc, y, X_paths, classes = pickle.load(f)\n",
    "\n",
    "# you can use this simple function to visualise an image, given a filepath\n",
    "def show_img(filepath):\n",
    "    img = image.load_img(filepath, target_size=(224,224))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "# example usage given some index < 8677:\n",
    "#index = 254\n",
    "#show_img(X_paths[index])\n",
    "\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Looking at the results, what can you say about the \"types\" of images that obtain good retrieval scores compared to those obtaining poor retrieval scores? Give an explanation and possible solution(s).\n",
    "\n",
    "(*HINT: How did we obtain data representations for similarity measures?*)\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 1.3: Subjective evaluation\n",
    "We will now use the \"fc2\"-features to do image retrieval for query images from the \"BACKGROUND_Google\" set from the Caltech101 dataset. These images are not associated to a particular class, so we will evaluate them subjectively instead.\n",
    "\n",
    "**a)** Find two query images from the \"BACKGROUND_Google\" class, such that for the first query image relevant/similar images are retrieved (according to your own definition of relevancy/similarity), and for the second image mainly irrelevant/dissimilar images are retrieved. For each of them, visualise its 5 nearest neighbors in the Caltech101 dataset (*so do NOT retrieve images from the \"BACKGROUND_Google\" class!*), according to the \"fc2-features\" and L2-distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the BACKGROUND_Google set\n",
    "with open(os.path.join(\"data\",\"caltech101_VGG16_fc2_bg.p\"), \"rb\") as f:\n",
    "    bg_fc2, bg_paths = pickle.load(f)\n",
    "\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Motivate your idea of \"relevance\": why do you consider the results for the first image relevant/similar, and those for the second image irrelevant/dissimilar?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**c)** Explain why you think this retrieval method (nearest neighbor for neural codes from VGG16) performs better on the first image than on the second.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 1.4: Dimensionality reduction\n",
    "\n",
    "**a)** So far we've been using 4096-dimensional neural codes. This space is however still quite high-dimensional. Apply a dimensionality reduction method and evaluate the effect on the retrieval performance.\n",
    "\n",
    "* Use PCA to obtain lower-dimensional representations of the Caltech101 data \"fc2\"-features (try the same compression rates as in Table 2 of the \"Neural Codes for Image Retrieval\" paper).\n",
    "* Evaluate the same retrieval task as explained at the start of this question for each of the compression rates/dimensionalities. Report the retrieval scores.\n",
    "\n",
    "*HINT: See http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html on how to transform a dataset with PCA.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "datapath = \"caltech101_VGG16_fc2.p\"\n",
    "with open(os.path.join(\"data\", datapath), \"rb\") as f:\n",
    "    X_fc, y, X_paths, classes = pickle.load(f)\n",
    "    \n",
    "# make random selection of n query images/indices, the same for all experiments\n",
    "n = 200\n",
    "n_examples = 8677  # the dataset has 8677 images\n",
    "indices = np.random.choice(range(n_examples), size=n, replace=False)\n",
    "\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Discuss your results: how much can you further reduce the dimensionality of the data representations, without affecting the retrieval performance (much)? Compare these results to those from the paper, are your conclusions similar or not?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Question 2: Fashion-MNIST (12.5pt)\n",
    "For this question we will work with the \"Fashion-MNIST\" dataset. This dataset is modelled to have the same specifics as MNIST; it consists of a training set of 60,000 examples, and a test set of 10,000 examples. Each example is a 28x28 greyscale image, associated with a label from one of 10 classes. The images represent various clothing items (as opposed to handwritten digits for MNIST), each class represents a different type of clothing item. The following classes exist:\n",
    "* 0:\tT-shirt/top\n",
    "* 1:\tTrouser\n",
    "* 2:\tPullover\n",
    "* 3:\tDress\n",
    "* 4:\tCoat\n",
    "* 5:\tSandal\n",
    "* 6:\tShirt\n",
    "* 7:\tSneaker\n",
    "* 8:\tBag\n",
    "* 9:\tAnkle boot\n",
    "\n",
    "In this question we will investigate various ways to model visual similarity for this dataset, in order to perform image retrieval. For more info about the dataset, see https://github.com/zalandoresearch/fashion-mnist.\n",
    "\n",
    "The dataset can directly be obtained through Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "y_train shape: (60000, 10)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1668ceefe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Ankle boot (9)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "# load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# properties of the data\n",
    "img_rows, img_cols, chns = 28, 28, 1\n",
    "n_classes = 10\n",
    "\n",
    "# reshape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], chns, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], chns, img_rows, img_cols)\n",
    "    input_shape = (chns, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, chns)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, chns)\n",
    "    input_shape = (img_rows, img_cols, chns)\n",
    "\n",
    "# normalise\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# transform labels to one-hot encoding, but also keep original single-digit encoding\n",
    "y_train_digits = y_train\n",
    "y_test_digits = y_test\n",
    "y_train = to_categorical(y_train_digits, n_classes)\n",
    "y_test = to_categorical(y_test_digits, n_classes)\n",
    "\n",
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# show an example\n",
    "example_id = 0  # pick any integer from 0 to 59999 to visualize a training example\n",
    "example = x_train[example_id].reshape(img_rows, img_cols)\n",
    "label = y_train[example_id]\n",
    "label_digit = y_train_digits[example_id]\n",
    "label_class = classes[label_digit]\n",
    "plt.matshow(example, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(\"Class: {} ({})\".format(label_class, label_digit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation: We have a fully labelled dataset (the ***labelled set***) of the images from the first 5 classes (t-shirts/tops, trousers, pullovers, dresses, coats). We are then supplied with an unlabelled dataset (the ***retrieval set***) containing the remaining Fashion-MNIST images (sandals, shirts, sneakers, bags, ankle boots) on which we want to be able to perform image retrieval. So we cannot use labels from the retrieval set, since we do not know them (note that in our case we *do* have the labels, but we will only use them for evaluation).\n",
    "\n",
    "The following code splits the dataset up into two sets representing 5 classes each. Observe that the labelled and the retrieval set have exactly the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 28, 28, 1)\n",
      "(30000, 28, 28, 1)\n",
      "(30000, 5)\n",
      "(30000, 5)\n",
      "(30000,)\n",
      "(30000,)\n",
      "(5000, 28, 28, 1)\n",
      "(5000, 28, 28, 1)\n",
      "(5000, 5)\n",
      "(5000, 5)\n",
      "(5000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# obtain indices of labelled and retrieval sets\n",
    "indices_train_l = np.argwhere(y_train_digits < 5).flatten()  # indices labelled training set\n",
    "indices_train_r = np.argwhere(y_train_digits >= 5).flatten()  # indices retrieval training set\n",
    "indices_test_l = np.argwhere(y_test_digits < 5).flatten()  # indices labelled test set\n",
    "indices_test_r = np.argwhere(y_test_digits >= 5).flatten()  # indices retrieval test set\n",
    "\n",
    "# split up train and test set (images and labels)\n",
    "x_train_l = x_train[indices_train_l]\n",
    "x_train_r = x_train[indices_train_r]\n",
    "y_train_l = y_train[indices_train_l]\n",
    "y_train_r = y_train[indices_train_r]\n",
    "y_train_digits_l = y_train_digits[indices_train_l]\n",
    "y_train_digits_r = y_train_digits[indices_train_r]\n",
    "x_test_l = x_test[indices_test_l]\n",
    "x_test_r = x_test[indices_test_r]\n",
    "y_test_l = y_test[indices_test_l]\n",
    "y_test_r = y_test[indices_test_r]\n",
    "y_test_digits_l = y_test_digits[indices_test_l]\n",
    "y_test_digits_r = y_test_digits[indices_test_r]\n",
    "\n",
    "# labels are now one-hot encoded 10-dimensional vectors, but only the first or last five dimensions are used\n",
    "# omit unused dimensions to obtain 5-dimensional one-hot encodings\n",
    "y_train_l = y_train_l[:, :5]\n",
    "y_train_r = y_train_r[:, 5:]\n",
    "y_test_l = y_test_l[:, :5]\n",
    "y_test_r = y_test_r[:, 5:]\n",
    "# (note that the dimensions of y_train_l/y_test_l do not correspond to those of y_train_r/y_test_r now)\n",
    "\n",
    "# print the shapes\n",
    "print(x_train_l.shape)\n",
    "print(x_train_r.shape)\n",
    "print(y_train_l.shape)\n",
    "print(y_train_r.shape)\n",
    "print(y_train_digits_l.shape)\n",
    "print(y_train_digits_r.shape)\n",
    "print(x_test_l.shape)\n",
    "print(x_test_r.shape)\n",
    "print(y_test_l.shape)\n",
    "print(y_test_r.shape)\n",
    "print(y_test_digits_l.shape)\n",
    "print(y_test_digits_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 2.1: Fashion neural retrieval\n",
    "**a)** Design an MLP (multilayer perceptron) for classification on the first 5 classes of the Fashion-MNIST dataset (i.e. only use `x_train_l` for training). You may include Dropout and BatchNormalization if needed. Let the last hidden dense layer (before the 5-dimensional output layer) have 128 dimensions. (*HINT: you can use* `name=\"neural_codes\"` *for this layer to make it easier to obtain features from it later.*)\n",
    "\n",
    "Train it to classify images into their corresponding classes. Make sure that it achieves decent accuracy (at least 90%) on the labelled test set `x_test_l` (show this!). Save the trained model to a \".h5\" file. (make sure you're using Keras version 2.1.3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sequential model and layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(256, input_shape=input_shape,name=\"neural_codes_mlp_1\"))\n",
    "#mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(128, name=\"neural_codes_mlp_2\"))\n",
    "#mlp.add(BatchNormalization())\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(5, activation='softmax',name=\"neural_codes_mlp_3\"))\n",
    "mlp.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "27000/27000 [==============================] - 109s 4ms/step - loss: 0.3749 - acc: 0.8691 - val_loss: 0.3420 - val_acc: 0.8720\n",
      "Epoch 2/20\n",
      "27000/27000 [==============================] - 110s 4ms/step - loss: 0.3401 - acc: 0.8815 - val_loss: 0.3648 - val_acc: 0.8650\n",
      "Epoch 3/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3367 - acc: 0.8839 - val_loss: 0.3347 - val_acc: 0.8767\n",
      "Epoch 4/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3231 - acc: 0.8861 - val_loss: 0.3230 - val_acc: 0.8893\n",
      "Epoch 5/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3187 - acc: 0.8881 - val_loss: 0.3222 - val_acc: 0.8870\n",
      "Epoch 6/20\n",
      "27000/27000 [==============================] - 109s 4ms/step - loss: 0.3139 - acc: 0.8899 - val_loss: 0.3456 - val_acc: 0.8807\n",
      "Epoch 7/20\n",
      "27000/27000 [==============================] - 115s 4ms/step - loss: 0.3139 - acc: 0.8893 - val_loss: 0.3120 - val_acc: 0.8917\n",
      "Epoch 8/20\n",
      "27000/27000 [==============================] - 113s 4ms/step - loss: 0.3118 - acc: 0.8907 - val_loss: 0.3264 - val_acc: 0.8907\n",
      "Epoch 9/20\n",
      "27000/27000 [==============================] - 111s 4ms/step - loss: 0.3121 - acc: 0.8910 - val_loss: 0.3188 - val_acc: 0.8897\n",
      "Epoch 10/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3039 - acc: 0.8932 - val_loss: 0.3645 - val_acc: 0.8603\n",
      "Epoch 11/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3038 - acc: 0.8929 - val_loss: 0.3218 - val_acc: 0.8923\n",
      "Epoch 12/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3009 - acc: 0.8949 - val_loss: 0.3296 - val_acc: 0.8793\n",
      "Epoch 13/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.3013 - acc: 0.8929 - val_loss: 0.3272 - val_acc: 0.8863\n",
      "Epoch 14/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2970 - acc: 0.8960 - val_loss: 0.3094 - val_acc: 0.8930\n",
      "Epoch 15/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2964 - acc: 0.8974 - val_loss: 0.3179 - val_acc: 0.8867\n",
      "Epoch 16/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2995 - acc: 0.8940 - val_loss: 0.3104 - val_acc: 0.8873\n",
      "Epoch 17/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2967 - acc: 0.8957 - val_loss: 0.3130 - val_acc: 0.8873\n",
      "Epoch 18/20\n",
      "27000/27000 [==============================] - 109s 4ms/step - loss: 0.2950 - acc: 0.8967 - val_loss: 0.3295 - val_acc: 0.8820\n",
      "Epoch 19/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2949 - acc: 0.8968 - val_loss: 0.3214 - val_acc: 0.8833\n",
      "Epoch 20/20\n",
      "27000/27000 [==============================] - 108s 4ms/step - loss: 0.2969 - acc: 0.8951 - val_loss: 0.3236 - val_acc: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16690ec2c18>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "mlp.fit(x_train_l, y_train_l,batch_size=batch_size,epochs=epochs,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.35241450238227845\n",
      "Test accuracy: 0.8754\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = mlp.evaluate(x_test_l, y_test_l, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if doesn't exist yet\n",
    "try:\n",
    "    os.mkdir(\"assignment2_models\")\n",
    "except(FileExistsError):\n",
    "    pass\n",
    "\n",
    "# save the model\n",
    "mlp.save(os.path.join(\"assignment2_models\", \"mlp_fashionmnist_l_lama.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(512, input_shape=input_shape,name=\"neural_codes_mlp_1\"))\n",
    "mlp.add(Dense(256, name=\"neural_codes_mlp_2\"))\n",
    "mlp.add(Dense(128, name=\"neural_codes_mlp_3\"))\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(5, activation='softmax',name=\"neural_codes_mlp_4\"))\n",
    "mlp.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "27000/27000 [==============================] - 391s 14ms/step - loss: 0.4238 - acc: 0.8490 - val_loss: 0.3500 - val_acc: 0.8790\n",
      "Epoch 2/20\n",
      "27000/27000 [==============================] - 383s 14ms/step - loss: 0.3444 - acc: 0.8796 - val_loss: 0.3639 - val_acc: 0.8713\n",
      "Epoch 3/20\n",
      "27000/27000 [==============================] - 368s 14ms/step - loss: 0.3405 - acc: 0.8793 - val_loss: 0.3387 - val_acc: 0.8793\n",
      "Epoch 4/20\n",
      "27000/27000 [==============================] - 368s 14ms/step - loss: 0.3271 - acc: 0.8859 - val_loss: 0.3458 - val_acc: 0.8780\n",
      "Epoch 5/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.3168 - acc: 0.8906 - val_loss: 0.3126 - val_acc: 0.8867\n",
      "Epoch 6/20\n",
      "27000/27000 [==============================] - 366s 14ms/step - loss: 0.3197 - acc: 0.8871 - val_loss: 0.3348 - val_acc: 0.8753\n",
      "Epoch 7/20\n",
      "27000/27000 [==============================] - 366s 14ms/step - loss: 0.3167 - acc: 0.8875 - val_loss: 0.3205 - val_acc: 0.8887\n",
      "Epoch 8/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.3087 - acc: 0.8920 - val_loss: 0.3220 - val_acc: 0.8920\n",
      "Epoch 9/20\n",
      "27000/27000 [==============================] - 366s 14ms/step - loss: 0.3078 - acc: 0.8925 - val_loss: 0.3496 - val_acc: 0.8833\n",
      "Epoch 10/20\n",
      "27000/27000 [==============================] - 364s 13ms/step - loss: 0.3127 - acc: 0.8906 - val_loss: 0.3239 - val_acc: 0.8870\n",
      "Epoch 11/20\n",
      "27000/27000 [==============================] - 362s 13ms/step - loss: 0.3100 - acc: 0.8916 - val_loss: 0.3113 - val_acc: 0.8907\n",
      "Epoch 12/20\n",
      "27000/27000 [==============================] - 364s 13ms/step - loss: 0.3022 - acc: 0.8950 - val_loss: 0.3118 - val_acc: 0.8920\n",
      "Epoch 13/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.3040 - acc: 0.8925 - val_loss: 0.3112 - val_acc: 0.8887\n",
      "Epoch 14/20\n",
      "27000/27000 [==============================] - 366s 14ms/step - loss: 0.3050 - acc: 0.8921 - val_loss: 0.3459 - val_acc: 0.8800\n",
      "Epoch 15/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.3011 - acc: 0.8943 - val_loss: 0.3310 - val_acc: 0.8817\n",
      "Epoch 16/20\n",
      "27000/27000 [==============================] - 364s 13ms/step - loss: 0.2951 - acc: 0.8958 - val_loss: 0.3269 - val_acc: 0.8843\n",
      "Epoch 17/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.3006 - acc: 0.8932 - val_loss: 0.3147 - val_acc: 0.8887\n",
      "Epoch 18/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.2945 - acc: 0.8960 - val_loss: 0.3306 - val_acc: 0.8847\n",
      "Epoch 19/20\n",
      "27000/27000 [==============================] - 367s 14ms/step - loss: 0.2958 - acc: 0.8969 - val_loss: 0.3357 - val_acc: 0.8837\n",
      "Epoch 20/20\n",
      "27000/27000 [==============================] - 365s 14ms/step - loss: 0.2956 - acc: 0.8976 - val_loss: 0.3173 - val_acc: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16690e62208>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "mlp.fit(x_train_l, y_train_l,batch_size=batch_size,epochs=epochs,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.35060056457519534\n",
      "Test accuracy: 0.8714\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = mlp.evaluate(x_test_l, y_test_l, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate how and why you chose this architecture.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 2.2: Fashion neural retrieval #2\n",
    "**a)** Design a CNN (convolutional neural network) for classification on the first 5 classes of the Fashion-MNIST dataset (i.e. only use x_train_l for training), consisting of a number of Convolutions with Max-Pooling, followed by one or more Dense layers. You may use Dropout and BatchNormalization to improve generalization and training speed. Let the last hidden dense layer (before the 5-dimensional output layer) have 128 dimensions. (*HINT: you can use* `name=\"neural_codes\"` *for this layer to make it easier to obtain features from it later.*)\n",
    "\n",
    "Train the CNN to classify images into their corresponding classes. Make sure that it achieves decent accuracy (at least 94%) on the test set `x_test_l` (show this!). Save the trained model to a \".h5\" file. (make sure you're using Keras version 2.1.3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional layers\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "neural_codes_cnn_1 (Conv2D)  (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "neural_codes_cnn_2 (Conv2D)  (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "neural_codes_cnn_3 (Dense)   (None, 12, 12, 128)       8320      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "neural_codes_cnn_4 (Dense)   (None, 5)                 92165     \n",
      "=================================================================\n",
      "Total params: 119,685\n",
      "Trainable params: 119,493\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape,name=\"neural_codes_cnn_1\"))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu',name=\"neural_codes_cnn_2\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(128, name=\"neural_codes_cnn_3\"))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(5, activation='softmax',name=\"neural_codes_cnn_4\"))\n",
    "cnn.summary()\n",
    "cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "27000/27000 [==============================] - 84s 3ms/step - loss: 0.5911 - acc: 0.8376 - val_loss: 1.4638 - val_acc: 0.5450\n",
      "Epoch 2/20\n",
      "27000/27000 [==============================] - 82s 3ms/step - loss: 0.4449 - acc: 0.8757 - val_loss: 0.3755 - val_acc: 0.8543\n",
      "Epoch 3/20\n",
      "27000/27000 [==============================] - 82s 3ms/step - loss: 0.3808 - acc: 0.8870 - val_loss: 0.2468 - val_acc: 0.9173\n",
      "Epoch 4/20\n",
      "27000/27000 [==============================] - 84s 3ms/step - loss: 0.3262 - acc: 0.8979 - val_loss: 0.2239 - val_acc: 0.9230\n",
      "Epoch 5/20\n",
      "27000/27000 [==============================] - 89s 3ms/step - loss: 0.2903 - acc: 0.9057 - val_loss: 0.2673 - val_acc: 0.9050\n",
      "Epoch 6/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.2645 - acc: 0.9106 - val_loss: 0.2520 - val_acc: 0.9140\n",
      "Epoch 7/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.2472 - acc: 0.9170 - val_loss: 0.2055 - val_acc: 0.9243\n",
      "Epoch 8/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.2359 - acc: 0.9189 - val_loss: 0.1894 - val_acc: 0.9320\n",
      "Epoch 9/20\n",
      "27000/27000 [==============================] - 82s 3ms/step - loss: 0.2174 - acc: 0.9229 - val_loss: 0.1855 - val_acc: 0.9370\n",
      "Epoch 10/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.2079 - acc: 0.9268 - val_loss: 0.1927 - val_acc: 0.9337\n",
      "Epoch 11/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.2030 - acc: 0.9279 - val_loss: 0.1905 - val_acc: 0.9330\n",
      "Epoch 12/20\n",
      "27000/27000 [==============================] - 82s 3ms/step - loss: 0.1963 - acc: 0.9300 - val_loss: 0.1841 - val_acc: 0.9343\n",
      "Epoch 13/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.1946 - acc: 0.9312 - val_loss: 0.2357 - val_acc: 0.9120\n",
      "Epoch 14/20\n",
      "27000/27000 [==============================] - 82s 3ms/step - loss: 0.1904 - acc: 0.9323 - val_loss: 0.1898 - val_acc: 0.9337\n",
      "Epoch 15/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.1808 - acc: 0.9343 - val_loss: 0.2122 - val_acc: 0.9247\n",
      "Epoch 16/20\n",
      "27000/27000 [==============================] - 80s 3ms/step - loss: 0.1756 - acc: 0.9387 - val_loss: 0.2098 - val_acc: 0.9197\n",
      "Epoch 17/20\n",
      "27000/27000 [==============================] - 80s 3ms/step - loss: 0.1739 - acc: 0.9381 - val_loss: 0.2036 - val_acc: 0.9233\n",
      "Epoch 18/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.1721 - acc: 0.9371 - val_loss: 0.1819 - val_acc: 0.9393\n",
      "Epoch 19/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.1665 - acc: 0.9406 - val_loss: 0.1773 - val_acc: 0.9377\n",
      "Epoch 20/20\n",
      "27000/27000 [==============================] - 81s 3ms/step - loss: 0.1714 - acc: 0.9386 - val_loss: 0.1764 - val_acc: 0.9353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1669646e198>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "cnn.fit(x_train_l, y_train_l,batch_size=batch_size,epochs=epochs,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.18353760069608688\n",
      "Test accuracy: 0.9382\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = cnn.evaluate(x_test_l, y_test_l, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "cnn.save(os.path.join(\"assignment2_models\", \"cnn_fashionmnist_l_9382.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate how and why you chose this architecture.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 2.3: Fashion neural retrieval #3\n",
    "**a)** Design a (convolutional) Denoising Autoencoder (DAE) for the *full* Fashion-MNIST dataset (i.e. use `x_train`, *not* `x_train_l`). For the encoder, use only Convolutional layers and Max-Pooling, followed by a Dense layer with 128 units. The output of this layer will be the \"code\" of the autoencoder (*HINT: you can use* `name=\"neural_codes\"` *for this layer to make it easier to obtain features from it later*). For the decoder, start with a Dense layer to upscale to a suitable dimension, and then use only Convolutional layers and UpSampling. You may use BatchNormalization to speed up training.\n",
    "\n",
    "Train the DAE to reconstruct noisy images to the original input images. Make sure that it achieves a binary cross-entropy loss of at most 0.29 on the test set (show this!). Save the trained model to a \".h5\" file. (make sure you're using Keras version 2.1.3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import additional layer\n",
    "from keras.layers import UpSampling2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain noisy version of data\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "\n",
    "# define autoencoder\n",
    "dae = Sequential()\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "dae.save(os.path.join(\"assignment2_models\", \"dae_fashionmnist.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate how and why you chose this architecture.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Visualise a few test examples, their noisy versions, and their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you don't need to change this code, just run it after having trained the DAE\n",
    "def plot_examples(x):\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i+1)\n",
    "        plt.imshow(x[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "x_test_reconstr = dae.predict(x_test_noisy, batch_size=batch_size)\n",
    "\n",
    "plot_examples(x_test)\n",
    "plot_examples(x_test_noisy)\n",
    "plot_examples(x_test_reconstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Do you consider the results acceptable? Do you think they can be useful for image retrieval? Explain why in one or two sentences.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**d)** Why can we train on the full dataset `x_train` here, whereas in Tasks 2.1 and 2.2 we had to use `x_train_l` (the first 5 classes only) for training?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 2.4: Fashion neural retrieval #4\n",
    "Autoencoders come in different shapes and sizes. One key defining property of autoencoders is the means the model uses to prevent the learning of the identity function. Typically, this is done with different regularization methods. In the previous task you used a model that uses noise as a regularizer. In this task you will develop a Sparse Autoencoder (SAE). A sparse autoencoder uses a sparsity regularization to obtain sparse representations of the input data. Sparsity can be achieved by using L1-regularization on the activations of the hidden \"code\" layer.\n",
    "\n",
    "**a)** Design a (convolutional) Sparse Autoencoder (SAE) for the *full* Fashion-MNIST dataset (i.e. use `x_train`, *not* `x_train_l`). For the encoder, use only Convolutional layers and Max-Pooling, followed by a Dense layer with 128 units. The output of this layer will be the \"code\" of the autoencoder (*HINT: you can use* `name=\"neural_codes\"` *for this layer to make it easier to obtain features from it later*). Add an activity regularizer to this layer, using `regularizers.l1(10e-5)` from Keras.\n",
    "For the decoder, start with a Dense layer to upscale to a suitable dimension, and then use only Convolutional layers and UpSampling. You may use BatchNormalization to speed up training.\n",
    "\n",
    "Train the SAE to reconstruct input images. Make sure that it achieves a loss value of at most 0.31 on the test set (show this!). Save the trained model to a \".h5\" file. (make sure you're using Keras version 2.1.3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import regularizers for sparse autoencoder\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define autoencoder\n",
    "sae = Sequential()\n",
    "\n",
    "# === SOLUTION: ===\n",
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "sae.save(os.path.join(\"assignment2_models\", \"sae_fashionmnist.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate how and why you chose this architecture.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Visualise a few test examples and their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you don't need to change this code, just run it after having trained the SAE\n",
    "def plot_examples(x):\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i+1)\n",
    "        plt.imshow(x[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "x_test_reconstr = sae.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "plot_examples(x_test)\n",
    "plot_examples(x_test_reconstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Compare the visual results to those of the DAE in Task 2.3. Also compare the loss values of the test set for the DAE and SAE. How can you explain the difference?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task 2.5: Comparison\n",
    "Obtain 128-dimensional neural code representations of the last five classes of the Fashion-MNIST dataset (the *retrieval set*: `x_train_r`) from the following models/layers:\n",
    "1. The last dense hidden layer (before the output layer) of the MLP you trained in Task 2.1\n",
    "2. The last dense hidden layer (before the output layer) of the CNN you trained in Task 2.2\n",
    "3. The center layer/code of the DAE you trained in Task 2.3\n",
    "4. The center layer/code of the SAE you trained in Task 2.4\n",
    "5. A PCA-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# additional imports\n",
    "from keras.models import load_model\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the previously trained and saved models\n",
    "mlp = load_model(os.path.join(\"assignment2_models\", \"mlp_fashionmnist_l.h5\"))\n",
    "cnn = load_model(os.path.join(\"assignment2_models\", \"cnn_fashionmnist_l.h5\"))\n",
    "dae = load_model(os.path.join(\"assignment2_models\", \"dae_fashionmnist.h5\"))\n",
    "sae = load_model(os.path.join(\"assignment2_models\", \"sae_fashionmnist.h5\"))\n",
    "    \n",
    "# NOTE: change the name \"neural codes\" if the layer from which you wish to retrieve neural codes has a different name\n",
    "mlp_nc = Model(inputs=mlp.input, outputs=mlp.get_layer(\"neural_codes\").output)\n",
    "cnn_nc = Model(inputs=cnn.input, outputs=cnn.get_layer(\"neural_codes\").output)\n",
    "dae_nc = Model(inputs=dae.input, outputs=dae.get_layer(\"neural_codes\").output)\n",
    "sae_nc = Model(inputs=sae.input, outputs=sae.get_layer(\"neural_codes\").output)\n",
    "\n",
    "# obtain flat representations of the data\n",
    "x_train_r_flat = x_train_r.reshape((x_train_r.shape[0], -1))\n",
    "x_test_r_flat = x_test_r.reshape((x_test_r.shape[0], -1))\n",
    "\n",
    "# train PCA on the retrieval set\n",
    "pca = PCA(n_components=128)\n",
    "pca.fit(x_train_r_flat)\n",
    "\n",
    "# obtain 128-dimensional representations\n",
    "nc_mlp_train = mlp_nc.predict(x_train_r)\n",
    "nc_mlp_test = mlp_nc.predict(x_test_r)\n",
    "nc_cnn_train = cnn_nc.predict(x_train_r)\n",
    "nc_cnn_test = cnn_nc.predict(x_test_r)\n",
    "nc_dae_train = dae_nc.predict(x_train_r)\n",
    "nc_dae_test = dae_nc.predict(x_test_r)\n",
    "nc_sae_train = sae_nc.predict(x_train_r)\n",
    "nc_sae_test = sae_nc.predict(x_test_r)\n",
    "nc_pca_train = pca.transform(x_train_r_flat)\n",
    "nc_pca_test = pca.transform(x_test_r_flat)\n",
    "\n",
    "# print the shapes to confirm all features are 128-dimensional\n",
    "print(nc_mlp_train.shape)\n",
    "print(nc_mlp_test.shape)\n",
    "print(nc_cnn_train.shape)\n",
    "print(nc_cnn_test.shape)\n",
    "print(nc_dae_train.shape)\n",
    "print(nc_dae_test.shape)\n",
    "print(nc_sae_train.shape)\n",
    "print(nc_sae_test.shape)\n",
    "print(nc_pca_train.shape)\n",
    "print(nc_pca_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**a)** Evaluate the retrieval task as described in Question 1 on the last 5 classes (the retrieval set) of the Fashion-MNIST dataset, for the five data representations given above. Use query images from the test set and retrieve images from the training set only. Print the five resulting retrieval scores (between 0 and 5).\n",
    "\n",
    "*HINT: you can use* `y_train_digits_r` *and* `y_test_digits_r` *to obtain digit encodings (as opposed to one-hot encodings) of the data labels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random selection of n query images/indices, the same for all experiments\n",
    "n = 200\n",
    "n_examples = 5000  # the retrieval test set has 5000 images\n",
    "indices = np.random.choice(range(n_examples), size=n, replace=False)\n",
    "\n",
    "representations = [\n",
    "    (nc_mlp_train, nc_mlp_test),\n",
    "    (nc_cnn_train, nc_cnn_test),\n",
    "    (nc_dae_train, nc_dae_test),\n",
    "    (nc_sae_train, nc_sae_test),\n",
    "    (nc_pca_train, nc_pca_test),\n",
    "]\n",
    "for (nc_train, nc_test) in representations:\n",
    "    # === SOLUTION: ===\n",
    "    # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**b)** Compare the \"baseline\" PCA-transformed data with the other methods. Is PCA a suitable method to obtain representations for image retrieval in this situation? Why do you think so? Would you expect a similar conclusion for the Caltech101 dataset from Question 1?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Observe the difference between encodings from the DAE and SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean activation for DAE encodings:\", np.mean(nc_dae_train))\n",
    "print(\"Mean activation for SAE encodings:\", np.mean(nc_sae_train))\n",
    "\n",
    "index = 1  # try a few indices here\n",
    "print(\"DAE encoding example:\")\n",
    "print(nc_dae_train[index])\n",
    "print(\"SAE encoding example:\")\n",
    "print(nc_sae_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Discuss the difference in encodings between the two autoencoders (denoising and sparse). Also discuss the difference in retrieval performance for these encodings. How would you explain this difference?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**d)** What is the best performing method you found in part a)? Describe what advantage you believe this method has over the others.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
