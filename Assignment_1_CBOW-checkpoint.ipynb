{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'D:/recsys/alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[242, 6, 26, 1, 63, 243],\n",
       " [11, 9, 584, 3, 67, 27, 244, 8, 585, 71, 14, 380, 21, 1],\n",
       " [381, 2, 8, 245, 112, 3, 49, 98, 57, 586, 4, 17, 587, 72, 1],\n",
       " [205, 14, 380, 9, 588, 19, 7, 17, 50, 246, 57, 589, 10],\n",
       " [7, 2, 44, 33, 1, 152, 8, 5, 205, 53, 11, 174, 246, 57],\n",
       " [30, 4, 9, 382, 10, 14, 247, 248, 15, 127, 15, 4, 58, 18, 1],\n",
       " [249, 250, 206, 14, 383, 27, 384, 2, 385, 386, 1, 590],\n",
       " [8, 387, 5, 591, 592, 59, 20, 388, 1, 389, 8, 138, 43, 2],\n",
       " [593, 1, 594, 46, 175, 5, 128, 63, 23, 595, 176, 207],\n",
       " [295, 71, 14],\n",
       " [55, 9, 112, 30, 27, 390, 10, 13, 596, 99, 11, 60, 7, 30],\n",
       " [27, 91, 31, 8, 1, 45, 3, 296, 1, 63, 83, 3, 297, 51, 64],\n",
       " [51, 64, 6, 92, 20, 298, 46, 4, 53, 7, 100, 597, 7],\n",
       " [598, 3, 14, 13, 4, 299, 3, 84, 599, 34, 28, 19, 34, 1, 66],\n",
       " [7, 22, 113, 78, 391, 19, 46, 1, 63, 600, 139, 5, 392],\n",
       " [31, 8, 114, 393, 251, 2, 129, 34, 7, 2, 79, 252, 21],\n",
       " [11, 394, 3, 14, 101, 18, 7, 601, 395, 14, 248, 13, 4, 17],\n",
       " [115, 130, 208, 5, 63, 23, 140, 5, 393, 251, 57, 5, 392],\n",
       " [3, 253, 31, 8, 7, 2, 602, 23, 603, 4, 207, 395, 1, 604],\n",
       " [102, 7, 2, 605, 9, 141, 10, 66, 3, 56, 7, 606, 26, 5, 153],\n",
       " [63, 243, 300, 1, 607],\n",
       " [10, 254, 131, 26, 52, 11, 102, 7, 115, 98, 382, 38],\n",
       " [10, 1, 301, 4, 9, 3, 67, 31, 54],\n",
       " [1, 63, 243, 52, 608, 21, 47, 5, 609, 18, 177, 45, 2, 79],\n",
       " [610, 175, 26, 30, 175, 13, 11, 17, 29, 5, 131, 3, 60],\n",
       " [40, 611, 41, 130, 4, 73, 41, 396, 26, 5, 27, 302],\n",
       " [140, 1, 127, 9, 27, 302, 57, 4, 209, 27, 303, 18, 4, 17],\n",
       " [612, 8, 66, 15, 4, 52, 26, 3, 154, 40, 14, 2, 3, 103, 44, 9],\n",
       " [116, 3, 304, 210, 104, 4, 211, 3, 154, 26, 2, 212, 31, 44],\n",
       " [4, 9, 255, 3, 19, 7, 9, 155, 305, 3, 56, 213, 79, 4],\n",
       " [129, 34, 1, 613, 8, 1, 127, 2, 306, 13, 39, 65, 397, 23],\n",
       " [398, 2, 205, 399, 68, 2, 55, 4, 400, 614, 2, 246],\n",
       " [615, 142, 616, 4, 139, 26, 5, 401, 256, 48, 8, 1, 399, 15],\n",
       " [4, 617, 7, 9, 618, 619, 620, 19, 3, 14, 117],\n",
       " [621, 7, 9, 622, 4, 99, 29, 47, 3, 623, 1, 401, 18, 402],\n",
       " [8, 624, 307, 30, 403, 3, 178, 7, 72, 48, 8, 1, 398, 15],\n",
       " [4, 209, 625, 7],\n",
       " [127, 53, 11, 3, 41, 102, 74, 5, 257, 15, 28, 6, 92],\n",
       " [60, 112, 8, 626, 26, 627, 38, 628, 39, 37, 22, 60, 42, 34],\n",
       " [308, 132, 6, 404, 35, 83, 213, 40, 7, 258, 32, 6, 209, 80, 1, 405],\n",
       " [8, 1, 156, 85, 9, 27, 406, 629],\n",
       " [26, 26, 26, 59, 1, 257, 115, 86, 3, 133, 309, 6, 103, 38],\n",
       " [310, 407, 6, 143, 311, 71, 28, 66, 4, 16, 408, 6, 69, 20, 138],\n",
       " [409, 259, 1, 630, 8, 1, 410, 105, 42, 56, 13, 59, 20, 214],\n",
       " [631, 407, 26, 6, 60, 18, 12, 56, 11, 17, 632, 312],\n",
       " [93, 8, 28, 179, 10, 14, 313, 10, 1, 633, 2, 411, 28],\n",
       " [9, 29, 5, 27, 157, 412, 18, 634, 80, 14, 413, 15, 55],\n",
       " [9, 50, 48, 3, 414, 3, 14, 314, 7, 9, 157, 635, 3, 83, 7, 100],\n",
       " [415, 13, 36, 40, 1, 158, 315, 19, 79, 6, 103, 44, 416],\n",
       " [57, 417, 6, 143, 118, 3, 11, 17, 50, 260, 44, 416, 9, 57],\n",
       " [417, 140, 19, 53, 39, 65, 215, 636, 159, 3, 83],\n",
       " [637, 4, 75, 54, 6, 103, 32, 6, 92, 257, 158, 144, 1],\n",
       " [410, 38, 418, 7, 37, 316, 3, 86, 31, 261, 1, 216, 13, 317, 23],\n",
       " [180, 318, 638, 1, 639, 6, 60, 4, 9, 181, 419],\n",
       " [55, 9, 50, 48, 640, 28, 66, 15, 7, 262, 35, 641, 34, 22, 1],\n",
       " [158, 642, 19, 6, 92, 84, 3, 263, 61, 44, 1, 319, 8, 1, 643],\n",
       " [33, 12, 62, 217, 320, 160, 33, 28, 420, 644, 57, 645, 2],\n",
       " [4, 211, 3, 646, 15, 4, 421, 321, 647, 15, 12, 322, 396],\n",
       " [144, 1, 264, 49, 12, 60, 12, 58, 323, 7, 2, 44, 133],\n",
       " [648, 24, 422, 4, 37, 60, 42, 18, 423, 50, 7, 37, 115, 49, 3],\n",
       " [263, 265, 6, 92, 56, 7, 649, 43, 409],\n",
       " [26, 26, 26, 55, 9, 112, 266, 3, 49, 30, 11, 76, 75],\n",
       " [182, 54, 87, 37, 424, 42, 27, 91, 3, 324, 6, 218, 60],\n",
       " [87, 9, 1, 267, 6, 650, 39, 37, 219, 14, 651, 8, 652, 34],\n",
       " [653, 66, 87, 70, 64, 6, 145, 12, 65, 26, 68, 23, 42, 55, 106, 50],\n",
       " [325, 10, 1, 264, 6, 77, 268, 19, 12, 161, 425, 5, 426, 2, 13, 36, 27],\n",
       " [47, 5, 25, 12, 62, 19, 49, 107, 134, 269, 6, 103, 2, 68, 11],\n",
       " [75, 3, 67, 181, 384, 2, 52, 21, 183, 3, 41, 10, 5, 654],\n",
       " [179, 8, 45, 49, 107, 134, 269, 49, 107, 134, 269, 2, 427, 49],\n",
       " [269, 134, 107, 18, 12, 56, 15, 4, 655, 35, 326, 140, 162],\n",
       " [7, 262, 35, 91, 428, 85, 45, 4, 178, 7, 4, 184, 13, 4, 9, 656],\n",
       " [80, 2, 17, 141, 429, 3, 657, 13, 4, 9, 430, 185, 10, 185, 23],\n",
       " [87, 2, 183, 3, 14, 27, 658, 81, 87, 146, 42, 1, 659],\n",
       " [99, 12, 119, 134, 5, 426, 46, 175, 431, 431, 26, 4, 163, 142],\n",
       " [5, 660, 8, 661, 2, 164, 662, 2, 1, 257, 9, 100],\n",
       " [11, 9, 29, 5, 432, 433, 2, 4, 663, 43, 21, 3, 14, 101, 10, 5, 131],\n",
       " [4, 129, 43, 19, 7, 9, 22, 305, 664, 130, 14, 9, 254],\n",
       " [94, 327, 2, 1, 128, 63, 9, 314, 10, 328, 665, 26, 7],\n",
       " [55, 9, 29, 5, 131, 3, 20, 434, 165, 52, 11, 47, 1, 666, 2],\n",
       " [9, 141, 10, 66, 3, 296, 7, 83, 15, 7, 220, 5, 435, 51, 70, 436],\n",
       " [2, 437, 38, 298, 7, 36, 138, 4, 9, 295, 329, 7, 46, 4],\n",
       " [220, 1, 435, 19, 1, 63, 9, 50, 667, 3, 20, 208, 4, 73],\n",
       " [41, 10, 5, 94, 186, 147, 85, 9, 668, 43, 71, 5, 438, 8, 669, 670],\n",
       " [256, 1, 439],\n",
       " [55, 65, 440, 22, 108, 1, 147, 19, 39, 65, 22, 671, 2, 46],\n",
       " [11, 17, 166, 22, 1, 45, 26, 48, 441, 2, 43, 1, 221, 187, 330],\n",
       " [95, 4, 442, 443, 26, 1, 444, 672, 38, 4, 9, 119, 3],\n",
       " [67, 31, 54],\n",
       " [175, 4, 163, 142, 5, 24, 331, 673, 120, 22, 206, 8, 674],\n",
       " [222, 55, 9, 112, 21, 7, 675, 5, 445, 188, 148, 2, 11, 36],\n",
       " [104, 53, 9, 13, 7, 161, 676, 3, 48, 8, 1, 440, 8, 1, 147],\n",
       " [19, 332, 140, 1, 677, 65, 155, 153, 57, 1, 148, 9, 155, 167],\n",
       " [19, 34, 121, 270, 7, 59, 29, 678, 121, 8, 61, 135, 21, 1, 679],\n",
       " [66, 108, 4, 163, 142, 5, 186, 680, 4, 17, 29, 306, 130, 2],\n",
       " [329, 7, 9, 5, 24, 95, 40, 681, 333, 168, 4, 211, 1],\n",
       " [24, 188, 148, 10, 1, 682, 2, 3, 14, 117, 683, 7, 684],\n",
       " [11, 446, 1, 95, 2, 73, 13, 7, 447, 72, 5, 167, 327, 29],\n",
       " [91, 448, 223, 5, 685, 243, 4, 686, 26, 2, 129, 334, 1, 327],\n",
       " [72, 1, 687, 169, 12, 119, 400, 38, 4, 688, 3, 67, 31, 8],\n",
       " [13, 305, 147, 2, 689, 40, 261, 271, 690, 8, 335, 691, 2],\n",
       " [271, 692, 693, 19, 4, 58, 29, 258, 67, 14, 189, 144, 1],\n",
       " [694, 2, 258, 32, 70, 189, 59, 82, 144, 53, 109, 11, 7],\n",
       " [59, 20, 8, 27, 24, 152, 174, 70, 695, 51, 38, 6, 145, 6, 58],\n",
       " [449, 43, 47, 5, 450, 6, 60, 6, 58, 32, 6, 122, 451, 38, 3, 696],\n",
       " [18, 12, 56, 30, 310, 31, 8, 1, 45, 93, 17, 336, 697],\n",
       " [13, 11, 17, 429, 3, 60, 13, 27, 337, 93, 190, 65, 272],\n",
       " [55, 113, 3, 20, 50, 152, 10, 452, 71, 1, 24, 95, 30, 4, 52],\n",
       " [110, 3, 1, 120, 149, 453, 4, 161, 150, 254, 148, 21, 7, 57, 34],\n",
       " [121, 270, 5, 205, 8, 454, 18, 455, 216, 43, 47, 698, 28],\n",
       " [66, 4, 73, 5, 24, 273, 21, 7, 85, 456, 9, 29, 68],\n",
       " [130, 16, 11, 2, 108, 1, 699, 8, 1, 273, 9, 5, 700],\n",
       " [701, 23, 1, 159, 338, 42, 457, 702, 21, 7, 10, 153],\n",
       " [7, 9, 22, 27, 127, 3, 83, 338, 42, 19, 1, 703, 24, 11, 9],\n",
       " [29, 116, 3, 49, 13, 10, 5, 339, 50, 6, 37, 154, 104, 4, 16, 2],\n",
       " [56, 386, 7, 36, 224, 340, 57, 29, 18, 4, 17, 704, 312, 215],\n",
       " [24, 705, 40, 274, 96, 17, 118, 706, 2, 707, 43, 71, 708],\n",
       " [709, 2, 221, 710, 93, 22, 711, 39, 59, 29, 219],\n",
       " [1, 712, 454, 180, 713, 17, 714, 61, 74, 15, 13, 5, 715, 249],\n",
       " [716, 191, 717, 12, 32, 12, 458, 7, 155, 94, 2, 13, 32, 12, 718, 88],\n",
       " [341, 27, 719, 23, 5, 720, 7, 459, 721, 2, 4, 17, 115],\n",
       " [460, 13, 32, 12, 338, 91, 256, 5, 273, 224, 340, 7, 33],\n",
       " [342, 343, 3, 722, 23, 12, 723, 57, 724],\n",
       " [135, 28, 273, 9, 29, 224, 340, 30, 11, 725, 3, 461],\n",
       " [7, 2, 462, 7, 27, 215, 7, 17, 10, 463, 5, 179, 8, 726, 727],\n",
       " [8, 728, 729, 730, 731, 732, 733, 734, 735, 2, 249],\n",
       " [736, 737, 4, 27, 76, 344, 7, 80],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [44, 5, 345, 464, 16, 11, 6, 69, 20, 455, 43, 47, 5],\n",
       " [2, 30, 7, 9, 190, 4, 9, 81, 122, 738, 333, 168, 2, 14, 346],\n",
       " [739, 43, 34, 1, 53, 13, 4, 9, 81, 1, 158, 465, 18, 116],\n",
       " [144, 1, 24, 95, 72, 13, 740, 169, 104, 135, 4],\n",
       " [466, 18, 5, 337, 467, 3, 56, 32, 4, 9, 116, 3, 741, 121, 742],\n",
       " [4, 184, 5, 24, 743, 40, 28, 18, 7, 161, 309, 12, 62, 16],\n",
       " [11, 3, 41, 10, 70, 116, 31, 468, 47, 5, 347, 6, 103],\n",
       " [44, 6, 218, 20, 47, 79, 2, 4, 211, 3, 321, 44, 1, 744, 8, 5],\n",
       " [347, 33, 47, 102, 1, 347, 33, 745, 31, 18, 4, 58, 29, 219],\n",
       " [119, 245, 208, 74, 5, 89],\n",
       " [102, 5, 192, 462, 13, 112, 123, 336, 4, 746, 21, 116],\n",
       " [72, 1, 169, 34, 98, 19, 332, 18, 109, 11, 46, 4, 118, 3, 1],\n",
       " [95, 4, 73, 4, 17, 460, 1, 24, 188, 148, 2, 46, 4],\n",
       " [52, 110, 3, 1, 120, 18, 7, 4, 73, 4, 58, 29, 747, 469],\n",
       " [7, 4, 58, 56, 7, 78, 748, 144, 1, 222, 2, 4, 211, 14],\n",
       " [225, 3, 749, 43, 48, 8, 1, 750, 8, 1, 120, 19, 7, 9, 155, 751],\n",
       " [2, 46, 4, 17, 244, 41, 31, 23, 187, 1, 109, 24, 89],\n",
       " [226, 26, 2, 124],\n",
       " [86, 55, 36, 50, 152, 10, 470, 47, 13, 16, 11, 3, 41],\n",
       " [181, 471, 6, 752, 12, 3, 472, 80, 28, 473, 4, 348],\n",
       " [474, 41, 27, 157, 753, 411, 4, 27, 754, 755, 7],\n",
       " [2, 427, 4, 756, 41, 30, 475, 15, 3, 757, 170, 72],\n",
       " [14, 176, 2, 98, 4, 349, 187, 3, 350, 14, 247, 436, 18, 245],\n",
       " [758, 41, 10, 5, 759, 8, 760, 4, 9, 761, 476, 41],\n",
       " [18, 28, 345, 477, 9, 27, 478, 8, 762, 3, 20, 227, 216],\n",
       " [19, 7, 36, 50, 152, 81, 53, 109, 11, 3, 763, 3, 20, 227, 216],\n",
       " [132, 55, 36, 764, 275, 8, 42, 351, 3, 212, 48, 765, 352],\n",
       " [76, 14, 353, 209, 21, 5, 24, 222, 350, 13, 9, 354, 300, 1, 120],\n",
       " [4, 446, 7, 2, 73, 10, 7, 5, 27, 167, 355, 21, 85, 1, 159],\n",
       " [134, 42, 65, 457, 224, 10, 766, 127, 6, 37, 134, 7, 16],\n",
       " [11, 2, 32, 7, 479, 42, 480, 448, 6, 97, 469, 1, 148, 2, 32, 7],\n",
       " [479, 42, 480, 767, 6, 97, 768, 300, 1, 95, 30, 140, 45, 6, 37],\n",
       " [67, 72, 1, 169, 2, 6, 171, 35, 769, 85, 481],\n",
       " [4, 770, 5, 24, 432, 2, 16, 276, 3, 41, 85, 45, 85],\n",
       " [45, 482, 14, 185, 21, 1, 405, 8, 14, 189, 3, 383, 85, 45, 7, 9],\n",
       " [483, 2, 4, 9, 78, 277, 3, 150, 13, 4, 771, 1, 193],\n",
       " [465, 3, 20, 136, 28, 348, 481, 46, 48, 772, 355, 19, 11],\n",
       " [17, 118, 30, 91, 72, 1, 45, 8, 773, 112, 19, 31, 8, 1, 45],\n",
       " [93, 3, 304, 13, 7, 113, 78, 774, 2, 385, 18, 356, 3, 82, 21],\n",
       " [10, 1, 775, 45],\n",
       " [30, 4, 776, 3, 777, 2, 27, 76, 344, 80, 1, 355],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [242, 778, 1, 137, 8, 170],\n",
       " [484, 2, 484, 124, 11, 4, 9, 30, 91, 277, 13],\n",
       " [18, 1, 131, 4, 78, 485, 38, 3, 278, 157, 228, 81, 6, 77],\n",
       " [779, 31, 47, 1, 780, 450, 13, 119, 9, 157, 781, 101],\n",
       " [18, 46, 4, 129, 26, 34, 14, 101, 39, 113, 3, 20, 342, 31, 8],\n",
       " [328, 39, 65, 138, 30, 486, 80, 51, 70, 109, 24, 101, 6, 103],\n",
       " [96, 191, 178, 21, 88, 782, 2, 783, 18, 12, 81, 487, 6, 77, 136],\n",
       " [6, 784, 35, 20, 785, 6, 92, 20, 5, 117, 357, 155, 486, 80, 3, 389],\n",
       " [786, 40, 12, 12, 69, 323, 1, 225, 45, 12, 97, 19, 6, 69, 20],\n",
       " [787, 3, 61, 53, 11, 57, 265, 39, 172, 35, 317, 1, 45, 6, 788],\n",
       " [3, 82, 105, 42, 56, 6, 37, 488, 61, 5, 420, 229, 8, 789, 330, 790],\n",
       " [2, 4, 52, 21, 791, 3, 41, 38, 4, 59, 323, 7, 39, 69],\n",
       " [82, 71, 1, 792, 4, 53, 2, 38, 418, 7, 37, 316, 489],\n",
       " [793, 3, 48, 36, 247, 101, 2, 38, 794, 1, 795, 191, 154],\n",
       " [11, 36, 158, 490, 796],\n",
       " [797],\n",
       " [259, 1, 798],\n",
       " [23, 11, 36, 799],\n",
       " [51, 64, 44, 491, 6, 77, 182],\n",
       " [141, 79, 14, 189, 800, 476, 1, 439, 8, 1, 147, 10, 463, 4, 9],\n",
       " [81, 123, 223, 492, 101, 168, 2, 4, 34, 98, 139, 43, 1, 24, 188],\n",
       " [148, 2, 252, 80, 3, 1, 169, 95],\n",
       " [109, 11, 7, 9, 15, 91, 15, 4, 58, 49, 354, 26, 21, 48, 441, 3],\n",
       " [154, 144, 72, 1, 169, 23, 48, 353, 19, 3, 67, 144, 9, 123],\n",
       " [801, 223, 119, 4, 226, 26, 2, 75, 3, 493, 54],\n",
       " [12, 299, 3, 20, 802, 8, 494, 16, 11, 5, 117, 422, 47],\n",
       " [12, 4, 161, 127, 83, 28, 3, 82, 21, 470, 10, 28, 45, 495, 28],\n",
       " [131, 6, 146, 12, 19, 4, 52, 21, 22, 1, 193, 803, 804, 8],\n",
       " [170, 805, 55, 9, 5, 153, 137, 22, 108, 14, 40, 214, 333],\n",
       " [302, 2, 806, 149, 26, 1, 147],\n",
       " [102, 5, 66, 4, 230, 5, 24, 496, 8, 101, 10, 1, 315, 2],\n",
       " [4, 279, 807, 14, 176, 3, 56, 44, 9, 255, 7, 9, 1, 128],\n",
       " [63, 808, 809, 810, 23, 5, 229, 8, 128, 231, 125, 10],\n",
       " [48, 185, 2, 5, 153, 126, 10, 1, 221, 111, 163, 497, 334, 10, 5, 117],\n",
       " [339, 498, 3, 811, 15, 111, 163, 51, 1, 280, 1, 280],\n",
       " [51, 172, 35, 4, 20, 812, 32, 6, 143, 281, 14, 452, 11, 184, 30],\n",
       " [813, 13, 4, 9, 232, 3, 263, 499, 8, 121, 48, 30, 46, 1, 63],\n",
       " [163, 259, 14, 4, 75, 10, 5, 186, 814, 233, 32, 12, 217, 500],\n",
       " [1, 63, 394, 815, 358, 1, 128, 231, 125, 2, 1, 126],\n",
       " [2, 816, 165, 72, 1, 817, 15, 501, 15, 111, 58, 82],\n",
       " [11, 139, 43, 1, 126, 2, 125, 2, 15, 1, 147, 9, 27, 249, 4],\n",
       " [281, 818, 41, 22, 1, 66, 4, 52, 21, 182, 64, 64, 38],\n",
       " [234, 282, 33, 3, 250, 2, 819, 93, 52, 21, 141, 15, 820],\n",
       " [6, 103, 32, 6, 143, 166, 235, 10, 1, 324, 105, 42, 60, 9, 6, 1],\n",
       " [193, 46, 6, 118, 43, 28, 502, 6, 342, 60, 6, 97, 219, 464, 5],\n",
       " [24, 821, 19, 32, 6, 77, 29, 1, 193, 1, 210, 162, 33, 96],\n",
       " [10, 1, 301, 160, 6, 503, 13, 36, 1, 117, 822, 2, 4, 75, 823],\n",
       " [100, 22, 1, 274, 4, 451, 13, 65, 8, 1, 193, 504, 15, 41, 3],\n",
       " [56, 32, 4, 58, 84, 166, 235, 18, 121, 8, 61],\n",
       " [6, 77, 136, 6, 77, 29, 824, 4, 16, 18, 14, 505, 825, 10, 74, 94],\n",
       " [506, 2, 507, 173, 35, 82, 10, 506, 34, 22, 2, 6, 77, 136, 6, 97, 35],\n",
       " [20, 283, 18, 6, 62, 22, 508, 8, 93, 2, 4, 51, 4, 826, 74, 5],\n",
       " [27, 24, 827, 4, 36, 4, 2, 6, 77, 6, 2, 51, 64, 38, 509],\n",
       " [7, 22, 33, 6, 37, 194, 32, 6, 62, 22, 1, 93, 6, 510, 3, 62, 105, 42],\n",
       " [56, 214, 359, 828, 33, 829, 2, 214, 359, 830, 33, 831, 2],\n",
       " [214, 359, 832, 33, 51, 64, 6, 92, 115, 67, 3, 833, 34, 13, 270],\n",
       " [135, 1, 834, 120, 173, 35, 835, 105, 36, 194, 836],\n",
       " [837, 33, 1, 284, 8, 511, 2, 511, 33, 1, 284, 8, 512, 2],\n",
       " [512, 50, 13, 36, 22, 838, 6, 77, 343, 6, 69, 84, 166, 235, 18],\n",
       " [283, 6, 37, 194, 2, 83, 38, 513, 1, 24, 2, 4, 839, 14],\n",
       " [514, 21, 14, 840, 15, 32, 4, 65, 183, 313, 2, 75, 3, 841, 7],\n",
       " [19, 14, 233, 842, 843, 2, 844, 2, 1, 159, 99, 29, 86, 1],\n",
       " [193, 15, 39, 510, 3, 49],\n",
       " [38, 513, 1, 24, 845],\n",
       " [846, 151, 847, 285],\n",
       " [2, 848, 1, 849, 8, 1, 850],\n",
       " [21, 330, 188, 851],\n",
       " [38, 852, 111, 360, 3, 853],\n",
       " [38, 854, 855, 151, 856],\n",
       " [2, 857, 24, 858, 10],\n",
       " [23, 859, 860, 861],\n",
       " [6, 77, 136, 271, 106, 29, 1, 158, 159, 16, 109, 11, 2, 14, 176],\n",
       " [397, 23, 170, 54, 15, 4, 52, 21, 6, 69, 20, 283, 102, 22, 2],\n",
       " [6, 92, 84, 3, 82, 2, 862, 10, 13, 863, 24, 156, 2, 84, 210, 3],\n",
       " [50, 864, 3, 865, 23, 2, 51, 119, 30, 310, 313, 3, 866, 50, 6, 143],\n",
       " [206, 43, 70, 248, 40, 7, 32, 6, 77, 283, 6, 37, 361, 26, 68, 7, 37, 20, 50],\n",
       " [152, 180, 867, 180, 318, 26, 2, 183, 86, 43, 54, 64, 6],\n",
       " [92, 122, 154, 43, 2, 83, 96, 160, 6, 79, 146, 42, 13, 104, 2, 79],\n",
       " [32, 6, 47, 362, 13, 352, 6, 37, 86, 43, 32, 29, 6, 37, 361, 26, 68],\n",
       " [868, 6, 77, 307, 266, 19, 51, 64, 124, 11, 23, 5, 363, 869],\n",
       " [8, 170, 6, 49, 145, 39, 59, 178, 180, 318, 26, 6, 160, 30, 27, 244],\n",
       " [8, 362, 22, 515, 68],\n",
       " [15, 4, 16, 28, 4, 129, 26, 34, 14, 514, 2, 9, 277, 3, 56],\n",
       " [13, 4, 17, 178, 21, 48, 8, 1, 63, 36, 24, 128, 231, 125, 192],\n",
       " [4, 9, 182, 38, 97, 6, 84, 516, 13, 4, 53, 6, 69],\n",
       " [20, 483, 167, 54, 4, 118, 43, 2, 52, 3, 1, 120, 3, 870],\n",
       " [41, 71, 7, 2, 73, 13, 15, 871, 15, 4, 58, 872, 4, 9, 81],\n",
       " [40, 227, 101, 168, 2, 9, 116, 21, 517, 873, 4, 76, 73],\n",
       " [31, 13, 1, 364, 8, 28, 9, 1, 126, 4, 9, 482, 2, 4, 358],\n",
       " [7, 279, 141, 10, 66, 3, 874, 517, 165, 468],\n",
       " [13, 9, 5, 875, 876, 16, 11, 5, 157, 357, 518, 34, 1],\n",
       " [363, 519, 19, 27, 419, 3, 150, 41, 314, 10, 877, 2],\n",
       " [81, 18, 1, 169, 2, 4, 207, 23, 22, 878, 110, 3, 1, 24, 95],\n",
       " [19, 332, 1, 24, 95, 9, 449, 54, 2, 1, 24, 188, 148, 9],\n",
       " [354, 21, 1, 222, 120, 15, 130, 2, 93, 106, 879, 223, 119],\n",
       " [53, 1, 109, 477, 18, 6, 115, 9, 30, 167, 15, 28, 130, 115],\n",
       " [2, 6, 880, 7, 36, 155, 520, 13, 7, 33],\n",
       " [15, 4, 16, 881, 159, 14, 490, 521, 2, 10, 254, 131, 882],\n",
       " [4, 9, 43, 3, 14, 883, 10, 522, 365, 14, 104, 260, 9, 13, 4],\n",
       " [17, 884, 311, 72, 1, 523, 2, 10, 13, 524, 6, 97, 82, 110, 71],\n",
       " [525, 4, 16, 3, 41, 11, 17, 166, 3, 1, 885, 98, 10],\n",
       " [14, 356, 2, 17, 86, 3, 1, 886, 887, 13, 888, 12, 82],\n",
       " [3, 21, 1, 228, 889, 12, 150, 5, 890, 8, 891, 892, 10, 1],\n",
       " [523, 177, 274, 893, 10, 1, 894, 23, 895, 896, 79, 5, 438],\n",
       " [8, 897, 898, 2, 329, 61, 5, 525, 899, 135, 4, 76],\n",
       " [206, 31, 13, 4, 9, 10, 1, 137, 8, 170, 85, 4, 17, 900, 46, 4],\n",
       " [9, 492, 101, 168],\n",
       " [6, 145, 6, 526, 35, 124, 30, 91, 16, 11, 15, 4, 236, 40, 187],\n",
       " [3, 150, 14, 45, 31, 6, 92, 20, 901, 18, 7, 81, 6, 527, 71],\n",
       " [362, 902, 10, 70, 247, 170, 13, 191, 20, 5, 234, 89, 3, 20, 136],\n",
       " [135, 282, 33, 234, 3, 250],\n",
       " [141, 79, 4, 230, 286, 903, 40, 10, 1, 137, 5, 24, 45],\n",
       " [80, 2, 4, 236, 904, 3, 212, 31, 44, 7, 9, 34, 104, 4, 53],\n",
       " [7, 69, 20, 5, 905, 57, 906, 19, 79, 4, 349, 38, 167],\n",
       " [4, 9, 81, 2, 4, 76, 206, 31, 13, 7, 9, 122, 5, 25, 13, 17],\n",
       " [521, 10, 47, 41],\n",
       " [59, 7, 20, 8, 121, 152, 81, 53, 11, 3, 278, 3, 28, 25],\n",
       " [282, 33, 30, 31, 8, 1, 45, 26, 68, 13, 6, 218, 60, 27],\n",
       " [406, 7, 97, 237, 34, 121, 270, 55, 36, 50, 907, 10, 187, 30, 4],\n",
       " [75, 366, 25, 49, 12, 62, 1, 45, 31, 8, 28, 137, 6, 160, 27, 244],\n",
       " [8, 528, 40, 68, 366, 25, 11, 53, 28, 69, 20, 1, 158],\n",
       " [45, 8, 529, 3, 5, 25, 4, 17, 115, 516, 74, 5, 89, 130, 19],\n",
       " [4, 349, 245, 208, 10, 14, 908, 36, 909, 910, 5, 25, 8],\n",
       " [5, 25, 3, 5, 25, 5, 25, 366, 25, 1, 25, 129, 34, 14, 181],\n",
       " [911, 2, 113, 3, 14, 3, 912, 23, 48, 8, 114, 24, 176],\n",
       " [19, 7, 16, 112],\n",
       " [265, 7, 173, 35, 530, 228, 53, 11, 6, 913, 7, 36],\n",
       " [5, 531, 25, 86, 100, 23, 287, 1, 532, 18, 23, 22],\n",
       " [14, 413, 8, 367, 11, 17, 50, 27, 914, 915, 38, 94, 916],\n",
       " [213, 17, 336, 30, 4, 75, 54, 917, 918, 320, 919, 85],\n",
       " [9, 1, 104, 920, 10, 14, 531, 533, 205, 1, 25, 474, 5],\n",
       " [363, 921, 31, 8, 1, 365, 2, 113, 3, 922, 22, 100, 23, 923],\n",
       " [51, 6, 195, 88, 288, 124, 11, 279, 268, 13, 4, 17, 433],\n",
       " [1, 109, 924, 36, 925, 6, 78, 485, 12, 262, 35, 47, 107],\n",
       " [29, 47, 107, 124, 1, 25, 10, 5, 926, 927, 233, 59],\n",
       " [12, 47, 107, 32, 12, 65, 42],\n",
       " [127, 265, 29, 16, 11, 10, 5, 928, 196, 171, 35, 20, 534],\n",
       " [40, 7, 2, 929, 6, 145, 6, 58, 535, 12, 197, 267, 87, 6, 60, 12, 198],\n",
       " [253, 5, 321, 3, 107, 32, 12, 58, 122, 56, 14, 4, 33, 74, 5, 64, 930],\n",
       " [89, 11, 52, 21, 149, 3, 41, 15, 4, 236, 931, 40, 10, 1],\n",
       " [137, 2, 4, 932, 933, 30, 934, 71, 1, 935, 936, 14, 536, 2],\n",
       " [937, 14, 346, 2, 4, 33, 74, 5, 215, 938, 89, 3, 537, 2, 4, 36],\n",
       " [74, 5, 284, 48, 18, 538, 325, 51, 6, 195, 88, 288, 124],\n",
       " [11, 54, 18, 28, 66, 1, 25, 9, 939, 22, 100, 2, 4],\n",
       " [184, 343, 7, 69, 20, 272, 238, 239, 172, 35, 237, 40, 14, 121],\n",
       " [123, 32, 12, 198, 181, 29],\n",
       " [239, 190, 124, 1, 25, 96, 9, 368, 26, 3, 1, 309, 8, 151],\n",
       " [285, 15, 32, 6, 59, 237, 21, 74, 5, 539, 197, 940, 369, 941],\n",
       " [107, 942, 186, 943, 93, 171, 35, 105, 42, 296, 1, 319, 54],\n",
       " [6, 172, 35, 190, 16, 11, 10, 5, 117, 339, 3, 519, 1, 539, 8],\n",
       " [944, 106, 12, 106, 12, 478, 8, 8, 370, 1, 25, 99, 29],\n",
       " [326, 30, 11, 52, 21, 371, 55, 33, 74, 5, 215, 24, 945, 259],\n",
       " [197, 156, 6, 218, 47, 3, 535, 12, 5, 24, 335, 946, 947, 12],\n",
       " [62, 23, 51, 74, 94, 948, 949, 505, 2, 7, 37, 372, 93, 46],\n",
       " [12, 950, 61, 2, 7, 37, 540, 43, 2, 195, 18, 114, 951, 2, 22, 508],\n",
       " [8, 93, 6, 97, 35, 219, 149, 8, 61, 2, 7, 952, 3, 5, 953],\n",
       " [12, 62, 2, 111, 541, 7, 36, 30, 542, 7, 36, 388, 5, 954, 955, 111],\n",
       " [541, 7, 956, 22, 1, 957, 2, 51, 64, 124, 11, 10, 5, 958],\n",
       " [196, 6, 77, 268, 6, 143, 238, 7, 54, 18, 1, 25, 9, 528],\n",
       " [165, 256, 14, 15, 501, 15, 7, 58, 82, 2, 387, 78, 5, 959, 10],\n",
       " [1, 137, 15, 7, 52],\n",
       " [30, 4, 199, 960, 102, 7, 25, 64, 49, 86, 110, 54, 2, 239],\n",
       " [172, 35, 237, 40, 107, 57, 370, 140, 32, 12, 171, 35, 47, 61, 46, 1],\n",
       " [25, 230, 28, 7, 220, 108, 2, 236, 303, 110, 3, 14, 114],\n",
       " [346, 9, 78, 961, 23, 962, 11, 53, 2, 7, 16, 10, 5, 186],\n",
       " [368, 233, 105, 373, 67, 3, 1, 543, 2, 79, 6, 37, 146, 12, 70],\n",
       " [367, 2, 12, 37, 530, 132, 7, 33, 6, 544, 107, 2, 370],\n",
       " [7, 9, 168, 66, 3, 82, 18, 1, 137, 9, 138, 78, 289, 23, 1],\n",
       " [200, 2, 545, 13, 17, 311, 72, 7, 55, 65, 5, 374, 2, 5, 90],\n",
       " [5, 201, 2, 133, 375, 2, 312, 221, 345, 963, 11, 447, 1],\n",
       " [45, 2, 1, 290, 240, 236, 3, 1, 543],\n",
       " [242, 964, 5, 376, 202, 2, 5, 94, 377],\n",
       " [39, 65, 190, 5, 234, 203, 240, 13, 965, 21, 1, 381, 1],\n",
       " [200, 23, 966, 967, 1, 545, 23, 180, 546, 968, 295],\n",
       " [3, 61, 2, 22, 969, 547, 970, 2, 971],\n",
       " [1, 104, 162, 8, 204, 9, 38, 3, 67, 164, 54, 39, 17, 5],\n",
       " [972, 40, 28, 2, 102, 5, 337, 467, 7, 113, 78, 391],\n",
       " [3, 11, 3, 150, 41, 182, 973, 23, 61, 15, 32, 4, 17],\n",
       " [974, 61, 22, 14, 356, 190, 4, 17, 78, 5, 94, 975, 23, 1],\n",
       " [201, 96, 34, 291, 220, 976, 2, 59, 122, 83, 6, 160, 977, 223],\n",
       " [12, 2, 69, 62, 548, 2, 28, 11, 59, 29, 978, 174],\n",
       " [979, 38, 292, 7, 9, 2, 15, 1, 201, 980, 981, 3, 146, 114],\n",
       " [504, 55, 9, 50, 123, 3, 20, 16],\n",
       " [34, 291, 1, 25, 96, 113, 3, 20, 5, 352, 8, 982, 261, 61],\n",
       " [199, 31, 540, 26, 22, 8, 12, 2, 414, 3, 42, 6, 37, 76, 212, 12],\n",
       " [164, 275, 39, 22, 226, 26, 34, 98, 10, 5, 153, 549, 23, 1, 25],\n",
       " [10, 1, 444, 11, 281, 14, 176, 276, 983, 21, 7, 18, 4, 184],\n",
       " [136, 4, 59, 425, 5, 520, 984, 32, 4, 99, 29, 67, 164, 27, 76],\n",
       " [985, 16, 1, 25, 23, 133, 986, 264, 106, 12, 22, 232, 28],\n",
       " [33, 1, 987, 89, 6, 62, 550, 22, 108, 32, 12, 217, 287],\n",
       " [1, 532, 988, 364, 9, 989, 71, 1, 990, 9, 76, 991],\n",
       " [3, 71, 1, 228, 96, 551, 992, 2, 17, 166, 8, 298, 91],\n",
       " [993, 3, 994, 2, 995, 552, 2, 553, 1, 554, 8],\n",
       " [555, 2, 556],\n",
       " [996, 16, 1, 201, 23, 5, 997],\n",
       " [6, 195, 88, 288, 16, 1, 25, 998, 19, 27, 999, 99],\n",
       " [29, 6, 16, 1, 201, 279],\n",
       " [6, 53, 12, 99, 16, 1, 25, 6, 1000, 552, 2, 553],\n",
       " [1, 554, 8, 555, 2, 556, 1001, 18, 293, 2, 258, 1002],\n",
       " [1, 1003, 1004, 8, 1005, 73, 7, 557],\n",
       " [73, 44, 16, 1, 374],\n",
       " [73, 7, 1, 25, 378, 181, 1006, 8, 204, 12, 62, 44],\n",
       " [6, 62, 44, 7, 1007, 127, 275, 46, 6, 150, 5, 89, 16, 1],\n",
       " [374, 7, 36, 348, 5, 1008, 57, 5, 1009, 1, 162, 33, 44, 99, 1],\n",
       " [1, 25, 99, 29, 1010, 28, 162, 19, 1011, 52, 21, 73],\n",
       " [7, 557, 3, 82, 23, 1012, 1013, 3, 558, 287, 2, 1014, 293, 1],\n",
       " [1015, 287, 36, 1016, 34, 104, 9, 1017, 19, 1, 1018, 8, 151],\n",
       " [1019, 38, 106, 12, 138, 21, 81, 70, 64, 7, 1020, 379],\n",
       " [3, 11, 15, 7, 421],\n",
       " [15, 547, 15, 119, 16, 11, 10, 5, 559, 196, 7, 173, 35, 316, 3],\n",
       " [164, 42, 34, 22],\n",
       " [10, 13, 524, 16, 1, 90, 560, 1021, 3, 114, 101, 6, 1022],\n",
       " [13, 1, 1023, 1024, 18, 1, 1025, 1026, 8, 123, 1027],\n",
       " [278, 228, 16, 1, 375, 6, 171, 35, 62, 1, 1028, 8, 149],\n",
       " [271, 94, 159, 2, 44, 36, 123, 6, 171, 35, 1029, 12, 49, 140, 2],\n",
       " [1, 375, 1030, 26, 114, 189, 3, 1031, 5, 1032, 177, 8, 1, 221, 200],\n",
       " [44, 6, 9, 116, 3, 83, 16, 1, 90, 10, 133, 238, 196, 9, 13],\n",
       " [1, 225, 89, 3, 67, 373, 164, 59, 20, 5, 376, 202],\n",
       " [44, 33, 5, 376, 202, 16, 11, 29, 13, 4, 551, 91, 3, 62],\n",
       " [19, 1, 90, 17, 1033, 15, 32, 7, 53, 13, 307, 299, 3, 278],\n",
       " [2, 50, 48, 266, 113, 1034, 3, 83, 213],\n",
       " [132, 16, 1, 90, 1, 225, 45, 3, 561, 7, 33, 3, 49, 7, 2, 15],\n",
       " [12, 161, 47, 3, 194, 1, 89, 494, 177, 1035, 250, 6, 191, 146],\n",
       " [12, 38, 1, 90, 403, 7],\n",
       " [104, 7, 224, 31, 5, 202, 204, 10, 5, 179, 8, 1036, 1, 1037],\n",
       " [1038, 173, 35, 428, 7, 16, 2, 79, 22, 1, 240, 65, 1039],\n",
       " [334, 1, 204, 68, 2, 55, 55, 9, 50, 48, 227, 331, 2],\n",
       " [165, 19, 39, 75, 562, 46, 39, 563, 2, 351, 80, 46, 39],\n",
       " [563, 30, 13, 7, 9, 29, 1040, 3, 62, 46, 1, 202, 9, 100, 135],\n",
       " [46, 39, 17, 166, 562, 149, 133, 1041, 57, 30, 2, 65, 78, 164, 54],\n",
       " [1, 90, 175, 199, 31, 1, 202, 33, 100, 2, 39, 22, 289],\n",
       " [108, 7, 1042, 2, 423, 19, 96, 564, 172],\n",
       " [28, 162, 1, 90, 58, 29, 326, 174, 5, 117, 357, 8, 53],\n",
       " [2, 7, 226, 18, 5, 94, 66, 23, 48, 341, 1043, 142, 114, 1044],\n",
       " [1, 1045, 10, 85, 12, 459, 56, 1046, 10, 1, 246],\n",
       " [8, 293, 192, 1, 1047, 466, 10, 550, 34, 291, 1, 90, 16],\n",
       " [1048, 564, 172, 2, 22, 69, 84, 241],\n",
       " [19, 96, 33, 3, 488, 1, 241, 78, 5, 565, 8, 1049, 1050],\n",
       " [132, 4, 8, 204, 16, 1, 90, 1051, 3, 11, 23, 48, 341],\n",
       " [2, 1, 290, 240, 34, 98, 289, 108, 14, 1052, 31, 10, 5, 1053],\n",
       " [45, 241, 241],\n",
       " [11, 17, 50, 260, 44, 3, 49, 2, 10, 1054, 4, 178, 14, 185, 10, 14],\n",
       " [251, 2, 1055, 31, 5, 350, 8, 566, 1056, 1, 522, 365, 17],\n",
       " [29, 118, 72, 7, 2, 1057, 61, 108, 15, 241, 55, 9, 1058, 48],\n",
       " [5, 1059, 22, 108],\n",
       " [19, 4, 69, 84, 5, 1060, 41, 12, 62, 16, 1, 25],\n",
       " [8, 204, 1, 90, 378, 27, 1061, 44, 266, 84, 12, 118, 10],\n",
       " [88, 251, 111, 52, 21, 379, 3, 11],\n",
       " [122, 5, 294, 16, 11, 443],\n",
       " [185, 7, 100, 68, 16, 1, 90],\n",
       " [79, 39, 22, 289, 108, 14, 98, 123, 192, 1, 90, 560],\n",
       " [1062, 1, 294, 183, 239, 195, 88, 1063, 8, 28, 1064],\n",
       " [294, 2, 46, 7, 17, 344, 28, 1065, 567, 39, 22, 1066],\n",
       " [11, 53, 1, 290, 89, 27, 1067, 19, 39, 22, 129, 30, 1068],\n",
       " [13, 4, 99, 29, 1069, 3, 1070, 2, 15, 4, 58, 29, 60, 8, 213],\n",
       " [3, 83, 4, 1071, 1072, 2, 139, 1, 294, 203, 15, 1073, 15, 4],\n",
       " [1, 210, 89, 9, 3, 134, 1, 566, 28, 568, 177, 1074, 2],\n",
       " [1075, 15, 1, 153, 200, 1076, 13, 39, 58, 29, 461],\n",
       " [1077, 2, 1, 167, 1078, 1079, 2, 17, 3, 20, 1080, 21, 1, 110],\n",
       " [135, 7, 9, 100, 34, 291, 2, 39, 226, 26, 54, 10, 5, 549, 2],\n",
       " [1081, 1, 25, 3, 146, 61, 286, 123],\n",
       " [12, 1082, 3, 146, 42, 88, 367, 12, 62, 16, 11, 2, 132],\n",
       " [7, 33, 12, 544, 1083, 2, 198, 4, 1084, 10, 5, 1085, 149, 268, 13, 7],\n",
       " [59, 20, 238, 54],\n",
       " [507, 33, 5, 94, 2, 5, 569, 377, 16, 1, 25, 379, 3, 11, 2],\n",
       " [7, 33, 5, 94, 285, 456, 16, 11, 203, 26, 23, 103, 34],\n",
       " [1, 25, 36, 285, 19, 132, 49, 12, 1086, 7, 569, 2, 4, 281, 21, 509],\n",
       " [40, 7, 192, 1, 25, 9, 529, 30, 13, 14, 260, 8, 1, 377, 9],\n",
       " [286, 47, 28],\n",
       " [570, 16, 3, 5],\n",
       " [25, 13, 111],\n",
       " [1087, 10, 1],\n",
       " [156],\n",
       " [105, 373],\n",
       " [1088, 82, 3],\n",
       " [1089, 6, 191],\n",
       " [1090],\n",
       " [12, 86],\n",
       " [6, 37, 253, 50],\n",
       " [1091, 239],\n",
       " [69, 84, 5],\n",
       " [571, 18],\n",
       " [272, 28],\n",
       " [502, 6, 143],\n",
       " [112],\n",
       " [3, 49],\n",
       " [16, 1],\n",
       " [25, 3, 1],\n",
       " [1092, 74],\n",
       " [5, 571],\n",
       " [64, 500],\n",
       " [23],\n",
       " [50, 572],\n",
       " [57, 573],\n",
       " [59, 20],\n",
       " [1093],\n",
       " [197],\n",
       " [1094],\n",
       " [6, 37, 20],\n",
       " [573, 6, 37],\n",
       " [20, 572],\n",
       " [16],\n",
       " [1095],\n",
       " [292, 570],\n",
       " [6, 37],\n",
       " [194, 1],\n",
       " [290],\n",
       " [364],\n",
       " [2],\n",
       " [1096],\n",
       " [12],\n",
       " [3],\n",
       " [1097],\n",
       " [12, 106, 29, 1098, 16, 1, 25, 3, 11, 475, 44, 106, 12],\n",
       " [6, 195, 88, 288, 16, 11, 27, 1099, 12, 17, 118, 3, 1, 1100],\n",
       " [1101, 6, 60],\n",
       " [6, 17, 29, 124, 1, 25, 471, 2, 27, 1102],\n",
       " [5, 1103, 16, 11, 369, 232, 3, 212, 41, 542, 2, 203],\n",
       " [276, 40, 14, 51, 49, 105, 42, 499, 3, 1104, 7],\n",
       " [6, 92, 49, 112, 8, 1, 179, 16, 1, 25, 138, 43, 2, 430],\n",
       " [165, 12, 1105, 42, 71, 182, 74, 491],\n",
       " [6, 262, 35, 1106, 7, 1107, 109, 11, 19, 12, 322, 30, 1108, 238],\n",
       " [1, 25, 122, 1109, 10, 1110],\n",
       " [217, 86, 110, 2, 574, 88, 575, 11, 199, 102, 7, 2, 1],\n",
       " [1111, 22, 1112, 10, 565, 415, 217, 49, 19, 1, 25, 122, 1113],\n",
       " [114, 189, 1114, 2, 442, 5, 24, 1115],\n",
       " [44, 5, 1116, 7, 404, 35, 361, 1117, 1, 201, 15, 76, 15, 7, 9, 78],\n",
       " [31, 8, 328, 2, 133, 292, 576, 139, 1, 412, 8, 183, 3, 14],\n",
       " [1118, 503, 70, 64, 105, 28, 20, 5, 533, 3, 12, 115, 3, 1119],\n",
       " [88, 1120, 458, 88, 1121, 320, 16, 1, 1122, 576, 5, 24],\n",
       " [1123, 12, 322, 275, 3, 194, 1, 1124, 8, 133, 1125],\n",
       " [6, 145, 6, 17, 197, 87, 68, 6, 62, 6, 49, 16, 11, 408, 1126],\n",
       " [577, 10, 1127, 4, 198, 76, 372, 7, 110],\n",
       " [2, 96, 33, 87, 32, 6, 161, 1128, 3, 263, 1, 162, 16, 1],\n",
       " [11, 378, 371, 18, 4, 9, 369, 232, 3, 237, 40, 14, 1129],\n",
       " [87, 36, 197, 267, 2, 4, 36, 74, 5, 284, 48, 18, 538, 325, 12],\n",
       " [97, 35, 60, 2, 51, 6, 145, 12, 58, 56, 14, 102, 1, 200, 132],\n",
       " [4, 37, 134, 5, 24, 1130, 15, 76, 15, 154, 34, 7],\n",
       " [28, 567, 568, 5, 390, 1131, 261, 1, 240, 177, 8, 1],\n",
       " [200, 252, 80, 34, 98, 48, 292, 1132, 75, 1133, 297, 43, 27],\n",
       " [1134, 1135, 6, 272, 69, 20, 138, 308, 1, 324, 264],\n",
       " [173, 35, 1136, 70, 1137, 2, 5, 1138, 199, 31, 10, 5, 368, 233, 3],\n",
       " [114, 274, 86, 165, 70, 487, 7, 36, 168, 66, 12, 65, 22, 10, 1139],\n",
       " [21, 1140, 1141, 39, 22, 1142, 80, 2, 11, 9, 76, 351, 515],\n",
       " [6, 145, 6, 526, 35, 1143, 87, 4, 16, 3, 41, 10, 5, 559],\n",
       " [196, 577, 360, 3, 47, 14, 26, 68, 2, 6, 77, 136, 4, 36, 1, 225],\n",
       " [267, 10, 1, 301, 51, 70, 64, 87, 6, 103, 32, 6, 92, 119, 56, 12],\n",
       " [121, 123, 2, 68, 109, 11, 75, 3, 493, 54, 18, 4, 184, 27],\n",
       " [1144, 2, 186, 1145, 10, 5, 24, 192, 135, 4, 54, 230],\n",
       " [5, 24, 496, 8, 1146, 10, 1, 315, 2, 4, 129, 43],\n",
       " [371, 149, 453, 13, 1, 25, 17, 235, 151, 248, 2, 9, 255],\n",
       " [110, 3, 574, 151, 575],\n",
       " [242, 1147, 1, 63, 1148, 10, 5, 24, 1149],\n",
       " [7, 9, 1, 128, 63, 497, 303, 110, 54, 2, 203],\n",
       " [276, 40, 15, 7, 52, 15, 32, 7, 17, 434, 286, 2, 4, 230],\n",
       " [7, 498, 3, 297, 1, 280, 1, 280, 51, 70, 64, 536, 51],\n",
       " [70, 546, 2, 437, 4, 37, 67, 42, 1150, 15, 136, 15, 578, 106],\n",
       " [578, 1151, 97, 6, 84, 358, 61, 6, 103, 11, 1152, 10, 5],\n",
       " [131, 13, 7, 9, 203, 18, 1, 126, 2, 1, 229, 8, 128, 231, 125],\n",
       " [2, 4, 27, 157, 1153, 75, 579, 40, 18, 61, 19, 39, 65],\n",
       " [1154, 3, 20, 208, 282, 113, 3, 84, 235, 1155, 14, 1156, 10],\n",
       " [1, 137, 2, 1, 117, 147, 23, 1, 222, 120, 2, 1, 24, 95],\n",
       " [17, 1157, 1158],\n",
       " [27, 76, 1, 63, 306, 11, 15, 4, 52, 579, 40, 2],\n",
       " [199, 31, 3, 14, 10, 133, 534, 196, 132, 580, 581, 44, 106, 12, 1159],\n",
       " [31, 68, 1160, 308, 28, 131, 2, 372, 42, 5, 229, 8, 125, 2, 5, 126],\n",
       " [1161, 81, 2, 11, 9, 30, 91, 518, 13, 4, 207, 80, 34, 98],\n",
       " [10, 1, 1162, 7, 1163, 3, 174, 187, 3, 561, 1, 1164, 7],\n",
       " [111, 139, 42, 18, 151, 1165, 4, 16, 3, 41, 15, 4, 207, 38],\n",
       " [277, 111, 37, 20, 46, 111, 1166, 31, 96, 6, 160, 19, 6, 198, 548, 253, 293],\n",
       " [151, 126, 2, 125, 13, 33, 32, 6, 97, 150, 61, 15, 4, 16, 28, 4],\n",
       " [163, 142, 5, 1167, 24, 156, 21, 1, 95, 8, 85, 9, 5, 335, 1168],\n",
       " [1169, 23, 1, 319, 1170, 63, 1171, 142, 7, 4, 52, 10, 174],\n",
       " [1172, 2, 252, 1173, 10, 117, 402, 1174, 4, 218, 558, 1],\n",
       " [1175, 580, 581, 2, 20, 220, 31, 8, 1, 156, 130, 4, 17, 73, 1],\n",
       " [126, 2, 125],\n",
       " [38, 234, 7, 360, 11, 16, 3, 41, 3, 20, 116, 582, 18],\n",
       " [5, 63, 6, 527, 87, 37, 20, 489, 42, 21, 582, 210, 2, 4],\n",
       " [75, 1176, 1, 179, 8, 89, 13, 59, 304, 424, 11, 86],\n",
       " [68, 1177, 2, 67, 232, 18, 88, 317, 255, 10, 5, 473],\n",
       " [537, 19, 6, 143, 118, 3, 56, 13, 1, 25, 173, 35, 67, 31, 122, 6, 171, 35],\n",
       " [60, 11, 52, 21, 13, 39, 198, 105, 87, 495, 10, 1, 156, 32, 7],\n",
       " [75, 1178, 216, 40, 47, 13],\n",
       " [71, 28, 66, 4, 17, 73, 14, 45, 72, 5, 1179, 24, 583, 23, 5, 120],\n",
       " [10, 1, 1180, 2, 21, 7, 15, 4, 17, 1181, 5, 126, 2, 227, 57, 331, 1182],\n",
       " [8, 445, 128, 231, 125, 4, 139, 43, 1, 126, 2, 5, 229, 8, 1, 125],\n",
       " [2, 9, 141, 116, 3, 472, 1, 583, 46, 14, 353, 209, 142, 5, 24]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the closest words to Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17896, 4)\n",
      "(17896, 2557)\n",
      "2557\n"
     ]
    }
   ],
   "source": [
    "#prepare data for cbow\n",
    "path = 'D:/recsys/alice.txt'\n",
    "corpus = open(path).readlines()\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "def prep_cbow_data(corpus=corpus,window_size=window_size,V=V):\n",
    "    x = []\n",
    "    y = []\n",
    "    w = window_size\n",
    "    for sentence in corpus:\n",
    "        word_length = len(sentence)\n",
    "        if word_length >4:\n",
    "            start = w\n",
    "            for i in range(w,word_length-w):\n",
    "                context_before = sentence[i-w:i]\n",
    "                target = np.zeros(V,dtype=int)\n",
    "                target[sentence[i]] = 1\n",
    "                context_after = sentence[i+1:i+1+start]\n",
    "                context = context_before + context_after\n",
    "                if len(context) == 4:\n",
    "                    #onehot_context = np.asarray(onehot_context)\n",
    "                    x.append(context)\n",
    "                    y.append(target)\n",
    "        else:\n",
    "            pass\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "\n",
    "print(prep_cbow_data()[0].shape)\n",
    "print(prep_cbow_data()[1].shape)\n",
    "print(V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 4, 50)             127850    \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 2557)              513957    \n",
      "=================================================================\n",
      "Total params: 641,807\n",
      "Trainable params: 641,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 4, 150)            383550    \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2557)              1536757   \n",
      "=================================================================\n",
      "Total params: 1,920,307\n",
      "Trainable params: 1,920,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 4, 300)            767100    \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2557)              3070957   \n",
      "=================================================================\n",
      "Total params: 3,838,057\n",
      "Trainable params: 3,838,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=50, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=150, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=300, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model\n",
    "from keras.layers import Flatten\n",
    "#X,Y = prep_cbow_data()\n",
    "#features = len(X)\n",
    "#print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "#print(X.shape,Y.shape)\n",
    "cbow_50 = Sequential(name=\"cbow50\")\n",
    "cbow_50.add(Embedding(input_dim=V, output_dim=50, init='glorot_uniform',input_length=4))\n",
    "cbow_50.add(Flatten())\n",
    "cbow_50.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "\n",
    "cbow_50.summary()\n",
    "cbow_150 = Sequential(name=\"cbow150\")\n",
    "cbow_150.add(Embedding(input_dim=V, output_dim=150, init='glorot_uniform',input_length=4))\n",
    "cbow_150.add(Flatten())\n",
    "cbow_150.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "cbow_150.summary()\n",
    "\n",
    "cbow_300 = Sequential(name=\"cbow300\")\n",
    "cbow_300.add(Embedding(input_dim=V, output_dim=300, init='glorot_uniform',input_length=4))\n",
    "cbow_300.add(Flatten())\n",
    "cbow_300.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "cbow_300.summary()\n",
    "\n",
    "cbow_models = [cbow_50,cbow_150,cbow_300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "for cbow in cbow_models:\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cbow50\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 6s 360us/step - loss: 7.8163 - val_loss: 7.7855\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 5s 280us/step - loss: 7.7427 - val_loss: 7.6932\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 7.5963 - val_loss: 7.4274\n",
      "Epoch 4/100\n",
      "16404/16404 [==============================] - 5s 282us/step - loss: 7.1894 - val_loss: 6.8644\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 5s 282us/step - loss: 6.7076 - val_loss: 6.5217\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 5s 292us/step - loss: 6.3511 - val_loss: 6.2869\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 5s 289us/step - loss: 6.0820 - val_loss: 6.1111\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 5s 301us/step - loss: 5.8677 - val_loss: 5.9751\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 5s 303us/step - loss: 5.6861 - val_loss: 5.8603\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 5s 303us/step - loss: 5.5273 - val_loss: 5.7679\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 5s 292us/step - loss: 5.3865 - val_loss: 5.6832\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 5s 298us/step - loss: 5.2592 - val_loss: 5.6123\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 5s 301us/step - loss: 5.1428 - val_loss: 5.5419\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 5.0359 - val_loss: 5.4910\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 5s 283us/step - loss: 4.9359 - val_loss: 5.4413\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 5s 288us/step - loss: 4.8418 - val_loss: 5.3938\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 4.7533 - val_loss: 5.3490\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 4.6693 - val_loss: 5.3092\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 5s 302us/step - loss: 4.5908 - val_loss: 5.2714\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 5s 320us/step - loss: 4.5156 - val_loss: 5.2404\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 5s 312us/step - loss: 4.4441 - val_loss: 5.2089\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 6s 353us/step - loss: 4.3757 - val_loss: 5.1835\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 6s 351us/step - loss: 4.3098 - val_loss: 5.1552\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 5s 325us/step - loss: 4.2467 - val_loss: 5.1307\n",
      "Epoch 25/100\n",
      "16404/16404 [==============================] - 6s 353us/step - loss: 4.1859 - val_loss: 5.1114\n",
      "Epoch 26/100\n",
      "16404/16404 [==============================] - 5s 327us/step - loss: 4.1267 - val_loss: 5.0913\n",
      "Epoch 27/100\n",
      "16404/16404 [==============================] - 5s 319us/step - loss: 4.0698 - val_loss: 5.0713\n",
      "Epoch 28/100\n",
      "16404/16404 [==============================] - 5s 328us/step - loss: 4.0142 - val_loss: 5.0556\n",
      "Epoch 29/100\n",
      "16404/16404 [==============================] - 5s 315us/step - loss: 3.9603 - val_loss: 5.0413\n",
      "Epoch 30/100\n",
      "16404/16404 [==============================] - 5s 305us/step - loss: 3.9078 - val_loss: 5.0246\n",
      "Epoch 31/100\n",
      "16404/16404 [==============================] - 5s 306us/step - loss: 3.8564 - val_loss: 5.0111\n",
      "Epoch 32/100\n",
      "16404/16404 [==============================] - 5s 291us/step - loss: 3.8060 - val_loss: 4.9982\n",
      "Epoch 33/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.7570 - val_loss: 4.9918\n",
      "Epoch 34/100\n",
      "16404/16404 [==============================] - 5s 294us/step - loss: 3.7088 - val_loss: 4.9862\n",
      "Epoch 35/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.6613 - val_loss: 4.9743\n",
      "Epoch 36/100\n",
      "16404/16404 [==============================] - 5s 290us/step - loss: 3.6155 - val_loss: 4.9688\n",
      "Epoch 37/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.5699 - val_loss: 4.9633\n",
      "Epoch 38/100\n",
      "16404/16404 [==============================] - 5s 290us/step - loss: 3.5249 - val_loss: 4.9547\n",
      "Epoch 39/100\n",
      "16404/16404 [==============================] - 5s 289us/step - loss: 3.4813 - val_loss: 4.9550\n",
      "Running cbow150\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 8s 493us/step - loss: 7.8092 - val_loss: 7.7661\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 7s 453us/step - loss: 7.6974 - val_loss: 7.5877\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 8s 476us/step - loss: 7.3816 - val_loss: 7.0293\n",
      "Epoch 4/100\n",
      "16404/16404 [==============================] - 8s 482us/step - loss: 6.8221 - val_loss: 6.5723\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 7s 456us/step - loss: 6.3646 - val_loss: 6.2693\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 7s 442us/step - loss: 6.0253 - val_loss: 6.0495\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 7s 453us/step - loss: 5.7594 - val_loss: 5.8894\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 8s 463us/step - loss: 5.5363 - val_loss: 5.7601\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 8s 504us/step - loss: 5.3427 - val_loss: 5.6534\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 8s 491us/step - loss: 5.1716 - val_loss: 5.5612\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 8s 467us/step - loss: 5.0175 - val_loss: 5.4755\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 8s 475us/step - loss: 4.8768 - val_loss: 5.4135\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 7s 450us/step - loss: 4.7484 - val_loss: 5.3459\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 8s 460us/step - loss: 4.6296 - val_loss: 5.2960\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 8s 477us/step - loss: 4.5185 - val_loss: 5.2449\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 7s 455us/step - loss: 4.4150 - val_loss: 5.2071\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 7s 457us/step - loss: 4.3160 - val_loss: 5.1683\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 7s 447us/step - loss: 4.2236 - val_loss: 5.1328\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 8s 460us/step - loss: 4.1347 - val_loss: 5.1074\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 8s 498us/step - loss: 4.0496 - val_loss: 5.0748\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 8s 469us/step - loss: 3.9688 - val_loss: 5.0521\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 8s 497us/step - loss: 3.8902 - val_loss: 5.0352\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 8s 484us/step - loss: 3.8134 - val_loss: 5.0218\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 8s 517us/step - loss: 3.7401 - val_loss: 5.0010\n",
      "Epoch 25/100\n",
      "16404/16404 [==============================] - 9s 532us/step - loss: 3.6689 - val_loss: 4.9881\n",
      "Epoch 26/100\n",
      "16404/16404 [==============================] - 8s 489us/step - loss: 3.5981 - val_loss: 4.9729\n",
      "Epoch 27/100\n",
      "16404/16404 [==============================] - 8s 487us/step - loss: 3.5299 - val_loss: 4.9651\n",
      "Epoch 28/100\n",
      "16404/16404 [==============================] - 8s 477us/step - loss: 3.4637 - val_loss: 4.9617\n",
      "Epoch 29/100\n",
      "16404/16404 [==============================] - 8s 464us/step - loss: 3.3972 - val_loss: 4.9513\n",
      "Epoch 30/100\n",
      "16404/16404 [==============================] - 8s 473us/step - loss: 3.3331 - val_loss: 4.9452\n",
      "Epoch 31/100\n",
      "16404/16404 [==============================] - 8s 458us/step - loss: 3.2699 - val_loss: 4.9243\n",
      "Epoch 32/100\n",
      "16404/16404 [==============================] - 7s 451us/step - loss: 3.2079 - val_loss: 4.9303\n",
      "Running cbow300\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 12s 716us/step - loss: 7.8028 - val_loss: 7.7491\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 7.6494 - val_loss: 7.4736\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 11s 673us/step - loss: 7.1978 - val_loss: 6.8234\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16404/16404 [==============================] - 11s 671us/step - loss: 6.5896 - val_loss: 6.3896\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 11s 675us/step - loss: 6.1250 - val_loss: 6.1009\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 11s 685us/step - loss: 5.7831 - val_loss: 5.9054\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 5.5104 - val_loss: 5.7445\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 11s 674us/step - loss: 5.2817 - val_loss: 5.6197\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 5.0842 - val_loss: 5.5200\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 11s 665us/step - loss: 4.9096 - val_loss: 5.4264\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 11s 665us/step - loss: 4.7529 - val_loss: 5.3648\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 11s 679us/step - loss: 4.6101 - val_loss: 5.2931\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 4.4782 - val_loss: 5.2396\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.3559 - val_loss: 5.1866\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 4.2409 - val_loss: 5.1422\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.1330 - val_loss: 5.1178\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.0299 - val_loss: 5.0817\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 11s 697us/step - loss: 3.9309 - val_loss: 5.0642\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 11s 690us/step - loss: 3.8360 - val_loss: 5.0332\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 12s 707us/step - loss: 3.7451 - val_loss: 5.0167\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 13s 766us/step - loss: 3.6560 - val_loss: 5.0033\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 12s 736us/step - loss: 3.5700 - val_loss: 4.9855\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 13s 790us/step - loss: 3.4866 - val_loss: 4.9716\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 13s 783us/step - loss: 3.4045 - val_loss: 4.9707\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "from keras import callbacks\n",
    "X,Y = prep_cbow_data()\n",
    "epochs=100\n",
    "b_size = 250\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "trained_models = []\n",
    "for cbow in cbow_models:\n",
    "    print(\"Running {}\".format(cbow.name))\n",
    "    cbow.fit(X,Y,batch_size=b_size,epochs=epochs,validation_split=1/12, callbacks=[earlyStopping])\n",
    "    cbow.save(cbow.name+\".h5\")\n",
    "    trained_models.append(cbow)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-69c580ab86a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrained_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "for model in trained_models:\n",
    "    model.save(model.name +'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#get x and y's for data\n",
    "#x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "#print(x[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#create Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=50, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=150, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=300, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "dims = [50,150,300]\n",
    "extended_models = []\n",
    "for dim in dims:\n",
    "    cbow = Sequential(name=\"cbow_extended_\"+str(dim))\n",
    "    cbow.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "    cbow.add(Flatten())\n",
    "    cbow.add(Dense(V,activation='relu',name=\"dense1\"))\n",
    "    cbow.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "    extended_models.append(cbow)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified linear unit is chosen as an activation function. This is due to the widespread use in several deep learning applications. Relu takes care of the vanishing gradient problem, which is where sigmoids are prone to these issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n",
    "for model in extended_models:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size X: 17896,size Y:17896\n",
      "(17896, 4) (17896, 2557)\n",
      "training cbow_extended_50\n",
      "Train on 16106 samples, validate on 1790 samples\n",
      "Epoch 1/50\n",
      "16106/16106 [==============================] - 81s 5ms/step - loss: 6.0568 - val_loss: 5.5039\n",
      "Epoch 2/50\n",
      "16106/16106 [==============================] - 75s 5ms/step - loss: 5.1264 - val_loss: 5.1492\n",
      "Epoch 3/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 4.6873 - val_loss: 4.9828\n",
      "Epoch 4/50\n",
      "16106/16106 [==============================] - 80s 5ms/step - loss: 4.3741 - val_loss: 4.8533\n",
      "Epoch 5/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 4.1143 - val_loss: 4.8014\n",
      "Epoch 6/50\n",
      "16106/16106 [==============================] - 78s 5ms/step - loss: 3.8781 - val_loss: 4.7468\n",
      "Epoch 7/50\n",
      "16106/16106 [==============================] - 80s 5ms/step - loss: 3.6612 - val_loss: 4.7839\n",
      "Epoch 8/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 3.4524 - val_loss: 4.8297\n",
      "training cbow_extended_150\n",
      "Train on 16106 samples, validate on 1790 samples\n",
      "Epoch 1/50\n",
      "16106/16106 [==============================] - 92s 6ms/step - loss: 5.9471 - val_loss: 5.3828\n",
      "Epoch 2/50\n",
      "16106/16106 [==============================] - 94s 6ms/step - loss: 4.8984 - val_loss: 5.0653\n",
      "Epoch 3/50\n",
      "16106/16106 [==============================] - 91s 6ms/step - loss: 4.4160 - val_loss: 4.8847\n",
      "Epoch 4/50\n",
      "11136/16106 [===================>..........] - ETA: 27s - loss: 4.0248"
     ]
    }
   ],
   "source": [
    "#train model for CBOW + dense\n",
    "from keras import callbacks\n",
    "X,Y = prep_cbow_data()\n",
    "features = len(X)\n",
    "print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "print(X.shape,Y.shape)\n",
    "epochs=50\n",
    "for model in extended_models:\n",
    "    earlyStopping=callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')\n",
    "    print(\"training {}\".format(model.name))\n",
    "    model.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])\n",
    "    model.save(model.name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-389a1c6c7ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mskipgram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glorot_uniform'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[0;32m    491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;31m# Inferring the output shape is only relevant for Theano.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;31m# input shape known? then we can compute the output shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             return (input_shape[0],) + self._fix_unknown_dimension(\n\u001b[1;32m--> 402\u001b[1;33m                 input_shape[1:], self.target_shape)\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=300, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(V,kernel_initializer='uniform',activation='relu'))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "\n",
    "def analogy(words):\n",
    "    if len(words) != 4:\n",
    "        print(\"model trained on window 4 give me 4 words plx\")\n",
    "        return None\n",
    "    else:\n",
    "        vector = tokenizer.texts_to_sequence(words)\n",
    "        model.predict(vector)\n",
    "        \n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualization results trained word embeddings\n",
    "tst = \"chapter\"\n",
    "\n",
    "tokenizer.texts_to_sequences([\"Alice down the rabbit hole\"])\n",
    "tokenizer.texts_to_matrix([\"Alice down the rabbit hole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'down'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = {v:k for k,v in tokenizer.word_index.items()}\n",
    "reverse[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
