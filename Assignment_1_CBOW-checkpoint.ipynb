{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'D:/recsys/alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the closest words to Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17896, 4)\n",
      "(17896, 2557)\n",
      "2557\n"
     ]
    }
   ],
   "source": [
    "#prepare data for cbow\n",
    "path = 'D:/recsys/alice.txt'\n",
    "corpus = open(path).readlines()\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "def prep_cbow_data(corpus=corpus,window_size=window_size,V=V):\n",
    "    x = []\n",
    "    y = []\n",
    "    w = window_size\n",
    "    for sentence in corpus:\n",
    "        word_length = len(sentence)\n",
    "        if word_length >4:\n",
    "            start = w\n",
    "            for i in range(w,word_length-w):\n",
    "                context_before = sentence[i-w:i]\n",
    "                target = np.zeros(V,dtype=int)\n",
    "                target[sentence[i]] = 1\n",
    "                context_after = sentence[i+1:i+1+start]\n",
    "                context = context_before + context_after\n",
    "                if len(context) == 4:\n",
    "                    #onehot_context = np.asarray(onehot_context)\n",
    "                    x.append(context)\n",
    "                    y.append(target)\n",
    "        else:\n",
    "            pass\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "\n",
    "print(prep_cbow_data()[0].shape)\n",
    "print(prep_cbow_data()[1].shape)\n",
    "print(V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 4, 50)             127850    \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 2557)              513957    \n",
      "=================================================================\n",
      "Total params: 641,807\n",
      "Trainable params: 641,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 4, 150)            383550    \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2557)              1536757   \n",
      "=================================================================\n",
      "Total params: 1,920,307\n",
      "Trainable params: 1,920,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 4, 300)            767100    \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2557)              3070957   \n",
      "=================================================================\n",
      "Total params: 3,838,057\n",
      "Trainable params: 3,838,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=50, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=150, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=300, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model\n",
    "from keras.layers import Flatten\n",
    "#X,Y = prep_cbow_data()\n",
    "#features = len(X)\n",
    "#print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "#print(X.shape,Y.shape)\n",
    "cbow_50 = Sequential(name=\"cbow50\")\n",
    "cbow_50.add(Embedding(input_dim=V, output_dim=50, init='glorot_uniform',input_length=4))\n",
    "cbow_50.add(Flatten())\n",
    "cbow_50.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "\n",
    "cbow_50.summary()\n",
    "cbow_150 = Sequential(name=\"cbow150\")\n",
    "cbow_150.add(Embedding(input_dim=V, output_dim=150, init='glorot_uniform',input_length=4))\n",
    "cbow_150.add(Flatten())\n",
    "cbow_150.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "cbow_150.summary()\n",
    "\n",
    "cbow_300 = Sequential(name=\"cbow300\")\n",
    "cbow_300.add(Embedding(input_dim=V, output_dim=300, init='glorot_uniform',input_length=4))\n",
    "cbow_300.add(Flatten())\n",
    "cbow_300.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "cbow_300.summary()\n",
    "\n",
    "cbow_models = [cbow_50,cbow_150,cbow_300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "for cbow in cbow_models:\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cbow50\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 6s 360us/step - loss: 7.8163 - val_loss: 7.7855\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 5s 280us/step - loss: 7.7427 - val_loss: 7.6932\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 7.5963 - val_loss: 7.4274\n",
      "Epoch 4/100\n",
      "16404/16404 [==============================] - 5s 282us/step - loss: 7.1894 - val_loss: 6.8644\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 5s 282us/step - loss: 6.7076 - val_loss: 6.5217\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 5s 292us/step - loss: 6.3511 - val_loss: 6.2869\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 5s 289us/step - loss: 6.0820 - val_loss: 6.1111\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 5s 301us/step - loss: 5.8677 - val_loss: 5.9751\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 5s 303us/step - loss: 5.6861 - val_loss: 5.8603\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 5s 303us/step - loss: 5.5273 - val_loss: 5.7679\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 5s 292us/step - loss: 5.3865 - val_loss: 5.6832\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 5s 298us/step - loss: 5.2592 - val_loss: 5.6123\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 5s 301us/step - loss: 5.1428 - val_loss: 5.5419\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 5.0359 - val_loss: 5.4910\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 5s 283us/step - loss: 4.9359 - val_loss: 5.4413\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 5s 288us/step - loss: 4.8418 - val_loss: 5.3938\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 4.7533 - val_loss: 5.3490\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 5s 287us/step - loss: 4.6693 - val_loss: 5.3092\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 5s 302us/step - loss: 4.5908 - val_loss: 5.2714\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 5s 320us/step - loss: 4.5156 - val_loss: 5.2404\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 5s 312us/step - loss: 4.4441 - val_loss: 5.2089\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 6s 353us/step - loss: 4.3757 - val_loss: 5.1835\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 6s 351us/step - loss: 4.3098 - val_loss: 5.1552\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 5s 325us/step - loss: 4.2467 - val_loss: 5.1307\n",
      "Epoch 25/100\n",
      "16404/16404 [==============================] - 6s 353us/step - loss: 4.1859 - val_loss: 5.1114\n",
      "Epoch 26/100\n",
      "16404/16404 [==============================] - 5s 327us/step - loss: 4.1267 - val_loss: 5.0913\n",
      "Epoch 27/100\n",
      "16404/16404 [==============================] - 5s 319us/step - loss: 4.0698 - val_loss: 5.0713\n",
      "Epoch 28/100\n",
      "16404/16404 [==============================] - 5s 328us/step - loss: 4.0142 - val_loss: 5.0556\n",
      "Epoch 29/100\n",
      "16404/16404 [==============================] - 5s 315us/step - loss: 3.9603 - val_loss: 5.0413\n",
      "Epoch 30/100\n",
      "16404/16404 [==============================] - 5s 305us/step - loss: 3.9078 - val_loss: 5.0246\n",
      "Epoch 31/100\n",
      "16404/16404 [==============================] - 5s 306us/step - loss: 3.8564 - val_loss: 5.0111\n",
      "Epoch 32/100\n",
      "16404/16404 [==============================] - 5s 291us/step - loss: 3.8060 - val_loss: 4.9982\n",
      "Epoch 33/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.7570 - val_loss: 4.9918\n",
      "Epoch 34/100\n",
      "16404/16404 [==============================] - 5s 294us/step - loss: 3.7088 - val_loss: 4.9862\n",
      "Epoch 35/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.6613 - val_loss: 4.9743\n",
      "Epoch 36/100\n",
      "16404/16404 [==============================] - 5s 290us/step - loss: 3.6155 - val_loss: 4.9688\n",
      "Epoch 37/100\n",
      "16404/16404 [==============================] - 5s 293us/step - loss: 3.5699 - val_loss: 4.9633\n",
      "Epoch 38/100\n",
      "16404/16404 [==============================] - 5s 290us/step - loss: 3.5249 - val_loss: 4.9547\n",
      "Epoch 39/100\n",
      "16404/16404 [==============================] - 5s 289us/step - loss: 3.4813 - val_loss: 4.9550\n",
      "Running cbow150\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 8s 493us/step - loss: 7.8092 - val_loss: 7.7661\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 7s 453us/step - loss: 7.6974 - val_loss: 7.5877\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 8s 476us/step - loss: 7.3816 - val_loss: 7.0293\n",
      "Epoch 4/100\n",
      "16404/16404 [==============================] - 8s 482us/step - loss: 6.8221 - val_loss: 6.5723\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 7s 456us/step - loss: 6.3646 - val_loss: 6.2693\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 7s 442us/step - loss: 6.0253 - val_loss: 6.0495\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 7s 453us/step - loss: 5.7594 - val_loss: 5.8894\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 8s 463us/step - loss: 5.5363 - val_loss: 5.7601\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 8s 504us/step - loss: 5.3427 - val_loss: 5.6534\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 8s 491us/step - loss: 5.1716 - val_loss: 5.5612\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 8s 467us/step - loss: 5.0175 - val_loss: 5.4755\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 8s 475us/step - loss: 4.8768 - val_loss: 5.4135\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 7s 450us/step - loss: 4.7484 - val_loss: 5.3459\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 8s 460us/step - loss: 4.6296 - val_loss: 5.2960\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 8s 477us/step - loss: 4.5185 - val_loss: 5.2449\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 7s 455us/step - loss: 4.4150 - val_loss: 5.2071\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 7s 457us/step - loss: 4.3160 - val_loss: 5.1683\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 7s 447us/step - loss: 4.2236 - val_loss: 5.1328\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 8s 460us/step - loss: 4.1347 - val_loss: 5.1074\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 8s 498us/step - loss: 4.0496 - val_loss: 5.0748\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 8s 469us/step - loss: 3.9688 - val_loss: 5.0521\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 8s 497us/step - loss: 3.8902 - val_loss: 5.0352\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 8s 484us/step - loss: 3.8134 - val_loss: 5.0218\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 8s 517us/step - loss: 3.7401 - val_loss: 5.0010\n",
      "Epoch 25/100\n",
      "16404/16404 [==============================] - 9s 532us/step - loss: 3.6689 - val_loss: 4.9881\n",
      "Epoch 26/100\n",
      "16404/16404 [==============================] - 8s 489us/step - loss: 3.5981 - val_loss: 4.9729\n",
      "Epoch 27/100\n",
      "16404/16404 [==============================] - 8s 487us/step - loss: 3.5299 - val_loss: 4.9651\n",
      "Epoch 28/100\n",
      "16404/16404 [==============================] - 8s 477us/step - loss: 3.4637 - val_loss: 4.9617\n",
      "Epoch 29/100\n",
      "16404/16404 [==============================] - 8s 464us/step - loss: 3.3972 - val_loss: 4.9513\n",
      "Epoch 30/100\n",
      "16404/16404 [==============================] - 8s 473us/step - loss: 3.3331 - val_loss: 4.9452\n",
      "Epoch 31/100\n",
      "16404/16404 [==============================] - 8s 458us/step - loss: 3.2699 - val_loss: 4.9243\n",
      "Epoch 32/100\n",
      "16404/16404 [==============================] - 7s 451us/step - loss: 3.2079 - val_loss: 4.9303\n",
      "Running cbow300\n",
      "Train on 16404 samples, validate on 1492 samples\n",
      "Epoch 1/100\n",
      "16404/16404 [==============================] - 12s 716us/step - loss: 7.8028 - val_loss: 7.7491\n",
      "Epoch 2/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 7.6494 - val_loss: 7.4736\n",
      "Epoch 3/100\n",
      "16404/16404 [==============================] - 11s 673us/step - loss: 7.1978 - val_loss: 6.8234\n",
      "Epoch 4/100\n",
      "16404/16404 [==============================] - 11s 671us/step - loss: 6.5896 - val_loss: 6.3896\n",
      "Epoch 5/100\n",
      "16404/16404 [==============================] - 11s 675us/step - loss: 6.1250 - val_loss: 6.1009\n",
      "Epoch 6/100\n",
      "16404/16404 [==============================] - 11s 685us/step - loss: 5.7831 - val_loss: 5.9054\n",
      "Epoch 7/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 5.5104 - val_loss: 5.7445\n",
      "Epoch 8/100\n",
      "16404/16404 [==============================] - 11s 674us/step - loss: 5.2817 - val_loss: 5.6197\n",
      "Epoch 9/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 5.0842 - val_loss: 5.5200\n",
      "Epoch 10/100\n",
      "16404/16404 [==============================] - 11s 665us/step - loss: 4.9096 - val_loss: 5.4264\n",
      "Epoch 11/100\n",
      "16404/16404 [==============================] - 11s 665us/step - loss: 4.7529 - val_loss: 5.3648\n",
      "Epoch 12/100\n",
      "16404/16404 [==============================] - 11s 679us/step - loss: 4.6101 - val_loss: 5.2931\n",
      "Epoch 13/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 4.4782 - val_loss: 5.2396\n",
      "Epoch 14/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.3559 - val_loss: 5.1866\n",
      "Epoch 15/100\n",
      "16404/16404 [==============================] - 11s 669us/step - loss: 4.2409 - val_loss: 5.1422\n",
      "Epoch 16/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.1330 - val_loss: 5.1178\n",
      "Epoch 17/100\n",
      "16404/16404 [==============================] - 11s 667us/step - loss: 4.0299 - val_loss: 5.0817\n",
      "Epoch 18/100\n",
      "16404/16404 [==============================] - 11s 697us/step - loss: 3.9309 - val_loss: 5.0642\n",
      "Epoch 19/100\n",
      "16404/16404 [==============================] - 11s 690us/step - loss: 3.8360 - val_loss: 5.0332\n",
      "Epoch 20/100\n",
      "16404/16404 [==============================] - 12s 707us/step - loss: 3.7451 - val_loss: 5.0167\n",
      "Epoch 21/100\n",
      "16404/16404 [==============================] - 13s 766us/step - loss: 3.6560 - val_loss: 5.0033\n",
      "Epoch 22/100\n",
      "16404/16404 [==============================] - 12s 736us/step - loss: 3.5700 - val_loss: 4.9855\n",
      "Epoch 23/100\n",
      "16404/16404 [==============================] - 13s 790us/step - loss: 3.4866 - val_loss: 4.9716\n",
      "Epoch 24/100\n",
      "16404/16404 [==============================] - 13s 783us/step - loss: 3.4045 - val_loss: 4.9707\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "from keras import callbacks\n",
    "X,Y = prep_cbow_data()\n",
    "epochs=100\n",
    "b_size = 250\n",
    "earlyStopping=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001, patience=0, verbose=0, mode='auto')\n",
    "trained_models = []\n",
    "for cbow in cbow_models:\n",
    "    print(\"Running {}\".format(cbow.name))\n",
    "    cbow.fit(X,Y,batch_size=b_size,epochs=epochs,validation_split=1/12, callbacks=[earlyStopping])\n",
    "    cbow.save(cbow.name+\".h5\")\n",
    "    trained_models.append(cbow)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#get x and y's for data\n",
    "#x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "#print(x[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#create Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=50, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2557, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=150, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n",
      "C:\\Users\\timothy\\Anaconda3\\envs\\recsys\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_length=4, output_dim=300, embeddings_initializer=\"glorot_uniform\", input_dim=2557)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "dims = [50,150,300]\n",
    "extended_models = []\n",
    "for dim in dims:\n",
    "    cbow = Sequential(name=\"cbow_extended_\"+str(dim))\n",
    "    cbow.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=4))\n",
    "    cbow.add(Flatten())\n",
    "    cbow.add(Dense(V,activation='relu',name=\"dense1\"))\n",
    "    cbow.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "    extended_models.append(cbow)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified linear unit is chosen as an activation function. This is due to the widespread use in several deep learning applications. Relu takes care of the vanishing gradient problem, which is where sigmoids are prone to these issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n",
    "for model in extended_models:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size X: 17896,size Y:17896\n",
      "(17896, 4) (17896, 2557)\n",
      "training cbow_extended_50\n",
      "Train on 16106 samples, validate on 1790 samples\n",
      "Epoch 1/50\n",
      "16106/16106 [==============================] - 81s 5ms/step - loss: 6.0568 - val_loss: 5.5039\n",
      "Epoch 2/50\n",
      "16106/16106 [==============================] - 75s 5ms/step - loss: 5.1264 - val_loss: 5.1492\n",
      "Epoch 3/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 4.6873 - val_loss: 4.9828\n",
      "Epoch 4/50\n",
      "16106/16106 [==============================] - 80s 5ms/step - loss: 4.3741 - val_loss: 4.8533\n",
      "Epoch 5/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 4.1143 - val_loss: 4.8014\n",
      "Epoch 6/50\n",
      "16106/16106 [==============================] - 78s 5ms/step - loss: 3.8781 - val_loss: 4.7468\n",
      "Epoch 7/50\n",
      "16106/16106 [==============================] - 80s 5ms/step - loss: 3.6612 - val_loss: 4.7839\n",
      "Epoch 8/50\n",
      "16106/16106 [==============================] - 79s 5ms/step - loss: 3.4524 - val_loss: 4.8297\n",
      "training cbow_extended_150\n",
      "Train on 16106 samples, validate on 1790 samples\n",
      "Epoch 1/50\n",
      "16106/16106 [==============================] - 92s 6ms/step - loss: 5.9471 - val_loss: 5.3828\n",
      "Epoch 2/50\n",
      "16106/16106 [==============================] - 94s 6ms/step - loss: 4.8984 - val_loss: 5.0653\n",
      "Epoch 3/50\n",
      "16106/16106 [==============================] - 91s 6ms/step - loss: 4.4160 - val_loss: 4.8847\n",
      "Epoch 4/50\n",
      "16106/16106 [==============================] - 92s 6ms/step - loss: 4.0403 - val_loss: 4.8444\n",
      "Epoch 5/50\n",
      "16106/16106 [==============================] - 89s 6ms/step - loss: 3.7114 - val_loss: 4.7163\n",
      "Epoch 6/50\n",
      "16106/16106 [==============================] - 89s 6ms/step - loss: 3.4036 - val_loss: 4.7544\n",
      "Epoch 7/50\n",
      "16106/16106 [==============================] - 89s 6ms/step - loss: 3.0970 - val_loss: 4.8709\n",
      "training cbow_extended_300\n",
      "Train on 16106 samples, validate on 1790 samples\n",
      "Epoch 1/50\n",
      "16106/16106 [==============================] - 113s 7ms/step - loss: 5.8962 - val_loss: 5.3818\n",
      "Epoch 2/50\n",
      "16106/16106 [==============================] - 114s 7ms/step - loss: 4.7501 - val_loss: 4.9172\n",
      "Epoch 3/50\n",
      "16106/16106 [==============================] - 117s 7ms/step - loss: 4.2229 - val_loss: 4.8008\n",
      "Epoch 4/50\n",
      "16106/16106 [==============================] - 116s 7ms/step - loss: 3.8088 - val_loss: 4.6947\n",
      "Epoch 5/50\n",
      "16106/16106 [==============================] - 117s 7ms/step - loss: 3.4297 - val_loss: 4.7549\n",
      "Epoch 6/50\n",
      "16106/16106 [==============================] - 116s 7ms/step - loss: 3.0417 - val_loss: 4.7753\n"
     ]
    }
   ],
   "source": [
    "#train model for CBOW + dense\n",
    "from keras import callbacks\n",
    "X,Y = prep_cbow_data()\n",
    "features = len(X)\n",
    "print(\"size X: {},size Y:{}\".format(len(X),len(Y)))\n",
    "print(X.shape,Y.shape)\n",
    "epochs=50\n",
    "for model in extended_models:\n",
    "    earlyStopping=callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')\n",
    "    print(\"training {}\".format(model.name))\n",
    "    model.fit(X,Y,epochs=epochs,validation_split=0.1, callbacks=[earlyStopping])\n",
    "    model.save(model.name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-389a1c6c7ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mskipgram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glorot_uniform'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[0;32m    491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;31m# Inferring the output shape is only relevant for Theano.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;31m# input shape known? then we can compute the output shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             return (input_shape[0],) + self._fix_unknown_dimension(\n\u001b[1;32m--> 402\u001b[1;33m                 input_shape[1:], self.target_shape)\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=300, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(V,kernel_initializer='uniform',activation='relu'))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "\n",
    "def analogy(words):\n",
    "    if len(words) != 4:\n",
    "        print(\"model trained on window 4 give me 4 words plx\")\n",
    "        return None\n",
    "    else:\n",
    "        vector = tokenizer.texts_to_sequence(words)\n",
    "        model.predict(vector)\n",
    "        \n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualization results trained word embeddings\n",
    "def draw_results(words):\n",
    "    words_2d = TSNE(n_components=2).fit_transform(X)\n",
    "    \n",
    "    for vec in words_2d:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'down'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = {v:k for k,v in tokenizer.word_index.items()}\n",
    "reverse[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "497.27933 -583.14703\n",
      "0.3175767752696409\n",
      "-0.25280162369099257\n",
      "-0.7034328904498413\n",
      "-1.0\n",
      "-0.8560170566471721\n",
      "-0.6267823293034819\n",
      "0.4674819068017782\n",
      "0.8527511908012754\n",
      "0.4854731088608342\n",
      "0.5256148003183075\n",
      "0.5996088559672884\n",
      "-0.15914733667556982\n",
      "0.052980186970636514\n",
      "0.35396944050525625\n",
      "-0.3636570088319571\n",
      "0.30639437965033683\n",
      "[[ 185.19395  -147.42052  -410.2048   -583.14703 ]\n",
      " [-499.1838   -365.50626   272.6107    497.27933 ]\n",
      " [ 283.1022    306.5107    349.66013   -92.8063  ]\n",
      " [  30.895239  206.41623  -212.0655    178.67297 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25282a1cc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0,0,0,0,0], [0,0,0,0,0, 1, 1], [1, 0,0,0,0,0, 1], [1, 1, 0,0,0,0,1]])\n",
    "print(X.shape)\n",
    "\n",
    "    def draw_word_vecs(X,strings):\n",
    "        col = ['r','b','g','y']\n",
    "        if len(X)!= len(strings):\n",
    "            print(\"mismatch in lengths between labels and vectors\")\n",
    "\n",
    "        X_embedded = TSNE(n_components=4,method='exact').fit_transform(X)\n",
    "        maxt = np.amax(X_embedded)\n",
    "        mint = np.amin(X_embedded)\n",
    "        print(maxt,mint)\n",
    "        maxt = max(maxt,mint*-1)\n",
    "        for i in range(len(X_embedded)):\n",
    "            plt.quiver(X_embedded[i][0]/maxt,X_embedded[i][1]/maxt,X_embedded[i][2]/maxt,X_embedded[i][3]/maxt,angles='xy', scale_units='xy',color=col[i], scale=1,label=strings[i])\n",
    "            for val in X_embedded[i]:\n",
    "                print(val/maxt)\n",
    "        print(X_embedded)\n",
    "        plt.ylim(-1.2,1.2)\n",
    "        plt.xlim(-1.2,1.2)\n",
    "        plt.show()\n",
    "    \n",
    "tst = ['one','two','three','four']\n",
    "\n",
    "\n",
    "draw_word_vecs(X,tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
